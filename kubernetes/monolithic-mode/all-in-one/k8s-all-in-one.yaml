apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: logging-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: profiles-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: tracing-system
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.12.0
    helm.sh/chart: kube-state-metrics-5.19.0
  name: kube-state-metrics
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.7.0
    helm.sh/chart: prometheus-node-exporter-4.32.0
  name: prometheus-node-exporter
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.4.2
    helm.sh/chart: tempo-1.8.0
  name: tempo
  namespace: tracing-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: profiles-system-pyroscope
  namespace: profiles-system
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.12.0
    helm.sh/chart: kube-state-metrics-5.19.0
  name: kube-state-metrics
rules:
- apiGroups:
  - certificates.k8s.io
  resources:
  - certificatesigningrequests
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - daemonsets
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - deployments
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - limitranges
  verbs:
  - list
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - replicasets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - replicationcontrollers
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - resourcequotas
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - list
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - volumeattachments
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-clusterrole
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: profiles-system-pyroscope
  namespace: profiles-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: profiles-system-pyroscope
subjects:
- kind: ServiceAccount
  name: pyroscope
  namespace: profiles-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy
subjects:
- kind: ServiceAccount
  name: alloy
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.12.0
    helm.sh/chart: kube-state-metrics-5.19.0
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: loki-clusterrole
subjects:
- kind: ServiceAccount
  name: loki
  namespace: logging-system
---
apiVersion: v1
data:
  gateway_loki.conf.template: |-
    server {
        listen 3100;
        listen [::]:3100;

        location = / {
          return 200 'OK';
          auth_basic off;
          access_log off;
        }

        # Distributor endpoints
        location = /api/prom/push {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/push {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }
        location = /distributor/ring {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }

        # Ingester endpoints
        location /flush {
          proxy_pass      http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }
        location ^~ /ingester/ {
          proxy_pass      http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }
        location = /ingester {
          internal;        # to suppress 301
        }

        # Ring
        location = /ring {
          proxy_pass http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }

        # MemberListKV
        location = /memberlist {
          proxy_pass http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }


        # Ruler endpoints
        location = /ruler/ring {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /api/prom/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /api/prom/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /prometheus/api/v1/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /prometheus/api/v1/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }


        # Compactor endpoints
        location = /compactor/ring {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/delete {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/cache/generation_numbers {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # IndexGateway endpoints
        location = /indexgateway/ring {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # Config endpoints
        location = /config {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # QueryFrontend, Querier endpoints
        location = /api/prom/tail {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
        }
        location = /loki/api/v1/tail {
          proxy_pass      http://${LOKI_QUERIER_HOST}:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
        }
        location ~ /api/prom/.* {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1.* {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
        }
      }
  gateway_mimir.conf.template: "server {\n    listen 8080;\n    listen [::]:8080;\n\n
    \   location = / {\n      return 200 'OK';\n      auth_basic off;\n      access_log
    off;\n    }\n\n    # Distributor endpoints\n    location /distributor {\n      proxy_pass
    \     http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n    }\n    location
    = /api/v1/push {\n      proxy_pass      http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location /otlp/v1/metrics {\n      proxy_pass      http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass
    \     http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n    location
    = /multitenant_alertmanager/status {\n      proxy_pass      http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n    # https://github.com/grafana/mimir/releases/tag/mimir-2.12.0\n    #
    Alertmanager deprecated the v1 API. All endpoints have a v2 equivalent.\n    location
    = /api/v2/alerts {\n      proxy_pass      http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n      proxy_pass
    \     http://${MIMIR_RULER_HOST}:8080$request_uri;\n    }\n    location /prometheus/api/v1/rules
    {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n    }\n
    \   \n    location /prometheus/api/v1/alerts {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n
    \   }\n    location = /ruler/ring {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n
    \   }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus
    {\n      proxy_pass      http://${MIMIR_QUERY_FRONTEND_HOST}:8080$request_uri;\n
    \   }\n\n    # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo
    {\n      proxy_pass      http://${MIMIR_QUERY_FRONTEND_HOST}:8080$request_uri;\n
    \   }\n\n    # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/
    {\n      proxy_pass      http://${MIMIR_COMPACTOR_HOST}:8080$request_uri;\n    }\n}"
  gateway_pyroscope.conf.template: |-
    server {
        listen 4040;
        listen [::]:4040;

        location = / {
          return 200 'OK';
          auth_basic off;
          access_log off;
        }

        # Distributor endpoints
        location /push.v1.PusherService {
          proxy_pass      http://${PYROSCOPE_DISTRIBUTOR_HOST}:4040$request_uri;
        }

        location /querier.v1.QuerierService {
          proxy_pass      http://${PYROSCOPE_QUERY_FRONTEND_HOST}:4040$request_uri;
        }
    }
  gateway_tempo.conf.template: "upstream grpc_otlp_tempo {\n    server ${TEMPO_DISTRIBUTOR_HOST}:4317;\n}\nserver
    {\n    listen 4317;\n    http2 on;\n\n    location / {\n      grpc_set_header
    X-Scope-OrgID $ensured_x_scope_orgid;\n      grpc_pass grpc://grpc_otlp_tempo;\n
    \   }\n}\n\nupstream http_otlp_tempo {\n    server ${TEMPO_DISTRIBUTOR_HOST}:4318;\n}\nserver
    {\n    listen 4318;\n\n    location / {\n      proxy_set_header X-Scope-OrgID
    $ensured_x_scope_orgid;\n      proxy_pass http://http_otlp_tempo;\n    }\n}\n\nserver
    {\n    listen 3200;\n    listen [::]:3200;\n\n    location = / {\n      return
    200 'OK';\n      auth_basic off;\n      access_log off;\n    }\n\n    # Distributor
    endpoints\n    location = /jaeger/api/traces {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:14268/api/traces;\n
    \   }\n    location = /zipkin/spans {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:9411/spans;\n
    \   }\n    location = /otlp/v1/traces {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:4318/v1/traces;\n
    \   }\n\n    location = /distributor/ring {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:3100$request_uri;\n
    \   }\n    location = /ingester/ring {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:3100$request_uri;\n
    \   }\n    \n    # Ingester endpoints\n    location = /flush {\n      proxy_pass
    \     http://${TEMPO_INGESTER_HOST}:3100$request_uri;\n    }\n    location = /shutdown
    {\n      proxy_pass      http://${TEMPO_INGESTER_HOST}:3100$request_uri;\n    }\n\n
    \   # Query endpoints\n    location ^~ /api {\n      proxy_pass      http://${TEMPO_QUERY_FRONTEND_HOST}:3100$request_uri;\n
    \   }\n\n    # Compactor endpoint\n    location = /compactor/ring {\n      proxy_pass
    \     http://${TEMPO_COMPACTOR_HOST}:3100$request_uri;\n    }\n}"
kind: ConfigMap
metadata:
  name: nginx-templates
  namespace: gateway
---
apiVersion: v1
data:
  config.yaml: |
    # Multi-tenant mode is set in the configuration with auth_enabled: true
    # https://grafana.com/docs/loki/latest/operations/multi-tenancy/
    auth_enabled: true

    # -reporting.enabled=false
    analytics:
     reporting_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9095
      log_level: info
      log_format: json

    # https://grafana.com/docs/loki/latest/configure/#use-environment-variables-in-the-configuration
    common:
      compactor_address: http://loki.logging-system.svc.cluster.local:3100
      path_prefix: /var/loki
      replication_factor: 1
      storage:
        s3:
          bucketnames: loki-data
          endpoint: ${LOKI_S3_ENDPOINT:-minio.minio-system.svc.cluster.local:443}
          access_key_id: ${LOKI_S3_ACCESS_KEY_ID:-lgtmp}
          secret_access_key: ${LOKI_S3_SECRET_ACCESS_KEY:-supersecret}
          insecure: ${LOKI_S3_INSECURE:-false}
          s3forcepathstyle: true
          http_config:
            insecure_skip_verify: true

    bloom_gateway:
      enabled: true
      client:
        addresses: "dns+loki.logging-system.svc.cluster.local:9095"
        cache_results: true
        results_cache:
          cache:
            memcached_client:
              addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

    bloom_compactor:
      enabled: true
      ring:
        kvstore:
          store: memberlist

    index_gateway:
      mode: simple

    compactor:
      working_directory: /tmp/compactor

    memberlist:
      join_members:
      - loki-memberlist.logging-system.svc.cluster.local:7946

    # https://github.com/grafana/loki/blob/main/docs/sources/configure/_index.md#query_range
    query_range:
      align_queries_with_step: true

      cache_results: true
      results_cache:
        cache:
          memcached_client:
            addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

      cache_index_stats_results: true
      index_stats_results_cache:
        cache:
          memcached_client:
            addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

    pattern_ingester:
      enabled: true

    limits_config:
      max_global_streams_per_user: 0
      ingestion_rate_mb: 50000
      ingestion_burst_size_mb: 50000
      volume_enabled: true

    ruler:
      storage:
        s3:
          bucketnames: loki-ruler
        type: s3

    runtime_config:
      file: /etc/loki/runtime-config/runtime-config.yaml

    schema_config:
      configs:
      - from: "2024-04-08"
        index:
          period: 24h
          prefix: loki_index_
        object_store: s3
        schema: v13
        store: tsdb

    storage_config:
      tsdb_shipper:
        active_index_directory: /var/loki/index
        cache_location: /var/loki/cache
        index_gateway_client:
          server_address: "dns+loki.logging-system.svc.cluster.local:9095"

    chunk_store_config:
      chunk_cache_config:
        memcached_client:
          addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-config-2mt25bhccf
  namespace: logging-system
---
apiVersion: v1
data:
  runtime-config.yaml: |
    # This file can be used to set overrides or other runtime config.
    overrides:
      "fake": # limits for anonymous that the whole cluster enforces
        ingestion_rate_mb: 1500000
        max_streams_per_user: 100000
        max_chunks_per_query: 100000
      "anonymous": # limits for anonymous that the whole cluster enforces
        ingestion_rate_mb: 1500000
        max_streams_per_user: 100000
        max_chunks_per_query: 100000
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-runtime-9599m5k6h2
  namespace: logging-system
---
apiVersion: v1
data:
  alloy-cluster-node.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Clustering documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/cli/run/#clustered-mode"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "Node Info",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Information about a specific cluster node.\n\n* Lamport clock time: The observed Lamport time on the specific node's clock used to provide partial ordering around gossip messages. Nodes should ideally be observing roughly the same time, meaning they are up-to-date on the cluster state. If a node is falling behind, it means that it has not recently processed the same number of messages and may have an outdated view of its peers.\n\n* Internal cluster state observers: The number of Observer functions that are registered to run whenever the node detects a cluster change.\n\n* Gossip health score: A health score assigned to this node by the memberlist implementation. The lower, the better.\n\n* Gossip protocol version: The protocol version used by nodes to communicate with one another. It should match across all nodes.\n",
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_lamport_time{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Lamport clock time"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_update_observers{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Internal cluster state observers"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_gossip_health_score{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Gossip health score"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_gossip_proto_version{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Gossip protocol version"
                   }
                ],
                "title": "Node Info",
                "transformations": [
                   {
                      "id": "renameByRegex",
                      "options": {
                         "regex": "Value #(.*)",
                         "renamePattern": "$1"
                      }
                   },
                   {
                      "id": "reduce",
                      "options": { }
                   },
                   {
                      "id": "organize",
                      "options": {
                         "excludeByName": { },
                         "indexByName": { },
                         "renameByName": {
                            "Field": "Metric",
                            "Max": "Value"
                         }
                      }
                   }
                ],
                "type": "table"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_node_gossip_received_events_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "{{event}}",
                      "range": true
                   }
                ],
                "title": "Gossip ops/s",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Known peers to the node (including the local node).\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "suffix:peers"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 9
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_peers{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Known peers",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Known peers to the node by state (including the local node).\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "suffix:nodes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 9
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_node_peers{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}",
                      "instant": false,
                      "legendFormat": "{{state}}",
                      "range": true
                   }
                ],
                "title": "Peers by state",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 17
                },
                "title": "Gossip Transport",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "axisCenteredZero": true
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_transport_rx_bytes_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "rx",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "-1 * rate(cluster_transport_tx_bytes_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "tx",
                      "range": true
                   }
                ],
                "title": "Transport bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\nrate(cluster_transport_tx_packets_failed_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]) /\nrate(cluster_transport_tx_packets_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "Tx success %",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_rx_packets_failed_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]) /\n  rate(cluster_transport_rx_packets_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])\n  )\n",
                      "instant": false,
                      "legendFormat": "Rx success %",
                      "range": true
                   }
                ],
                "title": "Packet write success rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of packets enqueued currently to be decoded or encoded and sent during communication with other nodes.\n\nThe incoming and outgoing packet queue should be as empty as possible; a growing queue means that Alloy cannot keep up with the number of messages required to have all nodes informed of cluster changes, and the nodes may not converge in a timely fashion.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "pkts"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_tx_packet_queue_length{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}",
                      "instant": false,
                      "legendFormat": "tx queue",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_rx_packet_queue_length{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}",
                      "instant": false,
                      "legendFormat": "rx queue",
                      "range": true
                   }
                ],
                "title": "Pending packet queue",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "axisCenteredZero": true
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_transport_stream_rx_bytes_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "rx",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "-1 * rate(cluster_transport_stream_tx_bytes_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "tx",
                      "range": true
                   }
                ],
                "title": "Stream bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_stream_tx_packets_failed_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]) /\n  rate(cluster_transport_stream_tx_packets_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])\n  )\n",
                      "instant": false,
                      "legendFormat": "Tx success %",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_stream_rx_packets_failed_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]) /\n  rate(cluster_transport_stream_rx_packets_total{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])\n  )\n",
                      "instant": false,
                      "legendFormat": "Rx success %",
                      "range": true
                   }
                ],
                "title": "Stream write success rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of open connections from this node to its peers.\n\nEach node picks up a subset of its peers to continuously gossip messages around cluster status using streaming HTTP/2 connections. This panel can be used to detect networking failures that result in cluster communication being disrupted and convergence taking longer than expected or outright failing.\n",
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_streams{instance=\"$instance\", cluster=\"$cluster\", namespace=\"$namespace\"}",
                      "instant": false,
                      "legendFormat": "Open streams",
                      "range": true
                   }
                ],
                "title": "Open transport streams",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "instance",
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Cluster Node",
          "uid": "4047e755d822da63c8158cde32ae4dce"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-cluster-node.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-cluster-overview.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Clustering documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/cli/run/#clustered-mode"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 9,
                   "w": 8,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "count(cluster_node_info{cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Nodes",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Nodes info.\n",
                "fieldConfig": {
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Dashboard"
                         },
                         "properties": [
                            {
                               "id": "mappings",
                               "value": [
                                  {
                                     "options": {
                                        "1": {
                                           "index": 0,
                                           "text": "Link"
                                        }
                                     },
                                     "type": "value"
                                  }
                               ]
                            },
                            {
                               "id": "links",
                               "value": [
                                  {
                                     "targetBlank": false,
                                     "title": "Detail dashboard for node",
                                     "url": "/d/4047e755d822da63c8158cde32ae4dce/alloy-cluster-node?var-instance=${__data.fields.instance}&var-datasource=${datasource}&var-loki_datasource=${loki_datasource}&var-cluster=${cluster}&var-namespace=${namespace}"
                                  }
                               ]
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 9,
                   "w": 16,
                   "x": 8,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_node_info{cluster=\"$cluster\", namespace=\"$namespace\"}",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Node table",
                "transformations": [
                   {
                      "id": "organize",
                      "options": {
                         "excludeByName": {
                            "Time": true,
                            "Value": false,
                            "__name__": true,
                            "cluster": true,
                            "namespace": true,
                            "state": false
                         },
                         "indexByName": { },
                         "renameByName": {
                            "Value": "Dashboard",
                            "instance": "",
                            "state": ""
                         }
                      }
                   }
                ],
                "type": "table"
             },
             {
                "datasource": "${datasource}",
                "description": "Whether the cluster state has converged.\n\nIt is normal for the cluster state to be diverged briefly as gossip events propagate. It is not normal for the cluster state to be diverged for a long period of time.\n\nThis will show one of the following:\n\n* Converged: Nodes are aware of all other nodes, with the correct states.\n* Not converged: A subset of nodes aren't aware of their peers, or don't have an updated view of peer states.\n",
                "fieldConfig": {
                   "defaults": {
                      "mappings": [
                         {
                            "options": {
                               "1": {
                                  "color": "red",
                                  "index": 1,
                                  "text": "Not converged"
                               }
                            },
                            "type": "value"
                         },
                         {
                            "options": {
                               "match": "null",
                               "result": {
                                  "color": "green",
                                  "index": 0,
                                  "text": "Converged"
                               }
                            },
                            "type": "special"
                         }
                      ],
                      "unit": "suffix:nodes"
                   }
                },
                "gridPos": {
                   "h": 9,
                   "w": 8,
                   "x": 0,
                   "y": 9
                },
                "options": {
                   "colorMode": "background",
                   "graphMode": "none",
                   "justifyMode": "auto",
                   "orientation": "auto",
                   "reduceOptions": {
                      "calcs": [
                         "lastNotNull"
                      ],
                      "fields": "",
                      "values": false
                   },
                   "textMode": "auto"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "clamp((\n  sum(stddev by (state) (cluster_node_peers{cluster=\"$cluster\", namespace=\"$namespace\"}) != 0) or\n  (sum(abs(sum without (state) (cluster_node_peers{cluster=\"$cluster\", namespace=\"$namespace\"})) - scalar(count(cluster_node_info{cluster=\"$cluster\", namespace=\"$namespace\"})) != 0))\n  ),\n  1, 1\n)\n",
                      "format": "time_series",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Convergance state",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 80,
                         "spanNulls": true
                      },
                      "mappings": [
                         {
                            "options": {
                               "0": {
                                  "color": "green",
                                  "text": "Yes"
                               }
                            },
                            "type": "value"
                         },
                         {
                            "options": {
                               "1": {
                                  "color": "red",
                                  "text": "No"
                               }
                            },
                            "type": "value"
                         }
                      ],
                      "max": 1,
                      "noValue": 0
                   }
                },
                "gridPos": {
                   "h": 9,
                   "w": 16,
                   "x": 8,
                   "y": 9
                },
                "options": {
                   "mergeValues": true
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "ceil(clamp((\n  sum(stddev by (state) (cluster_node_peers{cluster=\"$cluster\", namespace=\"$namespace\"})) or\n  (sum(abs(sum without (state) (cluster_node_peers{cluster=\"$cluster\", namespace=\"$namespace\"})) - scalar(count(cluster_node_info{cluster=\"$cluster\", namespace=\"$namespace\"}))))\n  ),\n  0, 1\n))\n",
                      "instant": false,
                      "legendFormat": "Converged",
                      "range": true
                   }
                ],
                "title": "Convergance state timeline",
                "type": "state-timeline"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Cluster Overview",
          "uid": "3a6b7020692f53d8e53b49196f7637dd"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-cluster-overview.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  config.alloy: "logging {\n\tlevel  = coalesce(env(\"ALLOY_LOG_LEVEL\"), \"warn\")\n\tformat
    = \"logfmt\"\n}\n\n/********************************************\n * Grafana LGTMP
    Stack Receiver Provider\n ********************************************/\nimport.git
    \"provider\" {\n\trepository     = \"https://github.com/qclaogui/codelab-monitoring.git\"\n\trevision
    \      = \"main\"\n\tpath           = \"alloy-modules/provider\"\n\tpull_frequency
    = \"24h\"\n}\n\nprovider.self_hosted_stack \"kubernetes\" {\n\tmetrics_endpoint_url
    \ = coalesce(env(\"SELF_HOSTED_METRICS_ENDPOINT_URL\"), \"http://nginx.gateway.svc:8080/api/v1/push\")\n\tlogs_endpoint_url
    \    = coalesce(env(\"SELF_HOSTED_LOGS_ENDPOINT_URL\"), \"http://nginx.gateway.svc:3100/loki/api/v1/push\")\n\ttraces_endpoint_url
    \  = coalesce(env(\"SELF_HOSTED_TRACES_ENDPOINT_URL\"), \"http://nginx.gateway.svc:4318\")\n\tprofiles_endpoint_url
    = coalesce(env(\"SELF_HOSTED_PROFILES_ENDPOINT_URL\"), \"http://nginx.gateway.svc:4040\")\n}\n\n/********************************************\n
    * Metrics\n ********************************************/\nimport.file \"metrics\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/metrics\"\n}\n\nmetrics.rules_to_mimir \"kubernetes\" { }\n\nmetrics.annotations_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"15s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\nmetrics.servicemonitors_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"15s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n//
    Jobs metrics\nimport.file \"jobs\" {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"),
    \"/etc/alloy/modules\") + \"/kubernetes/jobs\"\n}\n\njobs.kubelet_metrics_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"30s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\njobs.kube_state_metrics_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"30s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\njobs.node_exporter_metrics_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"30s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n/********************************************\n
    * Logs\n ********************************************/\nimport.file \"logs\" {\n\tfilename
    = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\") + \"/kubernetes/logs\"\n}\n\nlogs.rules_to_loki
    \"kubernetes\" { }\n\nlogs.kubernetes_cluster_events \"kubernetes\" {\n\tcluster
    = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\n\tforward_to = [logs.keep_labels.kubernetes.receiver]\n}\n\nlogs.annotations_scrape
    \"kubernetes\" {\n\tcluster = \"k3d-k3s-codelab\"\n\n\tforward_to = [logs.keep_labels.kubernetes.receiver]\n}\n\nlogs.keep_labels
    \"kubernetes\" {\n\tforward_to = [provider.self_hosted_stack.kubernetes.logs_receiver]\n}\n\n/********************************************\n
    * Traces\n ********************************************/\nimport.file \"traces\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/traces\"\n}\n\n// traces Processing And Transformation process_and_transform\ntraces.process_and_transform
    \"kubernetes\" {\n\tcluster = coalesce(env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\n\tmetrics_forward_to
    = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n\tlogs_forward_to
    \   = [provider.self_hosted_stack.kubernetes.logs_receiver]\n\ttraces_forward_to
    \ = [provider.self_hosted_stack.kubernetes.traces_receiver]\n}\n\ntracing {\n\t//
    Write all spans. Don't do this in production!\n\tsampling_fraction = 1\n\n\t//
    Forward Alloy internal spans to traces process.\n\twrite_to = [traces.process_and_transform.kubernetes.alloy_traces_input]\n}\n\n/********************************************\n
    * Profiles\n ********************************************/\nimport.file \"profiles\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/profiles\"\n}\n\nprofiles.annotations_scrape \"kubernetes\" {\n\tcluster
    = \"k3d-k3s-codelab\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.profiles_receiver]\n}\n\n//
    Alloy integration metrics\nremote.kubernetes.configmap \"integrations\" {\n\tnamespace
    = \"monitoring-system\"\n\tname      = \"alloy-integrations\"\n}\n\n// Memcached
    Integrations\nimport.string \"memcached\" {\n\tcontent = remote.kubernetes.configmap.integrations.data[\"memcached.alloy\"]\n}\n\nmemcached.memcached_metrics_scrape
    \"instance\" {\n\tnamespace = \"monitoring-system\"\n\tname      = remote.kubernetes.configmap.integrations.data[\"MEMCACHED_K8S_SECRET_NAME\"]\n\n\tforward_to
    = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n// // Redis Integrations\n//
    import.string \"redis\" {\n// \tcontent = remote.kubernetes.configmap.integrations.data[\"redis.alloy\"]\n//
    }\n\n// redis.redis_exporter_metrics_scrape \"instance\" {\n// \tnamespace = \"monitoring-system\"\n//
    \tname      = remote.kubernetes.configmap.integrations.data[\"REDIS_K8S_SECRET_NAME\"]\n\n//
    \tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n// }\n\n//
    // Mysql Integrations\n// import.string \"mysql\" {\n// \tcontent = remote.kubernetes.configmap.integrations.data[\"mysql.alloy\"]\n//
    }\n\n// mysql.mysql_metrics_scrape \"instance\" {\n// \tnamespace = \"monitoring-system\"\n//
    \tname      = remote.kubernetes.configmap.integrations.data[\"MYSQL_K8S_SECRET_NAME\"]\n\n//
    \tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n// }\n"
kind: ConfigMap
metadata:
  name: alloy-config-mt4f8cf687
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-controller.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Component controller documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/concepts/component_controller/"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "description": "The number of Alloy instances whose metrics are being sent and reported.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "instances"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 0
                },
                "options": {
                   "colorMode": "none",
                   "graphMode": "none"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "count(alloy_component_controller_evaluating{cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Running instances",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of running components across all running instances.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "components"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 4
                },
                "options": {
                   "colorMode": "none",
                   "graphMode": "none"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"})",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Running components",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentage of components which are in a healthy state.\n",
                "fieldConfig": {
                   "defaults": {
                      "max": 1,
                      "min": 0,
                      "noValue": "No components",
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 8
                },
                "options": {
                   "colorMode": "value",
                   "graphMode": "area",
                   "text": {
                      "valueSize": 80
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\",health_type=\"healthy\"}) /\nsum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"})\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Overall component health",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Breakdown of components by health across all running instances.\n\n* Healthy: components have been evaluated completely and are reporting themselves as healthy.\n* Unhealthy: Components either could not be evaluated or are reporting themselves as unhealthy.\n* Unknown: A component has been created but has not yet been started.\n* Exited: A component has exited. It will not return to the running state.\n\nMore information on a component's health state can be retrieved using\nthe Alloy UI.\n\nNote that components may be in a degraded state even if they report\nthemselves as healthy. Use component-specific dashboards and alerts\nto observe detailed information about the behavior of a component.\n",
                "fieldConfig": {
                   "defaults": {
                      "min": 0,
                      "thresholds": {
                         "mode": "absolute",
                         "steps": [
                            {
                               "color": "green",
                               "value": null
                            }
                         ]
                      }
                   },
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Unhealthy"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "red",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Unknown"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "blue",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Exited"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "orange",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 12,
                   "w": 14,
                   "x": 10,
                   "y": 0
                },
                "options": {
                   "orientation": "vertical",
                   "showUnfilled": true
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\", health_type=\"healthy\"}) or vector(0)",
                      "instant": true,
                      "legendFormat": "Healthy",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\", health_type=\"unhealthy\"}) or vector(0)",
                      "instant": true,
                      "legendFormat": "Unhealthy",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\", health_type=\"unknown\"}) or vector(0)",
                      "instant": true,
                      "legendFormat": "Unknown",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\", health_type=\"exited\"}) or vector(0)",
                      "instant": true,
                      "legendFormat": "Exited",
                      "range": false
                   }
                ],
                "title": "Components by health",
                "type": "bargauge"
             },
             {
                "datasource": "${datasource}",
                "description": "The frequency at which components get updated.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "drawStyle": "points",
                         "pointSize": 3
                      },
                      "unit": "ops"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 12
                },
                "options": {
                   "tooltip": {
                      "mode": "multi"
                   }
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (instance) (rate(alloy_component_evaluation_seconds_count{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Component evaluation rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentiles for how long it takes to complete component evaluations.\n\nComponent evaluations must complete for components to have the latest\narguments. The longer the evaluations take, the slower it will be to\nreconcile the state of components.\n\nIf evaluation is taking too long, consider sharding your components to\ndeal with smaller amounts of data and reuse data as much as possible.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.99, sum(rate(alloy_component_evaluation_seconds{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])))\nor\nhistogram_quantile(0.99, sum by (le) (rate(alloy_component_evaluation_seconds_bucket{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "99th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.50, sum(rate(alloy_component_evaluation_seconds{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])))\nor\nhistogram_quantile(0.50, sum by (le) (rate(alloy_component_evaluation_seconds_bucket{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "50th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "(\n  histogram_sum(sum(rate(alloy_component_evaluation_seconds{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval]))) /\n  histogram_count(sum(rate(alloy_component_evaluation_seconds{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])))\n)\nor\n(\n  sum(rate(alloy_component_evaluation_seconds_sum{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval])) /\n  sum(rate(alloy_component_evaluation_seconds_count{cluster=\"$cluster\",namespace=\"$namespace\"}[$__rate_interval]))\n)\n",
                      "instant": false,
                      "legendFormat": "Average",
                      "range": true
                   }
                ],
                "title": "Component evaluation time",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentage of time spent evaluating 'slow' components - components that took longer than 1 minute to evaluate.\n\nIdeally, no component should take more than 1 minute to evaluate. The components displayed in this chart\nmay be a sign of a problem with the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (component_path, component_id) (rate(alloy_component_evaluation_slow_seconds{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))\n/ scalar(sum(rate(alloy_component_evaluation_seconds_sum{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "{{component path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Slow components evaluation times",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Detailed histogram view of how long component evaluations take.\n\nThe goal is to design your config so that evaluations take as little\ntime as possible; under 100ms is a good goal.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 22
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "cellGap": 0,
                   "color": {
                      "scheme": "Spectral"
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 0.10000000000000001
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "s"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(increase(alloy_component_evaluation_seconds{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))\nor ignoring (le)\nsum by (le) (increase(alloy_component_evaluation_seconds_bucket{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Component evaluation histogram",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "description": "Detailed histogram of how long components wait to be evaluated after their dependency is updated.\n\nThe goal is to design your config so that most of the time components do not\nqueue for long; under 10ms is a good goal.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 22
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "cellGap": 0,
                   "color": {
                      "scheme": "Spectral"
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 0.10000000000000001
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "s"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(increase(alloy_component_dependencies_wait_seconds{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))\nor ignoring (le)\nsum by (le) (increase(alloy_component_dependencies_wait_seconds_bucket{cluster=\"$cluster\", namespace=\"$namespace\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Component dependency wait histogram",
                "type": "heatmap"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Controller",
          "uid": "bf9f456aad7108b2c808dbd9973e386f"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-controller.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  MEMCACHED_K8S_SECRET_NAME: alloy-integrations-memcached
  MYSQL_K8S_SECRET_NAME: alloy-integrations-mysql
  REDIS_K8S_SECRET_NAME: alloy-integrations-redis
  memcached.alloy: "/*\nModule Components: component_memcached\n*/\n\ndeclare \"memcached_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/memcached)\"\n\t\toptional =
    true\n\t}\n\n\targument \"namespace\" {\n\t\tcomment  = \"kubernetes secret name
    (default: monitoring-system)\"\n\t\toptional = true\n\t}\n\n\targument \"name\"
    {\n\t\tcomment  = \"kubernetes secret name (default: alloy-integrations-redis)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\toptional = true\n\t\tdefault
    \ = \"(up|memcached_commands_total|memcached_connections_total|memcached_current_bytes|memcached_current_connections|memcached_current_items|memcached_items_evicted_total|memcached_items_total|memcached_max_connections|memcached_read_bytes_total|memcached_up|memcached_uptime_seconds|memcached_version|memcached_written_bytes_total)\"\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\tremote.kubernetes.secret
    \"memcached\" {\n\t\tnamespace = coalesce(argument.namespace.value, \"monitoring-system\")\n\t\tname
    \     = coalesce(argument.name.value, \"alloy-integrations-memcached\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Memcached\n\t****************************************************************/\n\t//
    https://grafana.com/docs/alloy/latest/reference/components/prometheus.exporter.memcached/\n\tprometheus.exporter.memcached
    \"integrations_memcached_exporter\" {\n\t\taddress = nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-address\"])\n\t\ttimeout
    = nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-timeout\"])\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_memcached_exporter\" {\n\t\ttargets = prometheus.exporter.memcached.integrations_memcached_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/memcached\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label
    = \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_memcached_exporter\" {\n\t\ttargets = concat(\n\t\t\tdiscovery.relabel.integrations_memcached_exporter.output,\n\t\t)\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tscrape_interval = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.integrations_memcached_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_memcached_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = argument.keep_metrics.value\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  mysql.alloy: "/*\nModule Components: component_mysql\n*/\n\ndeclare \"mysql_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/mysql)\"\n\t\toptional = true\n\t}\n\n\targument
    \"namespace\" {\n\t\tcomment  = \"kubernetes secret namespace (default: monitoring-system)\"\n\t\toptional
    = true\n\t}\n\n\targument \"name\" {\n\t\tcomment  = \"kubernetes secret name
    (default: alloy-integrations-mysql)\"\n\t\toptional = true\n\t}\n\n\targument
    \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep (default: see
    below)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment
    \ = \"How often to scrape metrics from the targets (default: 60s)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment  = \"How long before
    a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\tremote.kubernetes.secret
    \"mysql\" {\n\t\tname      = coalesce(argument.name.value, \"alloy-integrations-mysql\")\n\t\tnamespace
    = coalesce(argument.namespace.value, \"monitoring-system\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Mysql\n\t****************************************************************/\n\tprometheus.exporter.mysql
    \"integrations_mysqld_exporter\" {\n\t\tdata_source_name = nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-username\"])
    + \":\" + nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-password\"])
    + \"@(\" + nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-host\"]) +
    \")/\"\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_mysqld_exporter\" {\n\t\ttargets = prometheus.exporter.mysql.integrations_mysqld_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/mysql\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label =
    \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.mysql.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_mysqld_exporter\" {\n\t\ttargets = concat(\n\t\t\tdiscovery.relabel.integrations_mysqld_exporter.output,\n\t\t)\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tscrape_interval = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.integrations_mysqld_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_mysqld_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(up|instance:mysql_heartbeat_lag_seconds|instance:mysql_slave_lag_seconds|mysql_global_status_aborted_clients|mysql_global_status_aborted_connects|mysql_global_status_buffer_pool_pages|mysql_global_status_bytes_received|mysql_global_status_bytes_sent|mysql_global_status_commands_total|mysql_global_status_created_tmp_disk_tables|mysql_global_status_created_tmp_files|mysql_global_status_created_tmp_tables|mysql_global_status_handlers_total|mysql_global_status_innodb_log_waits|mysql_global_status_innodb_mem_adaptive_hash|mysql_global_status_innodb_mem_dictionary|mysql_global_status_innodb_num_open_files|mysql_global_status_innodb_page_size|mysql_global_status_max_used_connections|mysql_global_status_open_files|mysql_global_status_open_table_definitions|mysql_global_status_open_tables|mysql_global_status_opened_files|mysql_global_status_opened_table_definitions|mysql_global_status_opened_tables|mysql_global_status_qcache_free_memory|mysql_global_status_qcache_hits|mysql_global_status_qcache_inserts|mysql_global_status_qcache_lowmem_prunes|mysql_global_status_qcache_not_cached|mysql_global_status_qcache_queries_in_cache|mysql_global_status_queries|mysql_global_status_questions|mysql_global_status_select_full_join|mysql_global_status_select_full_range_join|mysql_global_status_select_range|mysql_global_status_select_range_check|mysql_global_status_select_scan|mysql_global_status_slow_queries|mysql_global_status_sort_merge_passes|mysql_global_status_sort_range|mysql_global_status_sort_rows|mysql_global_status_sort_scan|mysql_global_status_table_locks_immediate|mysql_global_status_table_locks_waited|mysql_global_status_table_open_cache_hits|mysql_global_status_table_open_cache_misses|mysql_global_status_table_open_cache_overflows|mysql_global_status_threads_cached|mysql_global_status_threads_connected|mysql_global_status_threads_created|mysql_global_status_threads_running|mysql_global_status_uptime|mysql_global_status_wsrep_local_recv_queue|mysql_global_status_wsrep_local_state|mysql_global_status_wsrep_ready|mysql_global_variables_innodb_additional_mem_pool_size|mysql_global_variables_innodb_buffer_pool_size|mysql_global_variables_innodb_log_buffer_size|mysql_global_variables_key_buffer_size|mysql_global_variables_max_connections|mysql_global_variables_open_files_limit|mysql_global_variables_query_cache_size|mysql_global_variables_table_definition_cache|mysql_global_variables_table_open_cache|mysql_global_variables_thread_cache_size|mysql_global_variables_tokudb_cache_size|mysql_global_variables_wsrep_desync|mysql_heartbeat_now_timestamp_seconds|mysql_heartbeat_stored_timestamp_seconds|mysql_info_schema_processlist_threads|mysql_slave_status_seconds_behind_master|mysql_slave_status_slave_io_running|mysql_slave_status_slave_sql_running|mysql_slave_status_sql_delay|mysql_up)\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  redis.alloy: "/*\nModule Components: component_redis_exporter\n*/\n\ndeclare \"redis_exporter_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/redis_exporter)\"\n\t\toptional
    = true\n\t}\n\n\targument \"namespace\" {\n\t\tcomment  = \"kubernetes secret
    name (default: monitoring-system)\"\n\t\toptional = true\n\t}\n\n\targument \"name\"
    {\n\t\tcomment  = \"kubernetes secret name (default: alloy-integrations-redis)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\toptional = true\n\t\tdefault
    \ = \"(up|redis_blocked_clients|redis_cluster_slots_fail|redis_cluster_slots_pfail|redis_cluster_state|redis_commands_duration_seconds_total|redis_commands_total|redis_connected_clients|redis_connected_slaves|redis_db_keys|redis_db_keys_expiring|redis_evicted_keys_total|redis_keyspace_hits_total|redis_keyspace_misses_total|redis_master_last_io_seconds_ago|redis_memory_fragmentation_ratio|redis_memory_max_bytes|redis_memory_used_bytes|redis_memory_used_rss_bytes|redis_total_system_memory_bytes|redis_up)\"\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t\tdefault  = \"60s\"\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t\tdefault  = \"10s\"\n\t}\n\n\tremote.kubernetes.secret
    \"redis\" {\n\t\tnamespace = coalesce(argument.namespace.value, \"monitoring-system\")\n\t\tname
    \     = coalesce(argument.name.value, \"alloy-integrations-redis\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Redis\n\t****************************************************************/\n\tprometheus.exporter.redis
    \"integrations_redis_exporter\" {\n\t\tredis_addr     = nonsensitive(remote.kubernetes.secret.redis.data[\"instance-address\"])\n\t\tredis_password
    = nonsensitive(remote.kubernetes.secret.redis.data[\"instance-password\"])\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_redis_exporter\" {\n\t\ttargets = prometheus.exporter.redis.integrations_redis_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/redis_exporter\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label
    = \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.redis.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_redis_exporter\" {\n\t\ttargets = concat(\n\t\t\tdiscovery.relabel.integrations_redis_exporter.output,\n\t\t)\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tscrape_interval = argument.scrape_interval.value\n\t\tscrape_timeout
    \ = argument.scrape_timeout.value\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to
    = [prometheus.relabel.integrations_redis_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_redis_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = argument.keep_metrics.value\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-integrations
  namespace: monitoring-system
---
apiVersion: v1
data:
  apiserver.alloy: "/*\nModule Components: apiserver\nDescription: kubernetes Apiserver
    Metrics Scrape\n\n*/\n\ndeclare \"apiserver_metrics_scrape\" {\n\n\t/********************************************\n\t*
    ARGUMENTS\n\t********************************************/\n\targument \"forward_to\"
    {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected metrics should
    be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument \"namespaces\"
    {\n\t\tcomment  = \"The namespaces to look for targets in (default: default)\"\n\t\toptional
    = true\n\t}\n\n\targument \"field_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"metadata.name=kubernetes\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The value of the label
    for the selector (default: https)\"\n\t\toptional = true\n\t}\n\n\targument \"job_label\"
    {\n\t\tcomment  = \"The job label to add for all kube-apiserver metrics (default:
    integrations/kubernetes/apiserver)\"\n\t\toptional = true\n\t}\n\n\targument \"keep_metrics\"
    {\n\t\tcomment  = \"A regex of metrics to keep (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\t// drop metrics and les from kube-prometheus\n\t// https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml\n\targument
    \"drop_metrics\" {\n\t\tcomment  = \"A regular expression of metrics to drop (default:
    see below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_les\" {\n\t\tcomment
    \ = \"Regular expression of metric les label values to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"apiserver\" {\n\t\trole = \"service\"\n\n\t\tselectors {\n\t\t\trole  = \"service\"\n\t\t\tfield
    = join(coalesce(argument.field_selectors.value, [\"metadata.name=kubernetes\"]),
    \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value, []), \",\")\n\t\t}\n\n\t\tnamespaces
    {\n\t\t\tnames = coalesce(argument.namespaces.value, [\"default\"])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"apiserver\" {\n\t\ttargets = discovery.kubernetes.apiserver.targets\n\n\t\t//
    only keep targets with a matching port name\n\t\trule {\n\t\t\tsource_labels =
    [\"__meta_kubernetes_service_port_name\"]\n\t\t\tregex         = coalesce(argument.port_name.value,
    \"https\")\n\t\t\taction        = \"keep\"\n\t\t}\n\n\t\t// set the namespace\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the service_name\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_service_name\"]\n\t\t\ttarget_label
    \ = \"service\"\n\t\t}\n\n\t\t// set the app name if specified as metadata labels
    \"app:\" or \"app.kubernetes.io/name:\" or \"k8s-app:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_service_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_service_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_service_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"apiserver\" {\n\t\ttargets = discovery.relabel.apiserver.output\n\n\t\tjob_name
    \         = coalesce(argument.job_label.value, \"integrations/kubernetes/apiserver\")\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.apiserver.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"apiserver\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(((go|process)_.+)|kubelet_node_name|kubelet_(pod_(worker|start)_latency_microseconds|cgroup_manager_latency_microseconds|pleg_relist_(latency|interval)_microseconds|runtime_operations(_latency_microseconds|_errors)?|eviction_stats_age_microseconds|device_plugin_(registration_count|alloc_latency_microseconds)|network_plugin_operations_latency_microseconds)|scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_(predicate|priority|preemption)_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)|apiserver_(request_(count|latencies(_summary)?)|dropped_requests|storage_(data_key_generation|transformation_(failures_total|latencies_microseconds))|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)|kubelet_docker_(operations(_latency_microseconds|_errors|_timeout)?)|reflector_(items_per_(list|watch)|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)|etcd_(helper_(cache_(hit|miss)_count|cache_entry_count|object_counts)|request_(cache_(get|add)_latencies_summary|latencies_summary)|debugging.*|disk.*|server.*)|transformation_(latencies_microseconds|failures_total)|(admission_quota_controller|APIServiceOpenAPIAggregationControllerQueue1|APIServiceRegistrationController|autoregister|AvailableConditionController|crd_(autoregistration_controller|Establishing|finalizer|naming_condition_controller|openapi_controller)|DiscoveryController|non_structural_schema_condition_controller|kubeproxy_sync_proxy_rules|rest_client_request_latency|storage_operation_(errors_total|status_count))(_.*)|apiserver_admission_(controller_admission|step_admission)_latencies_seconds_.*)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// drop metrics whose name and le label match
    the drop_les regex\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__name__\",\n\t\t\t\t\"le\",\n\t\t\t]\n\t\t\tregex
    \ = coalesce(argument.drop_les.value, \"apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)\")\n\t\t\taction
    = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics regex\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(.+)\")\n\t\t\taction        = \"keep\"\n\t\t}\n\t}\n}\n"
  kube-state-metrics.alloy: "/*\nModule Components: kube_state_metrics\nDescription:
    kubernetes kube_state_metrics Metrics Scrape\n\n*/\n\ndeclare \"kube_state_metrics_scrape\"
    {\n\n\t/********************************************\n\t* ARGUMENTS\n\t********************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"namespaces\" {\n\t\tcomment  = \"The namespaces to look for targets in (default:
    [] is all namespaces)\"\n\t\toptional = true\n\t}\n\n\targument \"field_selectors\"
    {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"app.kubernetes.io/name=kube-state-metrics\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The of the port to
    scrape metrics from (default: http)\"\n\t\toptional = true\n\t}\n\n\targument
    \"job_label\" {\n\t\tcomment  = \"The job label to add for all kube_state_metrics
    metrics (default: integrations/kubernetes/kube-state-metrics)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics
    to keep (default: see below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\"
    {\n\t\tcomment  = \"A regular expression of metrics to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Service Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"kube_state_metrics\" {\n\t\trole = \"service\"\n\n\t\tselectors {\n\t\t\trole
    \ = \"service\"\n\t\t\tfield = join(coalesce(argument.field_selectors.value, []),
    \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value, [\"app.kubernetes.io/name=kube-state-metrics\"]),
    \",\")\n\t\t}\n\n\t\tnamespaces {\n\t\t\tnames = coalesce(argument.namespaces.value,
    [])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"kube_state_metrics\" {\n\t\ttargets = discovery.kubernetes.kube_state_metrics.targets\n\n\t\t//
    only keep targets with a matching port name\n\t\trule {\n\t\t\tsource_labels =
    [\"__meta_kubernetes_service_port_name\"]\n\t\t\tregex         = coalesce(argument.port_name.value,
    \"http\")\n\t\t\taction        = \"keep\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"kube_state_metrics\" {\n\t\ttargets = discovery.relabel.kube_state_metrics.output\n\n\t\tjob_name
    \       = coalesce(argument.job_label.value, \"integrations/kubernetes/kube-state-metrics\")\n\t\tscrape_interval
    = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value,
    \"10s\")\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to =
    [prometheus.relabel.kube_state_metrics.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"kube_state_metrics\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go|process)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(up|kube_(daemonset.*|deployment_(metadata_generation|spec_replicas|status_(observed_generation|replicas_(available|updated)))|horizontalpodautoscaler_(spec_(max|min)_replicas|status_(current|desired)_replicas)|job.*|namespace_status_phase|node.*|persistentvolumeclaim_resource_requests_storage_bytes|pod_(container_(info|resource_(limits|requests)|status_(last_terminated_reason|restarts_total|waiting_reason))|info|owner|start_time|status_(phase|reason))|replicaset.*|resourcequota|statefulset.*))\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  kubelet.alloy: "/*\nModule Components: kubelet\nDescription: kubernetes kubelet
    Metrics Scrape\n\n*/\n\ndeclare \"kubelet_metrics_scrape\" {\n\n\t/********************************************\n\t*
    ARGUMENTS\n\t********************************************/\n\targument \"forward_to\"
    {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected metrics should
    be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument \"keep_metrics\"
    {\n\t\tcomment  = \"A regex of metrics to keep (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment  = \"A regular expression
    of metrics to drop (default: see below)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"max_cache_size\" {\n\t\tcomment  = \"The maximum number of elements to hold
    in the relabeling cache (default: 100000).  This should be at least 2x-5x your
    largest scrape target or samples appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"node\" {\n\t\trole = \"node\"\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"node\" {\n\t\ttargets = discovery.kubernetes.node.targets\n\n\t\t// set the
    address to use the kubernetes service dns name\n\t\trule {\n\t\t\ttarget_label
    = \"__address__\"\n\t\t\treplacement  = \"kubernetes.default.svc.cluster.local:443\"\n\t\t}\n\n\t\t//
    set the node label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\ttarget_label
    \ = \"node\"\n\t\t}\n\n\t\t// set the app name if specified as metadata labels
    \"app:\" or \"app.kubernetes.io/name:\" or \"k8s-app:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_service_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_service_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_service_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubelet\" {\n\t\ttargets = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label
    = \"job\"\n\t\t\treplacement  = \"integrations/kubernetes/kubelet\"\n\t\t}\n\n\t\t//
    set the metrics path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         =
    \"(.+)\"\n\t\t\treplacement   = \"/api/v1/nodes/${1}/proxy/metrics\"\n\t\t\ttarget_label
    \ = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"resources\" {\n\t\ttargets
    = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement
    \ = \"integrations/kubernetes/kube-resources\"\n\t\t}\n\n\t\t// set the metrics
    path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         = \"(.+)\"\n\t\t\treplacement
    \  = \"/api/v1/nodes/${1}/proxy/metrics/resource\"\n\t\t\ttarget_label  = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"probes\" {\n\t\ttargets = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label
    = \"job\"\n\t\t\treplacement  = \"integrations/kubernetes/kube-probes\"\n\t\t}\n\n\t\t//
    set the metrics path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         =
    \"(.+)\"\n\t\t\treplacement   = \"/api/v1/nodes/${1}/proxy/metrics/probes\"\n\t\t\ttarget_label
    \ = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"cadvisor\" {\n\t\ttargets
    = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement
    \ = \"integrations/kubernetes/cadvisor\"\n\t\t}\n\n\t\t// set the metrics path
    to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         = \"(.+)\"\n\t\t\treplacement
    \  = \"/api/v1/nodes/${1}/proxy/metrics/cadvisor\"\n\t\t\ttarget_label  = \"__metrics_path__\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"kubelet\" {\n\t\ttargets = concat(\n\t\t\tdiscovery.relabel.kubelet.output,\n\t\t\tdiscovery.relabel.resources.output,\n\t\t\tdiscovery.relabel.probes.output,\n\t\t\tdiscovery.relabel.cadvisor.output,\n\t\t)\n\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.kubelet.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"kubelet\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go|process)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t\taction        = \"keep\"\n\t\t}\n\t\t//
    Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"container\"]\n\t\t\tseparator     =
    \"@\"\n\t\t\tregex         = \"(container_cpu_.*|container_fs_.*|container_memory_.*)@\"\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"image\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex
    \        = \"(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@\"\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// Normalizing unimportant labels (not deleting
    to continue satisfying <label>!=\"\" checks)\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"boot_id\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"machine_memory_bytes@.*\"\n\t\t\ttarget_label  = \"boot_id\"\n\t\t\treplacement
    \  = \"NA\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__name__\", \"system_uuid\"]\n\t\t\tseparator
    \    = \"@\"\n\t\t\tregex         = \"machine_memory_bytes@.*\"\n\t\t\ttarget_label
    \ = \"system_uuid\"\n\t\t\treplacement   = \"NA\"\n\t\t}\n\n\t\t// Filter out
    non-physical devices/interfaces\n\t\trule {\n\t\t\tsource_labels = [\"__name__\",
    \"device\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         = \"container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"__keepme\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_fs_.*@\"\n\t\t\taction        = \"drop\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = \"container_fs_.*\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"interface\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"__keepme\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_network_.*@\"\n\t\t\taction        = \"drop\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = \"container_network_.*\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"\"\n\t\t}\n\t}\n}\n"
  node-exporter.alloy: "/*\nModule Components: node_exporter\nDescription: kubernetes
    node_exporter Metrics Scrape\n\n*/\n\ndeclare \"node_exporter_metrics_scrape\"
    {\n\n\t/********************************************\n\t* ARGUMENTS\n\t********************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"namespaces\" {\n\t\tcomment  = \"The namespaces to look for targets in (default:
    [] is all namespaces)\"\n\t\toptional = true\n\t}\n\n\targument \"field_selectors\"
    {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"app.kubernetes.io/name=prometheus-node-exporter\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The of the port to
    scrape metrics from (default: metrics)\"\n\t\toptional = true\n\t}\n\n\targument
    \"job_label\" {\n\t\tcomment  = \"The job label to add for all node-exporter metrics
    (default: integrations/kubernetes/node-exporter)\"\n\t\toptional = true\n\t}\n\n\targument
    \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep (default: see
    below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regular expression of metrics to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Service Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"node_exporter\" {\n\t\trole = \"pod\"\n\n\t\tselectors {\n\t\t\trole  = \"pod\"\n\t\t\tfield
    = join(coalesce(argument.field_selectors.value, []), \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value,
    [\"app.kubernetes.io/name=prometheus-node-exporter\"]), \",\")\n\t\t}\n\n\t\tnamespaces
    {\n\t\t\tnames = coalesce(argument.namespaces.value, [])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"node_exporter\" {\n\t\ttargets = discovery.kubernetes.node_exporter.targets\n\n\t\t//
    keep only the specified metrics port name, and pods that are Running and ready\n\t\trule
    {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_container_port_name\",\n\t\t\t\t\"__meta_kubernetes_pod_phase\",\n\t\t\t\t\"__meta_kubernetes_pod_ready\",\n\t\t\t\t\"__meta_kubernetes_pod_container_init\",\n\t\t\t]\n\t\t\tseparator
    = \"@\"\n\t\t\tregex     = coalesce(argument.port_name.value, \"metrics\") + \"@Running@true@false\"\n\t\t\taction
    \   = \"keep\"\n\t\t}\n\n\t\t// set the namespace label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label  = \"namespace\"\n\t\t}\n\n\t\t//
    set the pod label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label
    \ = \"pod\"\n\t\t}\n\n\t\t// set the container label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label  = \"container\"\n\t\t}\n\n\t\t//
    set a workload label\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \"/\"\n\t\t\ttarget_label = \"workload\"\n\t\t}\n\t\t// remove the hash
    from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels = [\"workload\"]\n\t\t\tregex
    \        = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label  = \"workload\"\n\t\t}\n\n\t\t//
    set the app name if specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"
    or \"k8s-app:\"\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the component if specified
    as metadata labels \"component:\" or \"app.kubernetes.io/component:\" or \"k8s-component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule {\n\t\t\taction
    \      = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"node_exporter\" {\n\t\ttargets = discovery.relabel.node_exporter.output\n\n\t\tjob_name
    \       = coalesce(argument.job_label.value, \"integrations/kubernetes/node-exporter\")\n\t\tscrape_interval
    = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value,
    \"10s\")\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to =
    [prometheus.relabel.node_exporter.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"node_exporter\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(up|node_exporter_build_info|scrape_(duration_seconds|series_added|samples_(post_metric_relabeling|scraped))|node_(arp_entries|boot_time_seconds|context_switches_total|cpu_seconds_total|disk_(io_time_seconds_total|io_time_weighted_seconds_total|read_(bytes_total|time_seconds_total)|reads_completed_total|write_time_seconds_total|writes_completed_total|written_bytes_total)|file(fd_(allocated|maximum)|system_(avail_bytes|device_error|files(_free)?|readonly|size_bytes))|intr_total|load(1|15|5)|md_disks(_required)?|memory_(Active_(anon_bytes|bytes|file_bytes)|Anon(HugePages_bytes|Pages_bytes)|Bounce_bytes|Buffers_bytes|Cached_bytes|CommitLimit_bytes|Committed_AS_bytes|DirectMap(1G|2M|4k)_bytes|Dirty_bytes|HugePages_(Free|Rsvd|Surp|Total)|Hugepagesize_bytes|Inactive_(anon_bytes|bytes|file_bytes)|Mapped_bytes|Mem(Available|Free|Total)_bytes|S(Reclaimable|Unreclaim)_bytes|Shmem(HugePages_bytes|PmdMapped_bytes|_bytes)|Slab_bytes|SwapTotal_bytes|Vmalloc(Chunk|Total|Used)_bytes|Writeback(Tmp|)_bytes)|netstat_(Icmp6_(InErrors|InMsgs|OutMsgs)|Icmp_(InErrors|InMsgs|OutMsgs)|IpExt_(InOctets|OutOctets)|TcpExt_(Listen(Drops|Overflows)|TCPSynRetrans)|Tcp_(InErrs|InSegs|OutRsts|OutSegs|RetransSegs)|Udp6_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors)|Udp(Lite|)_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors))|network_(carrier|info|mtu_bytes|receive_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total)|speed_bytes|transmit_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total|queue_length)|up)|nf_conntrack_(entries(_limit)?|limit)|os_info|sockstat_(FRAG6|FRAG|RAW6|RAW|TCP6|TCP_(alloc|inuse|mem(_bytes)?|orphan|tw)|UDP6|UDPLITE6|UDPLITE|UDP_(inuse|mem(_bytes)?)|sockets_used)|softnet_(dropped_total|processed_total|times_squeezed_total)|systemd_unit_state|textfile_scrape_error|time_zone_offset_seconds|timex_(estimated_error_seconds|maxerror_seconds|offset_seconds|sync_status)|uname_info|vmstat_(oom_kill|pgfault|pgmajfault|pgpgin|pgpgout|pswpin|pswpout)|process_(max_fds|open_fds)))\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\n\t\t// Drop metrics for certain file systems\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"fstype\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex
    \        = \"node_filesystem.*@(tempfs)\"\n\t\t\taction        = \"drop\"\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-jobs-749kdbk475
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for logs based on kubernetes Pod annotations\n\n  Annotations:\n
    \   logs.grafana.com/scrape: true\n    logs.grafana.com/tenant: \"primary\"\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  =
    \"The tenant to filter logs to.  This does not have to be the tenantId, this is
    the value to look for in the logs.grafana.com/tenant annotation, and this can
    be a regex.\"\n\t\toptional = true\n\t\tdefault  = \".*\"\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"annotation_prefix\" {\n\t\tcomment  = \"The annotation_prefix
    to use (default: logs.grafana.com)\"\n\t\tdefault  = \"logs.grafana.com\"\n\t\toptional
    = true\n\t}\n\n\targument \"__sd_annotation\" {\n\t\toptional = true\n\t\tcomment
    \ = \"The logic is used to transform the annotation argument into a valid label
    name by removing unsupported characters.\"\n\t\tdefault  = replace(replace(replace(coalesce(argument.annotation_prefix.value,
    \"logs.grafana.com\"), \".\", \"_\"), \"/\", \"_\"), \"-\", \"_\")\n\t}\n\n\t//
    find all pods\n\tdiscovery.kubernetes \"annotation_logs\" {\n\t\trole = \"pod\"\n\t}\n\n\t//
    filter logs by kubernetes annotations\n\tdiscovery.relabel \"annotation_logs_filter\"
    {\n\t\ttargets = discovery.kubernetes.annotation_logs.targets\n\n\t\t// allow
    pods to declare their logs to be ingested or not, the default is true\n\t\t//
    \  i.e. logs.grafana.com/scrape: false\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\" + argument.__sd_annotation.value
    + \"_scrape\",\n\t\t\t]\n\t\t\tregex = \"^(true|)$\"\n\t\t}\n\n\t\t// allow pods
    to declare what tenant their logs should be written to, the following annotation
    is supported:\n\t\t//   logs.grafana.com/tenant: \"primary\"\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\"
    + argument.__sd_annotation.value + \"_tenant\",\n\t\t\t]\n\t\t\tregex = \"^(\"
    + argument.tenant.value + \")$\"\n\t\t}\n\n\t\t// set the instance label as the
    name of the worker node the pod is on\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_node_name\"]\n\t\t\ttarget_label  = \"instance\"\n\t\t}\n\n\t\t//
    set the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    set the namespace label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the pod label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\t//
    set the container label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\n\t\t// set a workload label\n\t\trule {\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \"/\"\n\t\t\ttarget_label = \"workload\"\n\t\t}\n\t\t// remove the hash
    from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels = [\"workload\"]\n\t\t\tregex
    \        = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label  = \"workload\"\n\t\t}\n\n\t\t//
    set the app name if specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"
    or \"k8s-app:\"\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the component if specified
    as metadata labels \"component:\" or \"app.kubernetes.io/component:\" or \"k8s-component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the version if specified as metadata labels
    \"version:\" or \"app.kubernetes.io/version:\" or \"app_version:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app_version\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"version\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\n\t\t//
    set the job label to be namespace / friendly pod name\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"workload\",\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t]\n\t\t\tregex
    \       = \".+\\\\/(.+);(.+)\"\n\t\t\treplacement  = \"$2/$1\"\n\t\t\ttarget_label
    = \"job\"\n\t\t}\n\n\t\t// make all labels on the pod available to the pipeline
    as labels,\n\t\t// they are omitted before write via labelallow unless explicitly
    set\n\t\trule {\n\t\t\taction = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\t//
    make all annotations on the pod available to the pipeline as labels,\n\t\t// they
    are omitted before write via labelallow unless explicitly set\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_annotation_(.+)\"\n\t\t}\n\n\t\t//
    as a result of kubernetes service discovery for pods, all of the meta data information
    is exposed in labels\n\t\t// __meta_kubernetes_pod_*, including __meta_kubernetes_pod_container_id
    which can be used to determine what\n\t\t// the pods container runtime is, docker
    (docker://...) or containerd (containerd://...) this will inform us\n\t\t// which
    parsing stage to use.  However, any labels that begin with __* are not passed
    to loki.process\n\t\t// (pipeline) stages. Use a relabeling stage to set a label
    that can be used a LogQL selector in the stage\n\t\t// below so parsing can be
    automatically determined, then drop the label from the loki.process stage.\n\t\t//
    set the container runtime as a label\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_id\"]\n\t\t\tregex         = \"^(\\\\w+):\\\\/\\\\/.+$\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t\ttarget_label  = \"tmp_container_runtime\"\n\t\t}\n\t}\n\n\tloki.source.kubernetes
    \"lsd_kubernetes_logs\" {\n\t\ttargets    = discovery.relabel.annotation_logs_filter.output\n\t\tforward_to
    = [loki.process.parse.receiver]\n\t}\n\n\t// parse the log based on the container
    runtime\n\tloki.process \"parse\" {\n\t\tforward_to = argument.forward_to.value\n\t\t/*******************************************************************************\n\t\t*
    \                        Container Runtime Parsing\n\t\t********************************************************************************/\n\t\t//
    if the label tmp_container_runtime from above is containerd parse using cri\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"containerd\\\"}\"\n\t\t\t// the
    cri processing stage extracts the following k/v pairs: log, stream, time, flags\n\t\t\tstage.cri
    { }\n\n\t\t\t// Set the extract flags and stream values as labels\n\t\t\tstage.labels
    {\n\t\t\t\tvalues = {\n\t\t\t\t\tflags  = \"\",\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t//
    if the label tmp_container_runtime from above is docker parse using docker\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"docker\\\"}\"\n\t\t\t// the docker
    processing stage extracts the following k/v pairs: log, stream, time\n\t\t\tstage.docker
    { }\n\n\t\t\t// Set the extract stream value as a label\n\t\t\tstage.labels {\n\t\t\t\tvalues
    = {\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// drop the temporary
    container runtime label as it is no longer needed\n\t\tstage.label_drop {\n\t\t\tvalues
    = [\"tmp_container_runtime\"]\n\t\t}\n\t}\n}\n"
  k8s-events.alloy: "/*\nModule Components: component_cluster_events\n*/\n\ndeclare
    \"kubernetes_cluster_events\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\toptional
    = true\n\t}\n\n\targument \"cluster\" { }\n\n\tloki.source.kubernetes_events \"cluster_events\"
    {\n\t\tjob_name   = coalesce(argument.job_label.value, \"integrations/kubernetes/eventhandler\")\n\t\tlog_format
    = \"logfmt\"\n\t\tforward_to = [loki.process.logs_service.receiver]\n\t}\n\n\tloki.process
    \"logs_service\" {\n\t\tstage.static_labels {\n\t\t\tvalues = {\n\t\t\t\tcluster
    = argument.cluster.value,\n\t\t\t}\n\t\t}\n\t\tforward_to = argument.forward_to.value\n\t}\n}\n"
  keep-labels.alloy: "/*\nModule Components: keep_labels\nDescription: Pre-defined
    set of labels to keep, this stage should always be in-place as the previous relabeing\n
    \            stages make every pod label and annotation a label in the pipeline,
    which we do not want created\n             in Loki as that would have extremely
    high-cardinality.\n*/\n\ndeclare \"keep_labels\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"keep_labels\" {\n\t\toptional
    = true\n\t\tcomment  = \"List of labels to keep before the log message is written
    to Loki\"\n\t\tdefault  = [\n\t\t\t\"app\",\n\t\t\t\"job\",\n\t\t\t\"region\",\n\t\t\t\"cluster\",\n\t\t\t\"namespace\",\n\t\t\t\"pod\",\n\t\t\t\"container\",\n\t\t\t\"component\",\n\t\t\t\"env\",\n\t\t\t\"level\",\n\t\t\t\"service\",\n\t\t\t\"squad\",\n\t\t\t\"team\",\n\t\t\t\"workload\",\n\t\t]\n\t}\n\n\t/*****************************************************************\n\t*
    LOKI PROCESS\n\t*****************************************************************/\n\tloki.process
    \"keep_labels\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t/*\n\t\tAs
    all of the pod labels and annotations we transformed into labels in the previous
    relabelings to make\n\t\tthem available to the pipeline processing we need to
    ensure they are not automatically created in Loki.\n\t\tThis would result in an
    extremely high number of labels and values severely impacting query performance.\n\t\tNot
    every log has to contain these labels, but this list should reflect the set of
    labels that you want\n\t\tto explicitly allow.\n\t\t*/\n\t\tstage.label_keep {\n\t\t\tvalues
    = argument.keep_labels.value\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"receiver\" {\n\t\tvalue = loki.process.keep_labels.receiver\n\t}\n}\n"
  rules-to-loki.alloy: "/*\nModule Components: rules_to_loki\nDescription: Auto discovers
    PrometheusRule Kubernetes resources and loads them into a Loki instance.\n*/\n\ndeclare
    \"rules_to_loki\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"address\" {\n\t\tcomment  = \"URL of the Loki ruler. (default: http://nginx.gateway.svc:3100)\"\n\t\toptional
    = true\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  = \"Mimir tenant ID. (default:
    fake)\"\n\t\toptional = true\n\t}\n\n\t/********************************************\n\t*
    Kubernetes Prometheus Rules To Loki\n\t********************************************/\n\tloki.rules.kubernetes
    \"rules_to_loki\" {\n\t\taddress   = coalesce(argument.address.value, \"http://nginx.gateway.svc:3100\")\n\t\ttenant_id
    = coalesce(argument.tenant.value, \"anonymous\")\n\n\t\t// rule_namespace_selector
    {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_loki= \"true\",\n\t\t//
    \t}\n\t\t// }\n\n\t\trule_selector {\n\t\t\tmatch_labels = {\n\t\t\t\tauto_rules_to_loki
    = \"true\",\n\t\t\t}\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-logs-7b5f7t9d8g
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for metrics based on kubernetes annotations\n\nNote: Every argument
    except for \"forward_to\" is optional, and does have a defined default value.
    \ However, the values for these\n      arguments are not defined using the default
    = \" ... \" argument syntax, but rather using the coalesce(argument.value, \"
    ... \").\n      This is because if the argument passed in from another consuming
    module is set to null, the default = \" ... \" syntax will\n      does not override
    the value passed in, where coalesce() will return the first non-null value.\n\nKubernetes
    Annotation Auto-Scraping\n------------------------------------------------------------------------------------------------------------------------------------\nThis
    module is meant to be used to automatically scrape targets based on a certain
    role and set of annotations.  This module can be consumed\nmultiple times with
    different roles.  The supported roles are:\n\n  - pod\n  - service\n  - endpoints\n\nTypically,
    if mimicking the behavior of the prometheus-operator, and ServiceMonitor functionality
    you would use role=\"endpoints\", if\nmimicking the behavior of the PodMonitor
    functionality you would use role=\"pod\".  It is important to understand that
    with endpoints,\nthe target is typically going to be a pod, and whatever annotations
    that are set on the service will automatically be propagated to the\nendpoints.
    \ This is why the role \"endpoints\" is used, because it will scrape the pod,
    but also consider the service annotations.  Using\nrole=\"endpoints\", which scrape
    each endpoint associated to the service.  If role=\"service\" is used, it will
    only scrape the service, only\nhitting one of the endpoints associated to the
    service.\n\nThis is where you must consider your scraping strategy, for example
    if you scrape a service like \"kube-state-metrics\" using\nrole=\"endpoints\"
    you should only have a single replica of the kube-state-metrics pod, if you have
    multiple replicas, you should use\nrole=\"service\" or a separate non-annotation
    job completely.  Scraping a service instead of endpoints, is typically a rare
    use case, but\nit is supported.\n\nThere are other considerations for using annotation
    based scraping as well, which is metric relabeling rules that happen post scrape.
    \ If\nyou have a target that you want to apply a bunch of relabelings to or a
    very large metrics response payload, performance wise it will be\nbetter to have
    a separate job for that target, rather than using use annotations.  As every targert
    will go through the ssame relabeling.\nTypical deployment strategies/options would
    be:\n\nOption #1 (recommended):\n  - Annotation Scraping for role=\"endpoints\"\n
    \ - Separate Jobs for specific service scrapes (i.e. kube-state-metrics, node-exporter,
    etc.) or large metric payloads\n  - Separate Jobs for K8s API scraping (i.e. cadvisor,
    kube-apiserver, kube-scheduler, etc.)\n\nOption #2:\n  - Annotation Scraping for
    role=\"pod\"\n  - Annotation Scraping for role=\"service\"  (i.e. kube-state-metrics,
    node-exporter, etc.)\n  - Separate Jobs for specific use cases or large metric
    payloads\n  - Separate Jobs for K8s API scraping (i.e. cadvisor, kube-apiserver,
    kube-scheduler, etc.)\n\nAt no point should you use role=\"endpoints\" and role=\"pod\"
    together, as this will result in duplicate targets being scraped, thus\ngenerating
    duplicate metrics.  If you want to scrape both the pod and the service, use Option
    #2.\n\nEach port attached to an service/pod/endpoint is an eligible target, oftentimes
    it will have multiple ports.\nThere may be instances when you want to scrape all
    ports or some ports and not others. To support this\nthe following annotations
    are available:\n\n  metrics.grafana.com/scrape: true\n\nthe default scraping scheme
    is http, this can be specified as a single value which would override, the schema
    being used for all\nports attached to the target:\n\n  metrics.grafana.com/scheme:
    https\n\nthe default path to scrape is /metrics, this can be specified as a single
    value which would override, the scrape path being used\nfor all ports attached
    to the target:\n\n  metrics.grafana.com/path: /metrics/some_path\n\nthe default
    port to scrape is the target port, this can be specified as a single value which
    would override the scrape port being\nused for all ports attached to the target,
    note that even if aan target had multiple targets, the relabel_config targets
    are\ndeduped before scraping:\n\n  metrics.grafana.com/port: 8080\n\nthe default
    interval to scrape is 1m, this can be specified as a single value which would
    override, the scrape interval being used\nfor all ports attached to the target:\n\n
    \ metrics.grafana.com/interval: 5m\n\nthe default timeout for scraping is 10s,
    this can be specified as a single value which would override, the scrape interval
    being\nused for all ports attached to the target:\n\n  metrics.grafana.com/timeout:
    30s\n\nthe default job is namespace/{{ service name }} or namespace/{{ controller_name
    }} depending on the role, there may be instances\nin which a different job name
    is required because of a set of dashboards, rules, etc. to support this there
    is a job annotation\nwhich will override the default value:\n\n  metrics.grafana.com/job:
    integrations/kubernetes/kube-state-metrics\n*/\n\ndeclare \"annotations_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"tenant\" {\n\t\tcomment
    \ = \"The tenant to filter metrics to.  This does not have to be the tenantId,
    this is the value to look for in the metrics.agent.grafana.com/tenant annotation,
    and this can be a regex.\"\n\t\toptional = true\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"annotation_prefix\" {\n\t\tcomment  = \"The annotation_prefix
    to use (default: metrics.grafana.com)\"\n\t\tdefault  = \"metrics.grafana.com\"\n\t\toptional
    = true\n\t}\n\n\targument \"role\" {\n\t\tcomment  = \"The role to use when looking
    for targets to scrape via annotations, can be: endpoints, service, pod (default:
    endpoints)\"\n\t\toptional = true\n\t}\n\n\targument \"__sd_annotation\" {\n\t\toptional
    = true\n\t\tcomment  = \"The logic is used to transform the annotation argument
    into a valid label name by removing unsupported characters.\"\n\t\tdefault  =
    replace(replace(replace(coalesce(argument.annotation_prefix.value, \"metrics.grafana.com\"),
    \".\", \"_\"), \"/\", \"_\"), \"-\", \"_\")\n\t}\n\n\targument \"__pod_role\"
    {\n\t\tcomment  = \"Most annotation targets service or pod that is all you want,
    however if the role is endpoints you want the pod\"\n\t\toptional = true\n\t\tdefault
    \ = replace(coalesce(argument.role.value, \"endpoints\"), \"endpoints\", \"pod\")\n\t}\n\n\targument
    \"__service_role\" {\n\t\tcomment  = \"Most annotation targets service or pod
    that is all you want, however if the role is endpoints you we also want to consider
    service annotations\"\n\t\toptional = true\n\t\tdefault  = replace(coalesce(argument.role.value,
    \"endpoints\"), \"endpoints\", \"service\")\n\t}\n\n\targument \"scrape_port_named_metrics\"
    {\n\t\tcomment  = \"Whether or not to automatically scrape endpoints that have
    a port with 'metrics' in the name\"\n\t\toptional = true\n\t\tdefault  = false\n\t}\n\n\targument
    \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep (default: (.+))\"\n\t\toptional
    = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment  = \"A regex of metrics
    to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_interval\"
    {\n\t\tcomment  = \"How often to scrape metrics from the targets (default: 60s)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment  = \"How long before
    a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"annotation_metrics\" {\n\t\trole = coalesce(argument.role.value, \"endpoints\")\n\n\t\tselectors
    {\n\t\t\trole = coalesce(argument.role.value, \"endpoints\")\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\t//
    filter metrics by kubernetes annotations\n\tdiscovery.relabel \"annotation_metrics_filter\"
    {\n\t\ttargets = discovery.kubernetes.annotation_metrics.targets\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Targets to Keep or Drop\n\t\t****************************************************************************************************************/\n\t\t//
    allow resources to declare their metrics scraped or not\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/scrape: false\n\t\t//\n\t\t// the label prometheus.io/service-monitor:
    \"false\" is a common label for headless services, when performing endpoint\n\t\t//
    service discovery, if there is both a load-balanced service and headless service,
    this can result in duplicate\n\t\t// scrapes if the name of the service is attached
    as a label.  any targets with this label or annotation set should be dropped\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = \"false\"\n\t\t\ttarget_label
    = \"__tmp_scrape\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_scrape\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_scrape\",\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value,
    \"endpoints\") + \"_label_prometheus_io_service_monitor\",\n\t\t\t]\n\t\t\tseparator
    = \";\"\n\t\t\t// only allow empty or true, otherwise defaults to false\n\t\t\tregex
    \       = \"^(?:;*)?(true)(;|true)*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"__tmp_scrape\"\n\t\t}\n\n\t\t// add a __tmp_scrape_port_named_metrics from
    the argument.scrape_port_named_metrics\n\t\trule {\n\t\t\treplacement  = format(\"%t\",
    argument.scrape_port_named_metrics.value)\n\t\t\ttarget_label = \"__tmp_scrape_port_named_metrics\"\n\t\t}\n\n\t\t//
    only keep targets that have scrape: true or \"metrics\" in the port name if the
    argument scrape_port_named_metrics\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__tmp_scrape\",\n\t\t\t\t\"__tmp_scrape_port_named_metrics\",\n\t\t\t\t//
    endpoints is the role and most meta labels started with \"endpoints\", however
    the port name is an exception and starts with \"endpoint\"\n\t\t\t\t\"__meta_kubernetes_\"
    + replace(coalesce(argument.role.value, \"endpoints\"), \"endpoints\", \"endpoint\")
    + \"_port_name\",\n\t\t\t]\n\t\t\tseparator = \";\"\n\t\t\tregex     = \"^(true;.*|(|true);true;(.*metrics.*))$\"\n\t\t}\n\n\t\t//
    only keep targets where the pod is running or the pod_phase is empty and is not
    an init container.  This will only exist for role=\"pod\" or\n\t\t// potentially
    role=\"endpoints\", if it is a service the value is empty and thus allowed to
    pass, if it is an endpoint but not associated to a\n\t\t// pod but rather a static
    IP or hostname, that could be outside of kubernetes allow endpoints to declare
    what tenant their metrics should be\n\t\t// written to\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_phase\"]\n\t\t\tregex
    \        = \"^(?i)(Running|)$\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_ready\"]\n\t\t\tregex         = \"^(true|)$\"\n\t\t}\n\t\t//
    if the container is an init container, drop it\n\t\trule {\n\t\t\taction        =
    \"drop\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_init\"]\n\t\t\tregex
    \        = \"^(true)$\"\n\t\t}\n\n\t\t// allow resources to declare their metrics
    the tenant their metrics should be sent to,\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/tenant: primary\n\t\t//\n\t\t// Note: This does not necessarily
    have to be the actual tenantId, it can be a friendly name as well that is simply
    used\n\t\t//       to determine if the metrics should be gathered for the current
    tenant\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_tenant\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_tenant\",\n\t\t\t]\n\t\t\tregex
    = \"^(\" + coalesce(argument.tenant.value, \".*\") + \")$\"\n\t\t}\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Setting Scrape Metadata i.e. path, port, interval etc.\n\t\t****************************************************************************************************************/\n\t\t//
    allow resources to declare the protocol to use when collecting metrics, the default
    value is \"http\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/scheme:
    http\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = \"http\"\n\t\t\ttarget_label
    = \"__scheme__\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_scheme\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_scheme\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?(https?).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scheme__\"\n\t\t}\n\n\t\t// allow resources
    to declare the port to use when collecting metrics, the default value is the discovered
    port from\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/port: 9090\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__address__\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_port\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value +
    \"_annotation_\" + argument.__sd_annotation.value + \"_port\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^([^:]+)(?::\\\\d+)?;(\\\\d+)$\"\n\t\t\treplacement
    \ = \"$1:$2\"\n\t\t\ttarget_label = \"__address__\"\n\t\t}\n\n\t\t// allow resources
    to declare their the path to use when collecting their metrics, the default value
    is \"/metrics\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/path:
    /metrics/foo\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_path\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_path\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__metrics_path__\"\n\t\t}\n\n\t\t// allow resources
    to declare how often their metrics should be collected, the default value is 1m,\n\t\t//
    the following duration formats are supported (s|m|ms|h|d):\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/interval: 5m\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\ttarget_label = \"__scrape_interval__\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_interval\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_interval\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?(\\\\d+(s|m|ms|h|d)).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scrape_interval__\"\n\t\t}\n\n\t\t// allow
    resources to declare the timeout of the scrape request, the default value is 10s,\n\t\t//
    the following duration formats are supported (s|m|ms|h|d):\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/timeout: 30s\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t\ttarget_label = \"__scrape_timeout__\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_timeout\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_timeout\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?(\\\\d+(s|m|ms|h|d)).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scrape_timeout__\"\n\t\t}\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Setting Common Labels\n\t\t****************************************************************************************************************/\n\t\t//
    set a source label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\n\t\t// set the cluster
    label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set the namespace label\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the target name label i.e. service name,
    pod name, etc.\n\t\t// if the role is endpoints, the first valued field is used
    which would be __meta_kubernetes_pod_name, if the pod name is empty\n\t\t// then
    the endpoint name would be used\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + argument.__pod_role.value + \"_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = argument.__pod_role.value\n\t\t}\n\n\t\t// set a
    default job label to be the namespace/pod_controller_name or namespace/service_name\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t\targument.__pod_role.value,\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^([^;]+)(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1/$2\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// if the controller
    is a ReplicaSet, drop the hash from the end of the ReplicaSet\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_type\",\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:ReplicaSet);([^;]+);([^;]+)-.+$\"\n\t\t\treplacement
    \ = \"$1/$2\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// allow resources to
    declare their the job label value to use when collecting their metrics, the default
    value is \"\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/job:
    integrations/kubernetes/cadvisor\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_job\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_job\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// set the app name if
    specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app\",\n\t\t\t]\n\t\t\tregex        =
    \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t//
    set the app component if specified as metadata labels \"component:\" or \"app.kubernetes.io/component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the version if specified as metadata labels
    \"version:\" or \"app.kubernetes.io/version:\" or \"app_version:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_version\",\n\t\t\t]\n\t\t\tregex        =
    \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label = \"version\"\n\t\t}\n\n\t\t//
    set a workload label if the resource is a pod\n\t\t// example: grafana-agent-68nv9
    becomes DaemonSet/grafana-agent\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"(.+);(.+)\"\n\t\t\treplacement  = \"$1/$2\"\n\t\t\ttarget_label
    = \"workload\"\n\t\t}\n\t\t// remove the hash from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels
    = [\"workload\"]\n\t\t\tregex         = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label
    \ = \"workload\"\n\t\t}\n\t}\n\n\t// only keep http targets\n\tdiscovery.relabel
    \"http_annotations\" {\n\t\ttargets = discovery.relabel.annotation_metrics_filter.output\n\n\t\trule
    {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\"__scheme__\"]\n\t\t\tregex
    \        = \"http\"\n\t\t}\n\t}\n\n\t// only keep https targets\n\tdiscovery.relabel
    \"https_annotations\" {\n\t\ttargets = discovery.relabel.annotation_metrics_filter.output\n\n\t\trule
    {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\"__scheme__\"]\n\t\t\tregex
    \        = \"https\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\t//
    scrape http only targtetsa\n\tprometheus.scrape \"http_annotations\" {\n\t\ttargets
    = discovery.relabel.http_annotations.output\n\n\t\tjob_name        = \"annotation-metrics-http\"\n\t\tscheme
    \         = \"http\"\n\t\tscrape_interval = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.annotations.receiver]\n\t}\n\n\t//
    scrape https only targtets\n\tprometheus.scrape \"https_annotations\" {\n\t\ttargets
    = discovery.relabel.https_annotations.output\n\n\t\tjob_name          = \"annotation-metrics-https\"\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.annotations.receiver]\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"annotations\" {\n\t\tforward_to = argument.forward_to.value\n\t\t// keep only
    metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(.+)\")\n\t\t}\n\n\t\t// drop metrics that match the drop_metrics regex\n\t\trule
    {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"\")\n\t\t}\n\t}\n}\n"
  podmonitors-scrape.alloy: "/*\nModule Components: podmonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.podmonitors\n*/\n\ndeclare
    \"podmonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep
    (default: (.+))\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regex of metrics to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape PodMonitors\n\t*****************************************************************/\n\tprometheus.operator.podmonitors
    \"scrape\" {\n\t\tforward_to = [prometheus.relabel.podmonitors.receiver]\n\n\t\tscrape
    {\n\t\t\tdefault_scrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\tdefault_scrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\t// selector {\n\t\t// \tmatch_expression
    {\n\t\t// \t\tkey      = \"team\"\n\t\t// \t\toperator = \"In\"\n\t\t// \t\tvalues
    \  = [\"team-infra\"]\n\t\t// \t}\n\t\t// }\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"podmonitors\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t// set the
    cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t}\n\n\t\t// drop metrics
    that match the drop_metrics regex\n\t\trule {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.drop_metrics.value,
    \"\")\n\t\t}\n\t}\n}\n"
  rules-to-mimir.alloy: "/*\nModule Components: rules_to_mimir\nDescription: Auto
    discovers PrometheusRule Kubernetes resources and loads them into a Mimir instance.\n*/\n\ndeclare
    \"rules_to_mimir\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"address\" {\n\t\tcomment  = \"URL of the Mimir ruler. (default: http://nginx.gateway.svc:8080)\"\n\t\toptional
    = true\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  = \"Mimir tenant ID. (default:
    anonymous)\"\n\t\toptional = true\n\t}\n\n\t/********************************************\n\t*
    Kubernetes Prometheus Rules To Mimir\n\t********************************************/\n\tmimir.rules.kubernetes
    \"rules_to_mimir\" {\n\t\taddress   = coalesce(argument.address.value, \"http://nginx.gateway.svc:8080\")\n\t\ttenant_id
    = coalesce(argument.tenant.value, \"anonymous\")\n\n\t\t// rule_namespace_selector
    {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_mimir= \"true\",\n\t\t//
    \t}\n\t\t// }\n\n\t\t// rule_selector {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_mimir=
    \"true\",\n\t\t// \t}\n\t\t// }\n\t}\n}\n"
  servicemonitors-scrape.alloy: "/*\nModule Components: servicemonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.servicemonitors\n*/\n\ndeclare
    \"servicemonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep
    (default: (.+))\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regex of metrics to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape ServiceMonitors\n\t*****************************************************************/\n\tprometheus.operator.servicemonitors
    \"scrape\" {\n\t\tforward_to = [prometheus.relabel.servicemonitors.receiver]\n\n\t\tscrape
    {\n\t\t\tdefault_scrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\tdefault_scrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"servicemonitors\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t// set
    the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t}\n\n\t\t// drop metrics
    that match the drop_metrics regex\n\t\trule {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.drop_metrics.value,
    \"\")\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-metrics-db96446m8f
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for metrics based on kubernetes Pod annotations\n\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(ProfilessReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\tdiscovery.kubernetes
    \"pyroscope_kubernetes\" {\n\t\trole = \"pod\"\n\t}\n\n\t// The default scrape
    config allows to define annotations based scraping.\n\t//\n\t// For example the
    following annotations:\n\t//\n\t// ```\n\t// profiles.grafana.com/memory.scrape:
    \"true\"\n\t// profiles.grafana.com/memory.port: \"8080\"\n\t// profiles.grafana.com/cpu.scrape:
    \"true\"\n\t// profiles.grafana.com/cpu.port: \"8080\"\n\t// profiles.grafana.com/goroutine.scrape:
    \"true\"\n\t// profiles.grafana.com/goroutine.port: \"8080\"\n\t// ```\n\t//\n\t//
    will scrape the `memory`, `cpu` and `goroutine` profiles from the `8080` port
    of the pod.\n\t//\n\t// For more information see https://grafana.com/docs/phlare/latest/operators-guide/deploy-kubernetes/#optional-scrape-your-own-workloads-profiles\n\tdiscovery.relabel
    \"kubernetes_pods\" {\n\t\ttargets = concat(discovery.kubernetes.pyroscope_kubernetes.targets)\n\n\t\trule
    {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_phase\"]\n\t\t\tregex
    \        = \"Pending|Succeeded|Failed|Completed\"\n\t\t}\n\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\t//
    set the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_memory_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_memory_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Memory\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_memory\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_memory_default_name.output, discovery.relabel.kubernetes_pods_memory_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_cpu_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_cpu_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape CPU\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_cpu\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_cpu_default_name.output, discovery.relabel.kubernetes_pods_cpu_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_goroutine_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_goroutine_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Goroutine\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_goroutine\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_goroutine_default_name.output,
    discovery.relabel.kubernetes_pods_goroutine_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled =
    false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_block_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_block_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Block\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_block\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_block_default_name.output, discovery.relabel.kubernetes_pods_block_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_mutex_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_mutex_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Mutex\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_mutex\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_mutex_default_name.output, discovery.relabel.kubernetes_pods_mutex_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_fgprof_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_fgprof_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Fgprof\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_fgprof\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_fgprof_default_name.output, discovery.relabel.kubernetes_pods_fgprof_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-profiles-8hhkt7m7k9
  namespace: monitoring-system
---
apiVersion: v1
data:
  process-and-transform.alloy: "/*\nModule Components: process_and_transform\n\nDescription:
    Traces data collection processing and transformation\n*/\n\n// Processing And
    Transformation\ndeclare \"process_and_transform\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"traces_forward_to\" {\n\t\tcomment = \"Must be a list(TracesReceiver) where
    collected traces should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"logs_forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"metrics_forward_to\" {\n\t\tcomment
    = \"Must be a list(MetricsReceiver) where collected metrics should be forwarded
    to\"\n\t}\n\n\targument \"otlp_http_endpoint\" {\n\t\toptional = true\n\t\tdefault
    \ = \"0.0.0.0:4318\"\n\t}\n\n\targument \"otlp_grpc_endpoint\" {\n\t\toptional
    = true\n\t\tdefault  = \"0.0.0.0:4317\"\n\t}\n\n\t/*****************************************************************\n\t*
    Jaeger for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.jaeger
    \"default\" {\n\t\tprotocols {\n\t\t\tgrpc {\n\t\t\t\tendpoint = \"0.0.0.0:14250\"\n\t\t\t}\n\n\t\t\tthrift_http
    {\n\t\t\t\tendpoint = \"0.0.0.0:14268\"\n\t\t\t}\n\n\t\t\tthrift_binary {\n\t\t\t\tendpoint
    = \"0.0.0.0:6832\"\n\t\t\t}\n\n\t\t\tthrift_compact {\n\t\t\t\tendpoint = \"0.0.0.0:6831\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\ttraces = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Otelcol for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.otlp
    \"default\" {\n\t\tdebug_metrics {\n\t\t\tdisable_high_cardinality_metrics = true\n\t\t}\n\n\t\tgrpc
    {\n\t\t\tendpoint = argument.otlp_grpc_endpoint.value\n\t\t}\n\n\t\thttp {\n\t\t\tendpoint
    = argument.otlp_http_endpoint.value\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.resourcedetection.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.resourcedetection.default.input]\n\t\t\ttraces  = [\n\t\t\t\totelcol.processor.resourcedetection.default.input,\n\t\t\t\totelcol.connector.spanlogs.autologging.input,\n\t\t\t]\n\t\t}\n\t}\n\n\totelcol.processor.resourcedetection
    \"default\" {\n\t\tdetectors = [\"env\", \"system\"]\n\n\t\tsystem {\n\t\t\thostname_sources
    = [\"os\"]\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]\n\n\t\t\tlogs
    \  = [otelcol.processor.k8sattributes.default.input]\n\t\t\ttraces = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.transform
    \"add_metric_datapoint_attributes\" {\n\t\terror_mode = \"ignore\"\n\n\t\tmetric_statements
    {\n\t\t\tcontext    = \"datapoint\"\n\t\t\tstatements = [\n\t\t\t\t\"set(attributes[\\\"deployment.environment\\\"],
    resource.attributes[\\\"deployment.environment\\\"])\",\n\t\t\t\t\"set(attributes[\\\"service.version\\\"],
    resource.attributes[\\\"service.version\\\"])\",\n\t\t\t]\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.k8sattributes
    \"default\" {\n\t\textract {\n\t\t\tmetadata = [\n\t\t\t\t\"k8s.namespace.name\",\n\t\t\t\t\"k8s.pod.name\",\n\t\t\t\t\"k8s.deployment.name\",\n\t\t\t\t\"k8s.statefulset.name\",\n\t\t\t\t\"k8s.daemonset.name\",\n\t\t\t\t\"k8s.cronjob.name\",\n\t\t\t\t\"k8s.job.name\",\n\t\t\t\t\"k8s.node.name\",\n\t\t\t\t\"k8s.pod.uid\",\n\t\t\t\t\"k8s.pod.start_time\",\n\t\t\t]\n\t\t}\n\n\t\tpod_association
    {\n\t\t\tsource {\n\t\t\t\tfrom = \"connection\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.transform.default.input]\n\t\t\tlogs    =
    [otelcol.processor.transform.default.input]\n\t\t\ttraces  = [\n\t\t\t\totelcol.processor.transform.default.input,\n\t\t\t\totelcol.connector.host_info.default.input,\n\t\t\t]\n\t\t}\n\t}\n\n\totelcol.connector.host_info
    \"default\" {\n\t\thost_identifiers = [\"k8s.node.name\"]\n\n\t\toutput {\n\t\t\tmetrics
    = [otelcol.processor.batch.host_info_batch.input]\n\t\t}\n\t}\n\n\totelcol.processor.batch
    \"host_info_batch\" {\n\t\toutput {\n\t\t\tmetrics = [otelcol.exporter.prometheus.host_info_metrics.input]\n\t\t}\n\t}\n\n\totelcol.exporter.prometheus
    \"host_info_metrics\" {\n\t\tadd_metric_suffixes = false\n\t\tforward_to          =
    argument.metrics_forward_to.value\n\t}\n\n\totelcol.processor.transform \"default\"
    {\n\t\terror_mode = \"ignore\"\n\n\t\tmetric_statements {\n\t\t\tcontext    =
    \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\tlog_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"pod\"],
    attributes[\"k8s.pod.name\"])`,\n\t\t\t\t`set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])`,\n\t\t\t\t`set(attributes[\"loki.resource.labels\"],
    \"pod, namespace, cluster, job\")`,\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\ttrace_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`limit(attributes,
    100, [])`,\n\t\t\t\t`truncate_all(attributes, 4096)`,\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\ttrace_statements
    {\n\t\t\tcontext    = \"span\"\n\t\t\tstatements = [\n\t\t\t\t`limit(attributes,
    100, [])`,\n\t\t\t\t`truncate_all(attributes, 4096)`,\n\t\t\t]\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.filter.default.input]\n\t\t\tlogs    = [otelcol.processor.filter.default.input]\n\t\t\ttraces
    \ = [otelcol.processor.filter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.filter
    \"default\" {\n\t\terror_mode = \"ignore\"\n\n\t\ttraces {\n\t\t\tspan = [\n\t\t\t\t\"attributes[\\\"http.route\\\"]
    == \\\"/live\\\"\",\n\t\t\t\t\"attributes[\\\"http.route\\\"] == \\\"/healthy\\\"\",\n\t\t\t\t\"attributes[\\\"http.route\\\"]
    == \\\"/ready\\\"\",\n\t\t\t]\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.batch.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.batch.default.input]\n\t\t\ttraces  = [otelcol.processor.batch.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.batch
    \"default\" {\n\t\tsend_batch_size     = 16384\n\t\tsend_batch_max_size = 0\n\t\ttimeout
    \            = \"5s\"\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.memory_limiter.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.memory_limiter.default.input]\n\t\t\ttraces  = [otelcol.processor.memory_limiter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.memory_limiter
    \"default\" {\n\t\tcheck_interval         = \"1s\"\n\t\tlimit_percentage       =
    50\n\t\tspike_limit_percentage = 30\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.exporter.prometheus.tracesmetrics.input]\n\t\t\tlogs
    \   = [otelcol.exporter.loki.traceslogs.input]\n\t\t\ttraces  = argument.traces_forward_to.value\n\t\t}\n\t}\n\n\totelcol.exporter.prometheus
    \"tracesmetrics\" {\n\t\tforward_to = argument.metrics_forward_to.value\n\t}\n\n\totelcol.exporter.loki
    \"traceslogs\" {\n\t\tforward_to = [loki.process.traceslogs.receiver]\n\t}\n\n\t//
    The OpenTelemetry spanlog connector processes incoming trace spans and extracts
    data from them ready\n\t// for logging.\n\totelcol.connector.spanlogs \"autologging\"
    {\n\t\t// We only want to output a line for each root span (ie. every single trace),
    and not for every\n\t\t// process or span (outputting a line for every span would
    be extremely verbose).\n\t\tspans     = false\n\t\troots     = true\n\t\tprocesses
    = false\n\n\t\t// We want to ensure that the following three span attributes are
    included in the log line, if present.\n\t\tspan_attributes = [\n\t\t\t\"http.method\",\n\t\t\t\"http.target\",\n\t\t\t\"http.status_code\",\n\t\t]\n\n\t\t//
    Overrides the default key in the log line to be `traceId`, which is then used
    by Grafana to\n\t\t// identify the trace ID for correlation with the Tempo datasource.\n\t\toverrides
    {\n\t\t\ttrace_id_key = \"traceId\"\n\t\t}\n\n\t\t// Send to the OpenTelemetry
    Loki exporter.\n\t\toutput {\n\t\t\tlogs = [otelcol.exporter.loki.autologging.input]\n\t\t}\n\t}\n\n\t//
    Simply forwards the incoming OpenTelemetry log format out as a Loki log.\n\t//
    We need this stage to ensure we can then process the logline as a Loki object.\n\totelcol.exporter.loki
    \"autologging\" {\n\t\tforward_to = [loki.process.autologging.receiver]\n\t}\n\n\t//
    The Loki processor allows us to accept a correctly formatted Loki log and mutate
    it into\n\t// a set of fields for output.\n\tloki.process \"autologging\" {\n\t\t//
    The JSON stage simply extracts the `body` (the actual logline) from the Loki log,
    ignoring\n\t\t// all other fields.\n\t\tstage.json {\n\t\t\texpressions = {\"body\"
    = \"\"}\n\t\t}\n\t\t// The output stage takes the body (the main logline) and
    uses this as the source for the output\n\t\t// logline. In this case, it essentially
    turns it into logfmt.\n\t\tstage.output {\n\t\t\tsource = \"body\"\n\t\t}\n\n\t\tforward_to
    = [loki.process.traceslogs.receiver]\n\t}\n\n\tloki.process \"traceslogs\" {\n\t\tstage.tenant
    {\n\t\t\tvalue = \"anonymous\"\n\t\t}\n\n\t\tforward_to = argument.logs_forward_to.value\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"alloy_traces_input\" {\n\t\tvalue = otelcol.processor.batch.default.input\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-traces-4h9946thb4
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-opentelemetry.json: |-
    {
          "graphTooltip": 1,
          "links": [
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "Receivers for traces [otelcol.receiver]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans successfully pushed into the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(receiver_accepted_spans_ratio_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }} / {{ transport }}",
                      "range": true
                   }
                ],
                "title": "Accepted spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans that could not be pushed into the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(receiver_refused_spans_ratio_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }} / {{ transport }}",
                      "range": true
                   }
                ],
                "title": "Refused spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The duration of inbound RPCs.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 0
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "color": {
                      "exponent": 0.5,
                      "fill": "dark-orange",
                      "mode": "scheme",
                      "scale": "exponential",
                      "scheme": "Oranges",
                      "steps": 65
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 1.0000000000000001e-09
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "ms"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (le) (increase(rpc_server_duration_milliseconds_bucket{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", rpc_service=\"opentelemetry.proto.collector.trace.v1.TraceService\"}[$__rate_interval]))",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "RPC server duration",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 10
                },
                "title": "Batching of logs, metrics, and traces [otelcol.processor.batch]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans, metric datapoints, or log lines in a batch\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 10
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "color": {
                      "exponent": 0.5,
                      "fill": "dark-orange",
                      "mode": "scheme",
                      "scale": "exponential",
                      "scheme": "Oranges",
                      "steps": 65
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 1.0000000000000001e-09
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "short"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (le) (increase(processor_batch_batch_send_size_ratio_bucket{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval]))",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Number of units in the batch",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of distinct metadata value combinations being processed\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 10
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "processor_batch_metadata_cardinality_ratio{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Distinct metadata values",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of times the batch was sent due to a timeout trigger\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 10
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(processor_batch_timeout_trigger_send_ratio_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Timeout trigger",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 20
                },
                "title": "Exporters for traces [otelcol.exporter]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans successfully sent to destination.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 20
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(exporter_sent_spans_ratio_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Exported sent spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans in failed attempts to send to destination.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 20
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(exporter_send_failed_spans_ratio_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Exported failed spans",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / OpenTelemetry",
          "uid": "9b6d37c8603e19e8922133984faad93d"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-opentelemetry.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-prometheus-remote-write.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Component documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/components/prometheus.remote_write/"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "collapsed": false,
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "prometheus.scrape",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Percentage of targets successfully scraped by prometheus.scrape\ncomponents.\n\nThis metric is calculated by dividing the number of targets\nsuccessfully scraped by the total number of targets scraped,\nacross all the namespaces in the selected cluster.\n\nLow success rates can indicate a problem with scrape targets,\nstale service discovery, or Alloy misconfiguration.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 0,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(up{cluster=\"$cluster\"})\n/\ncount (up{cluster=\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "% of targets successfully scraped",
                      "range": true
                   }
                ],
                "title": "Scrape success rate in $cluster",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Duration of successful scrapes by prometheus.scrape components,\nacross all the namespaces in the selected cluster.\n\nThis metric should be below your configured scrape interval.\nHigh durations can indicate a problem with a scrape target or\na performance issue with Alloy.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 12,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.99, scrape_duration_seconds{cluster=\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p99",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.95, scrape_duration_seconds{cluster=\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p95",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.50, scrape_duration_seconds{cluster=\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p50",
                      "range": true
                   }
                ],
                "title": "Scrape duration in $cluster",
                "type": "timeseries"
             },
             {
                "collapsed": false,
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 11
                },
                "title": "prometheus.remote_write",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "How far behind prometheus.remote_write from samples recently written\nto the WAL.\n\nEach endpoint prometheus.remote_write is configured to send metrics\nhas its own delay. The time shown here is the sum across all\nendpoints for the given component.\n\nIt is normal for the WAL delay to be within 1-3 scrape intervals. If\nthe WAL delay continues to increase beyond that amount, try\nincreasing the number of maximum shards.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 6,
                   "x": 0,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (instance, component_path, component_id) (\n  prometheus_remote_storage_highest_timestamp_in_seconds{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}\n  - ignoring(url, remote_name) group_right(instance)\n  prometheus_remote_storage_queue_highest_sent_timestamp_seconds{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "WAL delay",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data containing samples and metadata sent by\nprometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 6,
                   "x": 6,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (remote_name, url) (\n    rate(prometheus_remote_storage_bytes_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval]) +\n    rate(prometheus_remote_storage_metadata_bytes_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Data write throughput",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Latency of writes to the remote system made by\nprometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 6,
                   "x": 12,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.99, sum by (le) (\n  rate(prometheus_remote_storage_sent_batch_duration_seconds_bucket{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n))\n",
                      "instant": false,
                      "legendFormat": "99th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.50, sum by (le) (\n  rate(prometheus_remote_storage_sent_batch_duration_seconds_bucket{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n))\n",
                      "instant": false,
                      "legendFormat": "50th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(rate(prometheus_remote_storage_sent_batch_duration_seconds_sum{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}[$__rate_interval])) /\nsum(rate(prometheus_remote_storage_sent_batch_duration_seconds_count{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}[$__rate_interval]))\n",
                      "instant": false,
                      "legendFormat": "Average",
                      "range": true
                   }
                ],
                "title": "Write latency",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of shards which are concurrently sending samples read\nfrom the Write-Ahead Log.\n\nShards are bound to a minimum and maximum, displayed on the graph.\nThe lowest minimum and the highest maximum across all clients is\nshown.\n\nEach client has its own set of shards, minimum shards, and maximum\nshards; filter to a specific URL to display more granular\ninformation.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "none"
                   },
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Minimum"
                         },
                         "properties": [
                            {
                               "id": "custom.lineStyle",
                               "value": {
                                  "dash": [
                                     10,
                                     15
                                  ],
                                  "fill": "dash"
                               }
                            },
                            {
                               "id": "custom.showPoints",
                               "value": "never"
                            },
                            {
                               "id": "custom.hideFrom",
                               "value": {
                                  "legend": true,
                                  "tooltip": false,
                                  "viz": false
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Maximum"
                         },
                         "properties": [
                            {
                               "id": "custom.lineStyle",
                               "value": {
                                  "dash": [
                                     10,
                                     15
                                  ],
                                  "fill": "dash"
                               }
                            },
                            {
                               "id": "custom.showPoints",
                               "value": "never"
                            },
                            {
                               "id": "custom.hideFrom",
                               "value": {
                                  "legend": true,
                                  "tooltip": false,
                                  "viz": false
                               }
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 10,
                   "w": 6,
                   "x": 18,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (remote_name, url) (\n    prometheus_remote_storage_shards{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "min (\n    prometheus_remote_storage_shards_min{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "Minimum",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "max (\n    prometheus_remote_storage_shards_max{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "Maximum",
                      "range": true
                   }
                ],
                "title": "Shards",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total outgoing samples sent by prometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url, remote_name) (\n  rate(prometheus_remote_storage_samples_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Sent samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of samples which prometheus.remote_write could not send due to\nnon-recoverable errors.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url,remote_name) (\n  rate(prometheus_remote_storage_samples_failed_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Failed samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of samples which prometheus.remote_write attempted to resend\nafter receiving a recoverable error.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url,remote_name) (\n  rate(prometheus_remote_storage_samples_retried_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Retried samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series across all components.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 32
                },
                "options": {
                   "legend": {
                      "showLegend": false
                   }
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(prometheus_remote_write_wal_storage_active_series{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"})\n",
                      "instant": false,
                      "legendFormat": "Series",
                      "range": true
                   }
                ],
                "title": "Active series (total)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series which are currently being tracked by\nprometheus.remote_write components, with separate lines for each Alloy instance.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 32
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "prometheus_remote_write_wal_storage_active_series{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_id!=\"\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Active series (by instance/component)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series which are currently being tracked by\nprometheus.remote_write components, aggregated across all instances.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 32
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (component_path, component_id) (prometheus_remote_write_wal_storage_active_series{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_id!=\"\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"})\n",
                      "instant": false,
                      "legendFormat": "{{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Active series (by component)",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "component_path",
                   "multi": true,
                   "name": "component_path",
                   "query": {
                      "query": "label_values(prometheus_remote_write_wal_samples_appended_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_id=~\"prometheus\\\\.remote_write\\\\..*\", component_path=~\".*\"}, component_path)\n",
                      "refId": "component_path"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "component",
                   "multi": true,
                   "name": "component",
                   "query": {
                      "query": "label_values(prometheus_remote_write_wal_samples_appended_total{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_id=~\"prometheus\\\\.remote_write\\\\..*\"}, component_id)\n",
                      "refId": "component"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "url",
                   "multi": true,
                   "name": "url",
                   "query": {
                      "query": "label_values(prometheus_remote_storage_sent_batch_duration_seconds_sum{cluster=\"$cluster\", namespace=\"$namespace\", instance=~\"$instance\", component_id=~\"$component\"}, url)\n",
                      "refId": "url"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Prometheus Components",
          "uid": "e324cc55567d7f3a8e32860ff8e6d0d9"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-prometheus-remote-write.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-resources.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "description": "CPU usage of the Alloy process relative to 1 CPU core.\n\nFor example, 100% means using one entire CPU core.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_process_cpu_seconds_total{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}[$__rate_interval])",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "CPU usage",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Resident memory size of the Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "decbytes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "alloy_resources_process_resident_memory_bytes{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Memory (RSS)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate at which the Alloy process performs garbage collections.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "drawStyle": "points",
                         "pointSize": 3
                      },
                      "unit": "ops"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(go_gc_duration_seconds_count{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}[5m])\nand on(instance)\nalloy_build_info{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Garbage collections",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of goroutines which are running in parallel. An infinitely\ngrowing number of these indicates a goroutine leak.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "none"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "go_goroutines{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}\nand on(instance)\nalloy_build_info{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Goroutines",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Heap memory currently in use by the Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "decbytes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "go_memstats_heap_inuse_bytes{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}\nand on(instance)\nalloy_build_info{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Memory (heap inuse)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data received across all network interfaces for the machine\nAlloy is running on.\n\nData shown here is across all running processes and not exclusive to\nthe running Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 30,
                         "gradientMode": "none",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 16
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_machine_rx_bytes_total{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Network receive bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data sent across all network interfaces for the machine\nAlloy is running on.\n\nData shown here is across all running processes and not exclusive to\nthe running Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 30,
                         "gradientMode": "none",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 16
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_machine_tx_bytes_total{cluster=\"$cluster\",namespace=\"$namespace\",instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Network send bandwidth",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=\"$cluster\", namespace=\"$namespace\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Resources",
          "uid": "d6a8574c31f3d7cb8f1345ec84d15a67"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/Alloy Mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-resources.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alertmanager_fallback_config.yaml: |
    route:
      group_wait: 0s
      receiver: empty-receiver

    receivers:
      # In this example we're not going to send any notification out of Alertmanager.
      - name: 'empty-receiver'
  mimir.yaml: |
    # Do not use this configuration in production.
    # It is for demonstration purposes only.
    multitenancy_enabled: true

    # -usage-stats.enabled=false
    usage_stats:
      enabled: false

    server:
      http_listen_port: 8080
      grpc_listen_port: 9095
      log_level: info

    # https://grafana.com/docs/mimir/latest/references/configuration-parameters/#use-environment-variables-in-the-configuration
    common:
      storage:
        backend: s3
        s3:
          endpoint:          ${MIMIR_S3_ENDPOINT:minio.minio-system.svc:443}
          access_key_id:     ${MIMIR_S3_ACCESS_KEY_ID:lgtmp}
          secret_access_key: ${MIMIR_S3_SECRET_ACCESS_KEY:supersecret}
          insecure:          ${MIMIR_S3_INSECURE:false}
          http:
            insecure_skip_verify: true

    alertmanager:
      data_dir: /data/alertmanager
      enable_api: true
      external_url: /alertmanager
      fallback_config_file: /etc/mimir/alertmanager_fallback_config.yaml
    alertmanager_storage:
      s3:
        bucket_name: mimir-alertmanager


    memberlist:
      join_members: [ mimir-memberlist:7946 ]

    ingester:
      ring:
        replication_factor: 1

    store_gateway:
      sharding_ring:
        replication_factor: 1


    blocks_storage:
      s3:
        bucket_name: mimir-blocks
      tsdb:
        dir: /data/ingester
        ship_interval: 1m
        block_ranges_period: [ 2h ]
        retention_period: 3h
      bucket_store:
        index_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        chunks_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        metadata_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

    ruler:
      rule_path: /data/rules
      enable_api: true
      alertmanager_url: http://localhost:8080/alertmanager
    ruler_storage:
      s3:
        bucket_name: mimir-ruler
      cache:
        backend: memcached
        memcached:
          addresses: dns+memcached.memcached-system.svc:11211

    compactor:
      compaction_interval: 30s
      data_dir: /data/mimir-compactor
      cleanup_interval:    1m
      tenant_cleanup_delay: 1m

    limits:
      native_histograms_ingestion_enabled: true

    overrides_exporter:
      ring:
        enabled: true
        wait_stability_min_duration: 30s

    runtime_config:
      file: /etc/mimir/runtime.yaml
  runtime.yaml: |-
    # This file can be used to set overrides or other runtime config.
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_fetched_series_per_query: 100000
        native_histograms_ingestion_enabled: true
        ruler_max_rules_per_rule_group: 50
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir-config-mt42964996
  namespace: monitoring-system
---
apiVersion: v1
data:
  config.yaml: |
    multitenancy_enabled: true
    analytics:
      reporting_enabled: false

    # https://grafana.com/docs/pyroscope/latest/configure-server/configure-disk-storage/#configure-pyroscope-disk-storage
    pyroscopedb:
      max_block_duration: 5m

    # https://grafana.com/docs/pyroscope/latest/configure-server/reference-configuration-parameters/#use-environment-variables-in-the-configuration
    storage:
      backend: s3
      s3:
        bucket_name: pyroscope-data
        endpoint: ${PYROSCOPE_STORAGE_S3_ENDPOINT:-minio.minio-system.svc:443}
        access_key_id: ${PYROSCOPE_STORAGE_S3_ACCESS_KEY_ID:-lgtmp}
        secret_access_key: ${PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY:-supersecret}
        insecure: ${PYROSCOPE_STORAGE_S3_INSECURE:-false}
        http:
          insecure_skip_verify: true
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-config
  namespace: profiles-system
---
apiVersion: v1
data:
  overrides.yaml: |
    overrides:
      {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-overrides-config
  namespace: profiles-system
---
apiVersion: v1
data:
  overrides.yaml: |
    overrides:
      "anonymous":
        ingestion:
          rate_strategy: local
          rate_limit_bytes: 15000000
          burst_size_bytes: 20000000
          max_traces_per_user: 10000
        read:
          max_bytes_per_tag_values_query: 5000000
        # global:
        #   max_bytes_per_trace: 1500000
        # metrics_generator:
        #   processors:
        #   - service-graphs
        #   - span-metrics
        #   remote_write_headers:
        #     X-Scope-OrgID: "anonymous"
  tempo.yaml: |
    # For more information on this configuration, see the complete reference guide at
    # https://grafana.com/docs/tempo/latest/configuration/

    stream_over_http_enabled: true

    multitenancy_enabled: true
    usage_report:
      reporting_enabled: false

    compactor:
      compaction:
        block_retention: 1h

    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

    ingester:
      trace_idle_period: 10s
      max_block_bytes: 1_000_000
      max_block_duration: 5m

    querier:
      frontend_worker:
        frontend_address: tempo:9095

    metrics_generator:
      processor:
        span_metrics:
          # Configure extra dimensions to add as metric labels.
          dimensions:
          - http.method
          - http.target
          - http.status_code
          - service.version
        # Service graph metrics create node and edge metrics for determinng service interactions.
        service_graphs:
          # Configure extra dimensions to add as metric labels.
          dimensions:
          - http.method
          - http.target
          - http.status_code
          - service.version
      storage:
        path: /tmp/tempo/generator/wal
        remote_write_add_org_id_header: true
        remote_write:
        - url: http://nginx.gateway.svc.cluster.local:8080/api/v1/push
          send_exemplars: true
          send_native_histograms: true
          headers:
            X-Scope-OrgID: "anonymous"

    server:
      http_listen_port: 3100
      grpc_listen_port: 9095

    storage:
      trace:
        backend: s3
        wal:
          path: /tmp/tempo/wal
        s3:
          bucket: tempo-data
          endpoint: ${TEMPO_S3_ENDPOINT:-minio.minio-system.svc:443}
          access_key: ${TEMPO_S3_ACCESS_KEY:-lgtmp}
          secret_key: ${TEMPO_S3_SECRET_KEY:-supersecret}
          insecure: ${TEMPO_S3_INSECURE:-false}
          tls_insecure_skip_verify: true

    # https://github.com/grafana/tempo/blob/main/docs/sources/tempo/configuration/_index.md#cache
    cache:
      background:
        writeback_goroutines: 5
      caches:
      - roles:
        - bloom
        - parquet-footer
        - parquet-page
        - frontend-search
        - parquet-column-idx
        - parquet-offset-idx
        memcached:
          addresses: "dns+memcached.memcached-system.svc:11211"

    overrides:
      per_tenant_override_config: /conf/overrides.yaml
      defaults:
        global:
          max_bytes_per_trace: 1500000
        metrics_generator:
          processors:
          - service-graphs
          - span-metrics
          remote_write_headers:
            X-Scope-OrgID: "anonymous"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.4.2
    helm.sh/chart: tempo-1.8.0
  name: tempo
  namespace: tracing-system
---
apiVersion: v1
data:
  LOKI_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-env-58m52b99kc
  namespace: logging-system
type: Opaque
---
apiVersion: v1
data:
  instance-address: bWVtY2FjaGVkLm1lbWNhY2hlZC1zeXN0ZW0uc3ZjLmNsdXN0ZXIubG9jYWw6MTEyMTE=
  instance-name: cHJpbWFyeQ==
  instance-timeout: NXM=
kind: Secret
metadata:
  name: alloy-integrations-memcached
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  instance-name: cHJpbWFyeQ==
  mysql-host: bXlzcWwubXlzcWwtc3lzdGVtLnN2Yy5jbHVzdGVyLmxvY2Fs
  mysql-password: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
  mysql-username: bGd0bXA=
kind: Secret
metadata:
  name: alloy-integrations-mysql
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  instance-address: cmVkaXMtbWFzdGVyLnJlZGlzLXN5c3RlbS5zdmMuY2x1c3Rlci5sb2NhbDo2Mzc5
  instance-name: cHJpbWFyeQ==
  instance-password: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: alloy-integrations-redis
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  MIMIR_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir-env-92ddctt858
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: pyroscope-env-h982fgc652
  namespace: profiles-system
type: Opaque
---
apiVersion: v1
data:
  JAEGER_AGENT_HOST: Z3JhZmFuYS1hZ2VudC5tb25pdG9yaW5nLXN5c3RlbS5zdmMuY2x1c3Rlci5sb2NhbA==
  JAEGER_AGENT_PORT: NjgzMQ==
  JAEGER_SAMPLER_PARAM: MQ==
  JAEGER_SAMPLER_TYPE: Y29uc3Q=
  JAEGER_TAGS: YXBwPXRlbXBv
  TEMPO_S3_SECRET_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: tempo-env-gk54k88t7g
  namespace: tracing-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  ports:
  - name: http-metrics
    port: 3100
  - appProtocol: grpc
    name: grpc
    port: 9095
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
    prometheus.io/service-monitor: "false"
  name: loki-headless
  namespace: logging-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 3100
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
    prometheus.io/service-monitor: "false"
  name: loki-memberlist
  namespace: logging-system
spec:
  clusterIP: None
  ports:
  - name: tcp-gossip-ring
    port: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
  namespace: monitoring-system
spec:
  internalTrafficPolicy: Cluster
  ports:
  - name: http-metrics
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.12.0
    helm.sh/chart: kube-state-metrics-5.19.0
  name: kube-state-metrics
  namespace: monitoring-system
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
    prometheus.io/service-monitor: "false"
  name: mimir-memberlist
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "false"
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.7.0
    helm.sh/chart: prometheus-node-exporter-4.32.0
  name: prometheus-node-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: metrics
    port: 9100
    protocol: TCP
    targetPort: 9100
  selector:
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
    prometheus.io/service-monitor: "false"
  name: pyroscope-headless
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-memberlist
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: memberlist
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.4.2
    helm.sh/chart: tempo-1.8.0
  name: tempo
  namespace: tracing-system
spec:
  ports:
  - name: tempo-grpc
    port: 9095
    targetPort: 9095
  - name: tempo-prom-metrics
    port: 3100
    targetPort: 3100
  - name: tempo-jaeger-thrift-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: tempo-jaeger-thrift-binary
    port: 6832
    protocol: UDP
    targetPort: 6832
  - name: tempo-jaeger-thrift-http
    port: 14268
    protocol: TCP
    targetPort: 14268
  - name: grpc-tempo-jaeger
    port: 14250
    protocol: TCP
    targetPort: 14250
  - name: tempo-zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: tempo-otlp-legacy
    port: 55680
    protocol: TCP
    targetPort: 55680
  - name: tempo-otlp-http-legacy
    port: 55681
    protocol: TCP
    targetPort: 4318
  - name: grpc-tempo-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: tempo-otlp-http
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: tempo-opencensus
    port: 55678
    protocol: TCP
    targetPort: 55678
  selector:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/name: tempo
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.12.0
    helm.sh/chart: kube-state-metrics-5.19.0
  name: kube-state-metrics
  namespace: monitoring-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: kube-state-metrics
      app.kubernetes.io/name: kube-state-metrics
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/component: metrics
        app.kubernetes.io/instance: kube-state-metrics
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/version: 2.12.0
        helm.sh/chart: kube-state-metrics-5.19.0
    spec:
      containers:
      - args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      hostNetwork: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: kube-state-metrics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
      app.kubernetes.io/part-of: memberlist
  template:
    metadata:
      annotations:
        logs.grafana.com/scrape: "true"
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: mimir
      labels:
        app.kubernetes.io/component: mimir
        app.kubernetes.io/instance: mimir-monolithic-mode
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
    spec:
      containers:
      - args:
        - -target=all
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - secretRef:
            name: mimir-env-92ddctt858
        image: docker.io/grafana/mimir:2.11.0
        imagePullPolicy: IfNotPresent
        name: mimir
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
        resources:
          limits:
            cpu: 999m
            memory: 1Gi
          requests:
            cpu: 10m
            memory: 55Mi
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /data
          name: storage
      terminationGracePeriodSeconds: 60
      volumes:
      - configMap:
          name: mimir-config-mt42964996
        name: config
      - emptyDir: {}
        name: storage
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: loki
      app.kubernetes.io/instance: loki-monolithic-mode
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
  serviceName: loki-headless
  template:
    metadata:
      annotations:
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: loki
      labels:
        app.kubernetes.io/component: loki
        app.kubernetes.io/instance: loki-monolithic-mode
        app.kubernetes.io/name: loki
        app.kubernetes.io/part-of: memberlist
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/etc/loki/config/config.yaml
        - -target=all
        - -config.expand-env=true
        envFrom:
        - secretRef:
            name: loki-env-58m52b99kc
        image: docker.io/grafana/loki:3.0.0
        imagePullPolicy: IfNotPresent
        name: loki
        ports:
        - containerPort: 3100
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /tmp
          name: tmp
        - mountPath: /etc/loki/config
          name: config
        - mountPath: /etc/loki/runtime-config
          name: runtime-config
        - mountPath: /var/loki
          name: storage
      enableServiceLinks: true
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: loki
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: tmp
      - configMap:
          items:
          - key: config.yaml
            path: config.yaml
          name: loki-config-2mt25bhccf
        name: config
      - configMap:
          name: loki-runtime-9599m5k6h2
        name: runtime-config
  updateStrategy:
    rollingUpdate:
      partition: 0
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      labels:
        app.kubernetes.io/component: loki
        app.kubernetes.io/instance: loki-monolithic-mode
        app.kubernetes.io/name: loki
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
  serviceName: alloy
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        logs.grafana.com/scrape: "true"
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: alloy
      labels:
        app.kubernetes.io/instance: alloy
        app.kubernetes.io/name: alloy
    spec:
      containers:
      - args:
        - run
        - /etc/alloy/config.alloy
        - --storage.path=/tmp/alloy
        - --server.http.listen-addr=0.0.0.0:12345
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=alloy-cluster
        - --stability.level=experimental
        env:
        - name: ALLOY_DEPLOY_MODE
          value: helm
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
        - secretRef:
            name: alloy-env
            optional: true
        image: docker.io/grafana/alloy:v1.1.0
        imagePullPolicy: IfNotPresent
        name: alloy
        ports:
        - containerPort: 12345
          name: http-metrics
        - containerPort: 4317
          name: grpc-otlp
          protocol: TCP
        - containerPort: 4318
          name: http-otlp
          protocol: TCP
        - containerPort: 9411
          name: zipkin
          protocol: TCP
        - containerPort: 6831
          name: jaeger-compact
          protocol: UDP
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 12345
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /etc/alloy
          name: config
        - mountPath: /etc/alloy/modules/kubernetes/metrics
          name: modules-kubernetes-metrics
        - mountPath: /etc/alloy/modules/kubernetes/logs
          name: modules-kubernetes-logs
        - mountPath: /etc/alloy/modules/kubernetes/traces
          name: modules-kubernetes-traces
        - mountPath: /etc/alloy/modules/kubernetes/profiles
          name: modules-kubernetes-profiles
        - mountPath: /etc/alloy/modules/kubernetes/jobs
          name: modules-kubernetes-jobs
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: alloy
      volumes:
      - configMap:
          name: alloy-config-mt4f8cf687
        name: config
      - configMap:
          name: alloy-modules-kubernetes-metrics-db96446m8f
        name: modules-kubernetes-metrics
      - configMap:
          name: alloy-modules-kubernetes-logs-7b5f7t9d8g
        name: modules-kubernetes-logs
      - configMap:
          name: alloy-modules-kubernetes-traces-4h9946thb4
        name: modules-kubernetes-traces
      - configMap:
          name: alloy-modules-kubernetes-profiles-8hhkt7m7k9
        name: modules-kubernetes-profiles
      - configMap:
          name: alloy-modules-kubernetes-jobs-749kdbk475
        name: modules-kubernetes-jobs
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
  serviceName: pyroscope-headless
  template:
    metadata:
      annotations:
        checksum/config: b357ed79c949078f193a9e0254cfecb7a6747996038df7d269b85c32469c9077
        profiles.grafana.com/cpu.port_name: http2
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.port_name: http2
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/memory.port_name: http2
        profiles.grafana.com/memory.scrape: "true"
        pyroscope.io/service_name: pyroscope
      labels:
        app.kubernetes.io/component: all
        app.kubernetes.io/instance: pyroscope
        app.kubernetes.io/name: pyroscope
        name: pyroscope
    spec:
      containers:
      - args:
        - -target=all
        - -self-profiling.disable-push=true
        - -server.http-listen-port=4040
        - -memberlist.cluster-label=profiles-system-pyroscope
        - -memberlist.join=dns+pyroscope-memberlist.profiles-system.svc.cluster.local.:7946
        - -config.file=/etc/pyroscope/config.yaml
        - -runtime-config.file=/etc/pyroscope/overrides/overrides.yaml
        - -config.expand-env=true
        - -log.level=debug
        envFrom:
        - secretRef:
            name: pyroscope-env-h982fgc652
        image: grafana/pyroscope:1.5.0
        imagePullPolicy: IfNotPresent
        name: pyroscope
        ports:
        - containerPort: 4040
          name: http2
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http2
            scheme: HTTP
        resources: {}
        securityContext: {}
        volumeMounts:
        - mountPath: /etc/pyroscope/config.yaml
          name: config
          subPath: config.yaml
        - mountPath: /etc/pyroscope/overrides/
          name: overrides-config
        - mountPath: /data
          name: data
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: pyroscope
      volumes:
      - configMap:
          name: pyroscope-config
        name: config
      - configMap:
          name: pyroscope-overrides-config
        name: overrides-config
      - emptyDir: {}
        name: data
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.4.2
    helm.sh/chart: tempo-1.8.0
  name: tempo
  namespace: tracing-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
  serviceName: tempo-headless
  template:
    metadata:
      annotations:
        checksum/config: 6e256c2c082a7070567d783238019b9103b5ad1378811f697cde99adbbe0dd26
        logs.grafana.com/scrape: "true"
        logs.grafana.com/scrub-level: info
        profiles.grafana.com/cpu.port_name: prom-metrics
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.port_name: prom-metrics
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/memory.port_name: prom-metrics
        profiles.grafana.com/memory.scrape: "true"
        pyroscope.io/service_name: tempo
      labels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/conf/tempo.yaml
        - -mem-ballast-size-mbs=1024
        - -config.expand-env=true
        envFrom:
        - secretRef:
            name: tempo-env-gk54k88t7g
        image: grafana/tempo:2.4.1
        imagePullPolicy: IfNotPresent
        name: tempo
        ports:
        - containerPort: 9095
          name: tempo-grpc
        - containerPort: 3100
          name: prom-metrics
        - containerPort: 6831
          name: jaeger-thrift-c
          protocol: UDP
        - containerPort: 6832
          name: jaeger-thrift-b
          protocol: UDP
        - containerPort: 14268
          name: jaeger-thrift-h
        - containerPort: 14250
          name: jaeger-grpc
        - containerPort: 9411
          name: zipkin
        - containerPort: 55680
          name: otlp-legacy
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 55681
          name: otlp-httplegacy
        - containerPort: 4318
          name: otlp-http
        - containerPort: 55678
          name: opencensus
        resources: {}
        volumeMounts:
        - mountPath: /conf
          name: tempo-conf
        - mountPath: /tmp
          name: tmp
      serviceAccountName: tempo
      volumes:
      - configMap:
          name: tempo
        name: tempo-conf
      - emptyDir: {}
        name: tmp
  updateStrategy:
    type: RollingUpdate
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.7.0
    helm.sh/chart: prometheus-node-exporter-4.32.0
  name: prometheus-node-exporter
  namespace: monitoring-system
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: prometheus-node-exporter
      app.kubernetes.io/name: prometheus-node-exporter
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app.kubernetes.io/component: metrics
        app.kubernetes.io/instance: prometheus-node-exporter
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/version: 1.7.0
        helm.sh/chart: prometheus-node-exporter-4.32.0
    spec:
      automountServiceAccountToken: false
      containers:
      - args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --path.udev.data=/host/root/run/udev/data
        - --web.listen-address=[$(HOST_IP)]:9100
        env:
        - name: HOST_IP
          value: 0.0.0.0
        image: quay.io/prometheus/node-exporter:v1.7.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /
            port: 9100
            scheme: HTTP
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: node-exporter
        ports:
        - containerPort: 9100
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /
            port: 9100
            scheme: HTTP
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        securityContext:
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /host/proc
          name: proc
          readOnly: true
        - mountPath: /host/sys
          name: sys
          readOnly: true
        - mountPath: /host/root
          mountPropagation: HostToContainer
          name: root
          readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus-node-exporter
      tolerations:
      - effect: NoSchedule
        operator: Exists
      volumes:
      - hostPath:
          path: /proc
        name: proc
      - hostPath:
          path: /sys
        name: sys
      - hostPath:
          path: /
        name: root
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alloy-mixin-alerts
  namespace: monitoring-system
spec:
  groups:
  - name: clustering
    rules:
    - alert: ClusterNotConverging
      annotations:
        message: 'Cluster is not converging: nodes report different number of peers
          in the cluster.'
      expr: stddev by (cluster, namespace) (sum without (state) (cluster_node_peers))
        != 0
      for: 10m
    - alert: ClusterNodeCountMismatch
      annotations:
        message: Nodes report different number of peers vs. the count of observed
          Alloy metrics. Some Alloy metrics may be missing or the cluster is in a
          split brain state.
      expr: |
        sum without (state) (cluster_node_peers) !=
        on (cluster, namespace) group_left
        count by (cluster, namespace) (cluster_node_info)
      for: 15m
    - alert: ClusterNodeUnhealthy
      annotations:
        message: Cluster node is reporting a gossip protocol health score > 0.
      expr: |
        cluster_node_gossip_health_score > 0
      for: 10m
    - alert: ClusterNodeNameConflict
      annotations:
        message: A node tried to join the cluster with a name conflicting with an
          existing peer.
      expr: sum by (cluster, namespace) (rate(cluster_node_gossip_received_events_total{event="node_conflict"}[2m]))
        > 0
      for: 10m
    - alert: ClusterNodeStuckTerminating
      annotations:
        message: Cluster node stuck in Terminating state.
      expr: sum by (cluster, namespace, instance) (cluster_node_peers{state="terminating"})
        > 0
      for: 10m
    - alert: ClusterConfigurationDrift
      annotations:
        message: Cluster nodes are not using the same configuration file.
      expr: |
        count without (sha256) (
            max by (cluster, namespace, sha256) (alloy_config_hash and on(cluster, namespace) cluster_node_info)
        ) > 1
      for: 5m
  - name: alloy_controller
    rules:
    - alert: SlowComponentEvaluations
      annotations:
        message: Component evaluations are taking too long.
      expr: sum by (cluster, namespace, component_path, component_id) (rate(alloy_component_evaluation_slow_seconds[10m]))
        > 0
      for: 15m
    - alert: UnhealthyComponents
      annotations:
        message: Unhealthy components detected.
      expr: sum by (cluster, namespace) (alloy_component_controller_running_components{health_type!="healthy"})
        > 0
      for: 15m
  - name: otelcol
    rules:
    - alert: OtelcolReceiverRefusedSpans
      annotations:
        message: The receiver could not push some spans to the pipeline.
      expr: sum(rate(receiver_refused_spans_ratio_total{}[1m])) > 0
      for: 5m
    - alert: OtelcolExporterFailedSpans
      annotations:
        message: The exporter failed to send spans to their destination.
      expr: sum(rate(exporter_send_failed_spans_ratio_total{}[1m])) > 0
      for: 5m
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  endpoints:
  - interval: 15s
    port: http-metrics
    relabelings:
    - action: replace
      replacement: logging-system/loki
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - logging-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
  namespace: monitoring-system
spec:
  endpoints:
  - honorLabels: true
    port: http-metrics
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/mimir
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  endpoints:
  - port: http2
    relabelings:
    - action: replace
      replacement: profiles-system/pyroscope
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - profiles-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tempo
  namespace: tracing-system
spec:
  endpoints:
  - interval: 15s
    port: tempo-prom-metrics
    relabelings:
    - action: replace
      replacement: tracing-system/tempo
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - tracing-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.1.0
    helm.sh/chart: alloy-0.3.1
  name: alloy
  namespace: monitoring-system
spec:
  rules:
  - host: alloy.localhost
    http:
      paths:
      - backend:
          service:
            name: alloy
            port:
              number: 12345
        path: /
        pathType: Prefix
