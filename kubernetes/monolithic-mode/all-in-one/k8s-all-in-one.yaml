apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: logging-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: profiles-system
---
apiVersion: v1
kind: Namespace
metadata:
  name: tracing-system
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.3.1
    helm.sh/chart: tempo-1.7.2
  name: tempo
  namespace: tracing-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: profiles-system-pyroscope
  namespace: profiles-system
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-clusterrole
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: profiles-system-pyroscope
  namespace: profiles-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: profiles-system-pyroscope
subjects:
- kind: ServiceAccount
  name: pyroscope
  namespace: profiles-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy
subjects:
- kind: ServiceAccount
  name: alloy
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: loki-clusterrole
subjects:
- kind: ServiceAccount
  name: loki
  namespace: logging-system
---
apiVersion: v1
data:
  gateway_loki.conf.template: |-
    server {
        listen 3100;
        listen [::]:3100;

        location = / {
          return 200 'OK';
          auth_basic off;
          access_log off;
        }

        proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

        # Distributor endpoints
        location = /api/prom/push {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/push {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }
        location = /distributor/ring {
          proxy_pass      http://${LOKI_DISTRIBUTOR_HOST}:3100$request_uri;
        }

        # Ingester endpoints
        location /flush {
          proxy_pass      http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }
        location ^~ /ingester/ {
          proxy_pass      http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }
        location = /ingester {
          internal;        # to suppress 301
        }

        # Ring
        location = /ring {
          proxy_pass http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }

        # MemberListKV
        location = /memberlist {
          proxy_pass http://${LOKI_INGESTER_HOST}:3100$request_uri;
        }


        # Ruler endpoints
        location = /ruler/ring {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /api/prom/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /api/prom/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /prometheus/api/v1/alerts.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }
        location ~ /prometheus/api/v1/rules.* {
          proxy_pass      http://${LOKI_RULER_HOST}:3100$request_uri;
        }


        # Compactor endpoints
        location = /compactor/ring {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/delete {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }
        location = /loki/api/v1/cache/generation_numbers {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # IndexGateway endpoints
        location = /indexgateway/ring {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # Config endpoints
        location = /config {
          proxy_pass      http://${LOKI_COMPACTOR_HOST}:3100$request_uri;
        }

        # QueryFrontend, Querier endpoints
        location = /api/prom/tail {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
        }
        location = /loki/api/v1/tail {
          proxy_pass      http://${LOKI_QUERIER_HOST}:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
        }
        location ~ /api/prom/.* {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
        }
        location ~ /loki/api/v1.* {
          proxy_pass      http://${LOKI_QUERY_FRONTEND_HOST}:3100$request_uri;
        }
      }
  gateway_mimir.conf.template: "server {\n    listen 8080;\n    listen [::]:8080;\n\n
    \   location = / {\n      return 200 'OK';\n      auth_basic off;\n      access_log
    off;\n    }\n\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n
    \   # Distributor endpoints\n    location /distributor {\n      proxy_pass      http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location = /api/v1/push {\n      proxy_pass      http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location /otlp/v1/metrics {\n      proxy_pass      http://${MIMIR_DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass
    \     http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n    location
    = /multitenant_alertmanager/status {\n      proxy_pass      http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n    # https://github.com/grafana/mimir/releases/tag/mimir-2.12.0\n    #
    Alertmanager deprecated the v1 API. All endpoints have a v2 equivalent.\n    location
    = /api/v2/alerts {\n      proxy_pass      http://${MIMIR_ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n      proxy_pass
    \     http://${MIMIR_RULER_HOST}:8080$request_uri;\n    }\n    location /prometheus/api/v1/rules
    {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n    }\n
    \   \n    location /prometheus/api/v1/alerts {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n
    \   }\n    location = /ruler/ring {\n      proxy_pass      http://${MIMIR_RULER_HOST}:8080$request_uri;\n
    \   }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus
    {\n      proxy_pass      http://${MIMIR_QUERY_FRONTEND_HOST}:8080$request_uri;\n
    \   }\n\n    # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo
    {\n      proxy_pass      http://${MIMIR_QUERY_FRONTEND_HOST}:8080$request_uri;\n
    \   }\n\n    # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/
    {\n      proxy_pass      http://${MIMIR_COMPACTOR_HOST}:8080$request_uri;\n    }\n}"
  gateway_pyroscope.conf.template: |-
    server {
        listen 4040;
        listen [::]:4040;

        location = / {
          return 200 'OK';
          auth_basic off;
          access_log off;
        }

        proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

        # Distributor endpoints
        location /push.v1.PusherService {
          proxy_pass      http://${PYROSCOPE_DISTRIBUTOR_HOST}:4040$request_uri;
        }

        location /querier.v1.QuerierService {
          proxy_pass      http://${PYROSCOPE_QUERY_FRONTEND_HOST}:4040$request_uri;
        }
    }
  gateway_tempo.conf.template: "upstream grpc_otlp_tempo {\n    server ${TEMPO_DISTRIBUTOR_HOST}:4317;\n}\nserver
    {\n    listen 4317;\n    http2 on;\n\n    location / {\n      grpc_set_header
    X-Scope-OrgID $ensured_x_scope_orgid;\n      grpc_pass grpc://grpc_otlp_tempo;\n
    \   }\n}\n\nupstream http_otlp_tempo {\n    server ${TEMPO_DISTRIBUTOR_HOST}:4318;\n}\nserver
    {\n    listen 4318;\n\n    location / {\n      proxy_set_header X-Scope-OrgID
    $ensured_x_scope_orgid;\n      proxy_pass http://http_otlp_tempo;\n    }\n}\n\nserver
    {\n    listen 3200;\n    listen [::]:3200;\n\n    location = / {\n      return
    200 'OK';\n      auth_basic off;\n      access_log off;\n    }\n\n    proxy_set_header
    X-Scope-OrgID $ensured_x_scope_orgid;\n\n    # Distributor endpoints\n    location
    = /jaeger/api/traces {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:14268/api/traces;\n
    \   }\n    location = /zipkin/spans {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:9411/spans;\n
    \   }\n    location = /otlp/v1/traces {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:4318/v1/traces;\n
    \   }\n\n    location = /distributor/ring {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:3100$request_uri;\n
    \   }\n    location = /ingester/ring {\n      proxy_pass      http://${TEMPO_DISTRIBUTOR_HOST}:3100$request_uri;\n
    \   }\n    \n    # Ingester endpoints\n    location = /flush {\n      proxy_pass
    \     http://${TEMPO_INGESTER_HOST}:3100$request_uri;\n    }\n    location = /shutdown
    {\n      proxy_pass      http://${TEMPO_INGESTER_HOST}:3100$request_uri;\n    }\n\n
    \   # Query endpoints\n    location ^~ /api {\n      proxy_pass      http://${TEMPO_QUERY_FRONTEND_HOST}:3100$request_uri;\n
    \   }\n\n    # Compactor endpoint\n    location = /compactor/ring {\n      proxy_pass
    \     http://${TEMPO_COMPACTOR_HOST}:3100$request_uri;\n    }\n}"
kind: ConfigMap
metadata:
  name: nginx-templates
  namespace: gateway
---
apiVersion: v1
data:
  config.yaml: |2

    auth_enabled: false

    # -reporting.enabled=false
    analytics:
     reporting_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9095
      log_level: info
      log_format: json

    # https://grafana.com/docs/loki/latest/configure/#use-environment-variables-in-the-configuration
    common:
      compactor_address: http://loki.logging-system.svc.cluster.local:3100
      path_prefix: /var/loki
      replication_factor: 1
      storage:
        s3:
          bucketnames: loki-data
          endpoint: ${LOKI_S3_ENDPOINT:-minio.minio-system.svc.cluster.local:443}
          access_key_id: ${LOKI_S3_ACCESS_KEY_ID:-lgtmp}
          secret_access_key: ${LOKI_S3_SECRET_ACCESS_KEY:-supersecret}
          insecure: ${LOKI_S3_INSECURE:-false}
          s3forcepathstyle: true
          http_config:
            insecure_skip_verify: true

    bloom_gateway:
      enabled: true
      client:
        addresses: "dns+loki.logging-system.svc.cluster.local:9095"
        cache_results: true
        results_cache:
          cache:
            memcached_client:
              addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

    bloom_compactor:
      enabled: true
      ring:
        kvstore:
          store: memberlist

    index_gateway:
      mode: simple

    compactor:
      working_directory: /tmp/compactor

    memberlist:
      join_members:
      - loki-memberlist.logging-system.svc.cluster.local:7946

    # https://github.com/grafana/loki/blob/main/docs/sources/configure/_index.md#query_range
    query_range:
      align_queries_with_step: true

      cache_results: true
      results_cache:
        cache:
          memcached_client:
            addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

      cache_index_stats_results: true
      index_stats_results_cache:
        cache:
          memcached_client:
            addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"

    pattern_ingester:
      enabled: true

    limits_config:
      max_global_streams_per_user: 0
      ingestion_rate_mb: 50000
      ingestion_burst_size_mb: 50000
      volume_enabled: true

    ruler:
      storage:
        s3:
          bucketnames: loki-ruler
        type: s3

    runtime_config:
      file: /etc/loki/runtime-config/runtime-config.yaml

    schema_config:
      configs:
      - from: "2024-04-08"
        index:
          period: 24h
          prefix: loki_index_
        object_store: s3
        schema: v13
        store: tsdb

    storage_config:
      tsdb_shipper:
        active_index_directory: /var/loki/index
        cache_location: /var/loki/cache
        index_gateway_client:
          server_address: "dns+loki.logging-system.svc.cluster.local:9095"

    chunk_store_config:
      chunk_cache_config:
        memcached_client:
          addresses: "dns+memcached.memcached-system.svc.cluster.local:11211"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-config-5468cb29k9
  namespace: logging-system
---
apiVersion: v1
data:
  runtime-config.yaml: |
    # This file can be used to set overrides or other runtime config.
    overrides:
      "fake": # limits for anonymous that the whole cluster enforces
        ingestion_rate_mb: 1500000
        max_streams_per_user: 100000
        max_chunks_per_query: 100000
      "anonymous": # limits for anonymous that the whole cluster enforces
        ingestion_rate_mb: 1500000
        max_streams_per_user: 100000
        max_chunks_per_query: 100000
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-runtime-9599m5k6h2
  namespace: logging-system
---
apiVersion: v1
data:
  config.alloy: "logging {\n\tlevel  = coalesce(env(\"ALLOY_LOG_LEVEL\"), \"info\")\n\tformat
    = \"logfmt\"\n}\n\n/********************************************\n * Grafana LGTMP
    Stack Receiver Provider\n ********************************************/\nimport.git
    \"provider\" {\n\trepository     = \"https://github.com/qclaogui/codelab-monitoring.git\"\n\trevision
    \      = \"main\"\n\tpath           = \"alloy-modules/provider\"\n\tpull_frequency
    = \"24h\"\n}\n\nprovider.self_hosted_stack \"kubernetes\" {\n\tmetrics_endpoint_url
    \ = coalesce(env(\"SELF_HOSTED_METRICS_ENDPOINT_URL\"), \"http://nginx.gateway.svc:8080/api/v1/push\")\n\tlogs_endpoint_url
    \    = coalesce(env(\"SELF_HOSTED_LOGS_ENDPOINT_URL\"), \"http://nginx.gateway.svc:3100/loki/api/v1/push\")\n\ttraces_endpoint_url
    \  = coalesce(env(\"SELF_HOSTED_TRACES_ENDPOINT_URL\"), \"http://nginx.gateway.svc:4318\")\n\tprofiles_endpoint_url
    = coalesce(env(\"SELF_HOSTED_PROFILES_ENDPOINT_URL\"), \"http://nginx.gateway.svc:4040\")\n}\n\n/********************************************\n
    * Metrics\n ********************************************/\nimport.file \"metrics\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/metrics\"\n}\n\nmetrics.integrations_scrape \"kubernetes\" {\n\tforward_to
    = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\nmetrics.podmonitors_scrape
    \"kubernetes\" {\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\nmetrics.servicemonitors_scrape
    \"kubernetes\" {\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n/********************************************\n
    * Logs\n ********************************************/\nimport.file \"logs\" {\n\tfilename
    = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\") + \"/kubernetes/logs\"\n}\n\nlogs.annotations_scrape
    \"kubernetes\" {\n\tforward_to        = [logs.keep_labels.kubernetes.receiver]\n\tannotation_prefix
    = \"logs.grafana.com\"\n}\n\nlogs.keep_labels \"kubernetes\" {\n\tforward_to =
    [provider.self_hosted_stack.kubernetes.logs_receiver]\n}\n\n/********************************************\n
    * Traces\n ********************************************/\nimport.file \"traces\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/traces\"\n}\n\n// traces Processing And Transformation process_and_transform\ntraces.process_and_transform
    \"kubernetes\" {\n\tmetrics_forward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n\tlogs_forward_to
    \   = [provider.self_hosted_stack.kubernetes.logs_receiver]\n\ttraces_forward_to
    \ = [provider.self_hosted_stack.kubernetes.traces_receiver]\n}\n\ntracing {\n\t//
    Write all spans. Don't do this in production!\n\tsampling_fraction = 1\n\n\t//
    Forward Alloy internal spans to traces process.\n\twrite_to = [traces.process_and_transform.kubernetes.alloy_traces_input]\n}\n\n/********************************************\n
    * Profiles\n ********************************************/\nimport.file \"profiles\"
    {\n\tfilename = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/profiles\"\n}\n\nprofiles.annotations_scrape \"kubernetes\" {\n\tforward_to
    = [provider.self_hosted_stack.kubernetes.profiles_receiver]\n\t// annotation_prefix
    = \"profiles.grafana.com\"\n}\n"
kind: ConfigMap
metadata:
  name: alloy-config-6b695gbdm6
  namespace: monitoring-system
---
apiVersion: v1
data:
  memcached.alloy: "/*\nModule Components: component_memcached\n*/\n\ndeclare \"component_memcached\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"memcached_address\" {\n\t\tcomment
    \ = \"address of the Memcached\"\n\t\toptional = true\n\t\tdefault  = \"memcached:11211\"\n\t}\n\n\targument
    \"memcached_timeout\" {\n\t\tcomment  = \"timeout of the Memcached\"\n\t\toptional
    = true\n\t\tdefault  = \"5s\"\n\t}\n\n\targument \"instance_name\" {\n\t\tcomment
    \ = \"instance of the Memcached\"\n\t\toptional = true\n\t}\n\n\targument \"keep_metrics\"
    {\n\t\toptional = true\n\t\tdefault  = \"(up|memcached_commands_total|memcached_connections_total|memcached_current_bytes|memcached_current_connections|memcached_current_items|memcached_items_evicted_total|memcached_items_total|memcached_max_connections|memcached_read_bytes_total|memcached_up|memcached_uptime_seconds|memcached_version|memcached_written_bytes_total)\"\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t\tdefault  = \"60s\"\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t\tdefault  = \"10s\"\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Memcached\n\t****************************************************************/\n\t//
    https://grafana.com/docs/alloy/latest/reference/components/prometheus.exporter.memcached/\n\tprometheus.exporter.memcached
    \"integrations_memcached_exporter\" {\n\t\taddress = argument.memcached_address.value\n\t\ttimeout
    = argument.memcached_timeout.value\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_memcached_exporter\" {\n\t\ttargets = prometheus.exporter.memcached.integrations_memcached_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = \"integrations/kubernetes/memcached\"\n\t\t}\n\n\t\trule
    {\n\t\t\ttarget_label = \"instance\"\n\t\t\treplacement  = coalesce(argument.instance_name.value,
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_memcached_exporter\" {\n\t\ttargets = concat(\n\t\t\tdiscovery.relabel.integrations_memcached_exporter.output,\n\t\t)\n\n\t\tenable_protobuf_negotiation
    = true\n\t\tscrape_classic_histograms   = true\n\n\t\tscrape_interval = argument.scrape_interval.value\n\t\tscrape_timeout
    \ = argument.scrape_timeout.value\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to
    = [prometheus.relabel.integrations_memcached_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_memcached_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = argument.keep_metrics.value\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-integrations-9k86fb8hkg
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for logs based on kubernetes Pod annotations\n\n  Annotations:\n
    \   logs.grafana.com/ingest: true\n    logs.grafana.com/tenant: \"primary\"\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  =
    \"The tenant to filter logs to.  This does not have to be the tenantId, this is
    the value to look for in the logs.agent.grafana.com/tenant annotation, and this
    can be a regex.\"\n\t\toptional = true\n\t\tdefault  = \".*\"\n\t}\n\n\t// arguments
    for kubernetes discovery\n\targument \"namespaces\" {\n\t\tcomment  = \"The namespaces
    to look for targets in (default: [\\\"kube-system\\\"] is all namespaces)\"\n\t\toptional
    = true\n\t}\n\n\targument \"annotation_prefix\" {\n\t\tcomment  = \"The annotation_prefix
    to use (default: logs.grafana.com)\"\n\t\tdefault  = \"logs.grafana.com\"\n\t\toptional
    = true\n\t}\n\n\targument \"__sd_annotation\" {\n\t\toptional = true\n\t\tcomment
    \ = \"The logic is used to transform the annotation argument into a valid label
    name by removing unsupported characters.\"\n\t\tdefault  = replace(replace(replace(coalesce(argument.annotation_prefix.value,
    \"logs.grafana.com\"), \".\", \"_\"), \"/\", \"_\"), \"-\", \"_\")\n\t}\n\n\t//
    find all pods\n\tdiscovery.kubernetes \"annotation_logs\" {\n\t\trole = \"pod\"\n\n\t\tnamespaces
    {\n\t\t\tnames = coalesce(argument.namespaces.value, [])\n\t\t}\n\t}\n\n\t// filter
    logs by kubernetes annotations\n\tdiscovery.relabel \"annotation_logs_filter\"
    {\n\t\ttargets = discovery.kubernetes.annotation_logs.targets\n\n\t\t// allow
    pods to declare their logs to be ingested or not, the default is true\n\t\t//
    \  i.e. logs.grafana.com/ingest: false\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\" + argument.__sd_annotation.value
    + \"_scrape\",\n\t\t\t]\n\t\t\tregex = \"^(true|)$\"\n\t\t}\n\n\t\t// allow pods
    to declare what tenant their logs should be written to, the following annotation
    is supported:\n\t\t//   logs.grafana.com/tenant: \"primary\"\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\"
    + argument.__sd_annotation.value + \"_tenant\",\n\t\t\t]\n\t\t\tregex = \"^(\"
    + argument.tenant.value + \")$\"\n\t\t}\n\n\t\t// set the instance label as the
    name of the worker node the pod is on\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_node_name\"]\n\t\t\ttarget_label  = \"instance\"\n\t\t}\n\n\t\t//
    set the namespace label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the pod label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\t//
    set the container label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\n\t\t// set a workload label\n\t\trule {\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \"/\"\n\t\t\ttarget_label = \"workload\"\n\t\t}\n\t\t// remove the hash
    from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels = [\"workload\"]\n\t\t\tregex
    \        = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label  = \"workload\"\n\t\t}\n\n\t\t//
    set the app name if specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"
    or \"k8s-app:\"\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the component if specified
    as metadata labels \"component:\" or \"app.kubernetes.io/component:\" or \"k8s-component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the version if specified as metadata labels
    \"version:\" or \"app.kubernetes.io/version:\" or \"app_version:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app_version\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"version\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\n\t\t//
    set the job label to be namespace / friendly pod name\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"workload\",\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t]\n\t\t\tregex
    \       = \".+\\\\/(.+);(.+)\"\n\t\t\treplacement  = \"$2/$1\"\n\t\t\ttarget_label
    = \"job\"\n\t\t}\n\n\t\t// make all labels on the pod available to the pipeline
    as labels,\n\t\t// they are omitted before write via labelallow unless explicitly
    set\n\t\trule {\n\t\t\taction = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\t//
    make all annotations on the pod available to the pipeline as labels,\n\t\t// they
    are omitted before write via labelallow unless explicitly set\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_annotation_(.+)\"\n\t\t}\n\n\t\t//
    as a result of kubernetes service discovery for pods, all of the meta data information
    is exposed in labels\n\t\t// __meta_kubernetes_pod_*, including __meta_kubernetes_pod_container_id
    which can be used to determine what\n\t\t// the pods container runtime is, docker
    (docker://...) or containerd (containerd://...) this will inform us\n\t\t// which
    parsing stage to use.  However, any labels that begin with __* are not passed
    to loki.process\n\t\t// (pipeline) stages. Use a relabeling stage to set a label
    that can be used a LogQL selector in the stage\n\t\t// below so parsing can be
    automatically determined, then drop the label from the loki.process stage.\n\t\t//
    set the container runtime as a label\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_id\"]\n\t\t\tregex         = \"^(\\\\w+):\\\\/\\\\/.+$\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t\ttarget_label  = \"tmp_container_runtime\"\n\t\t}\n\t}\n\n\tloki.source.kubernetes
    \"lsd_kubernetes_logs\" {\n\t\ttargets    = discovery.relabel.annotation_logs_filter.output\n\t\tforward_to
    = [loki.process.parse.receiver]\n\t}\n\n\t// parse the log based on the container
    runtime\n\tloki.process \"parse\" {\n\t\tforward_to = argument.forward_to.value\n\t\t/*******************************************************************************\n\t\t*
    \                        Container Runtime Parsing\n\t\t********************************************************************************/\n\t\t//
    if the label tmp_container_runtime from above is containerd parse using cri\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"containerd\\\"}\"\n\t\t\t// the
    cri processing stage extracts the following k/v pairs: log, stream, time, flags\n\t\t\tstage.cri
    { }\n\n\t\t\t// Set the extract flags and stream values as labels\n\t\t\tstage.labels
    {\n\t\t\t\tvalues = {\n\t\t\t\t\tflags  = \"\",\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t//
    if the label tmp_container_runtime from above is docker parse using docker\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"docker\\\"}\"\n\t\t\t// the docker
    processing stage extracts the following k/v pairs: log, stream, time\n\t\t\tstage.docker
    { }\n\n\t\t\t// Set the extract stream value as a label\n\t\t\tstage.labels {\n\t\t\t\tvalues
    = {\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// drop the temporary
    container runtime label as it is no longer needed\n\t\tstage.label_drop {\n\t\t\tvalues
    = [\"tmp_container_runtime\"]\n\t\t}\n\t}\n}\n"
  keep-labels.alloy: "/*\nModule Components: keep_labels\nDescription: Pre-defined
    set of labels to keep, this stage should always be in-place as the previous relabeing\n
    \            stages make every pod label and annotation a label in the pipeline,
    which we do not want created\n             in Loki as that would have extremely
    high-cardinality.\n*/\n\ndeclare \"keep_labels\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"keep_labels\" {\n\t\toptional
    = true\n\t\tcomment  = \"List of labels to keep before the log message is written
    to Loki\"\n\t\tdefault  = [\n\t\t\t\"app\",\n\t\t\t\"cluster\",\n\t\t\t\"component\",\n\t\t\t\"container\",\n\t\t\t\"env\",\n\t\t\t\"job\",\n\t\t\t\"level\",\n\t\t\t\"namespace\",\n\t\t\t\"region\",\n\t\t\t\"service\",\n\t\t\t\"squad\",\n\t\t\t\"team\",\n\t\t\t\"workload\",\n\t\t]\n\t}\n\n\t/*****************************************************************\n\t*
    LOKI PROCESS\n\t*****************************************************************/\n\tloki.process
    \"keep_labels\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t/*\n\t\tAs
    all of the pod labels and annotations we transformed into labels in the previous
    relabelings to make\n\t\tthem available to the pipeline processing we need to
    ensure they are not automatically created in Loki.\n\t\tThis would result in an
    extremely high number of labels and values severely impacting query performance.\n\t\tNot
    every log has to contain these labels, but this list should reflect the set of
    labels that you want\n\t\tto explicitly allow.\n\t\t*/\n\t\tstage.label_keep {\n\t\t\tvalues
    = argument.keep_labels.value\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"receiver\" {\n\t\tvalue = loki.process.keep_labels.receiver\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-logs-d7c756mt2f
  namespace: monitoring-system
---
apiVersion: v1
data:
  integrations-scrape.alloy: "/*\nModule Components: integrations_scrape\nDescription:
    Integrations Module Components Scrape\n\nNote: Every argument except for \"forward_to\"
    is optional, and does have a defined default value.  However, the values for these\n
    \     arguments are not defined using the default = \" ... \" argument syntax,
    but rather using the coalesce(argument.value, \" ... \").\n      This is because
    if the argument passed in from another consuming module is set to null, the default
    = \" ... \" syntax will\n      does not override the value passed in, where coalesce()
    will return the first non-null value.\n*/\n\ndeclare \"integrations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"name\" {\n\t\tcomment  =
    \"Name of the integrations config\"\n\t\toptional = true\n\t\tdefault  = \"alloy-integrations\"\n\t}\n\n\targument
    \"namespace\" {\n\t\tcomment  = \"Namespace of the integrations config\"\n\t\toptional
    = true\n\t\tdefault  = \"default\"\n\t}\n\n\t/*****************************************************************\n\t*
    Import Integrations Components\n\t*****************************************************************/\n\t//
    integrate components local files\n\timport.file \"integrate\" {\n\t\tfilename
    = coalesce(env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\") + \"/kubernetes/integrations\"\n\t}\n\n\t//
    // integrate components kubernetes configmap\n\t// remote.kubernetes.configmap
    \"integrations\" {\n\t// \tname      = argument.name.value\n\t// \tnamespace =
    argument.namespace.value\n\t// }\n\t// import.string \"integrate\" {\n\t// \tcontent
    = remote.kubernetes.configmap.integrations.data[\"memcached.alloy\"]\n\t// }\n\n\t/*****************************************************************\n\t*
    Memcached Integrations\n\t*****************************************************************/\n\tintegrate.component_memcached
    \"primary\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\tinstance_name
    \    = \"primary\"\n\t\tmemcached_address = \"memcached.memcached-system.svc.cluster.local:11211\"\n\t\tmemcached_timeout
    = \"5s\"\n\t}\n}\n"
  podmonitors-scrape.alloy: "/*\nModule Components: podmonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.podmonitors\n*/\n\ndeclare
    \"podmonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape PodMonitors\n\t*****************************************************************/\n\tprometheus.operator.podmonitors
    \"scrape\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\n\t\t// selector {\n\t\t// \tmatch_expression {\n\t\t// \t\tkey
    \     = \"team\"\n\t\t// \t\toperator = \"In\"\n\t\t// \t\tvalues   = [\"team-infra\"]\n\t\t//
    \t}\n\t\t// }\n\t}\n}\n"
  servicemonitors-scrape.alloy: "/*\nModule Components: servicemonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.servicemonitors\n*/\n\ndeclare
    \"servicemonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape ServiceMonitors\n\t*****************************************************************/\n\tprometheus.operator.servicemonitors
    \"scrape\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-metrics-tf9cd8b6bg
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for metrics based on kubernetes Pod annotations\n\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(ProfilessReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\tdiscovery.kubernetes \"pyroscope_kubernetes\"
    {\n\t\trole = \"pod\"\n\t}\n\n\t// The default scrape config allows to define
    annotations based scraping.\n\t//\n\t// For example the following annotations:\n\t//\n\t//
    ```\n\t// profiles.grafana.com/memory.scrape: \"true\"\n\t// profiles.grafana.com/memory.port:
    \"8080\"\n\t// profiles.grafana.com/cpu.scrape: \"true\"\n\t// profiles.grafana.com/cpu.port:
    \"8080\"\n\t// profiles.grafana.com/goroutine.scrape: \"true\"\n\t// profiles.grafana.com/goroutine.port:
    \"8080\"\n\t// ```\n\t//\n\t// will scrape the `memory`, `cpu` and `goroutine`
    profiles from the `8080` port of the pod.\n\t//\n\t// For more information see
    https://grafana.com/docs/phlare/latest/operators-guide/deploy-kubernetes/#optional-scrape-your-own-workloads-profiles\n\tdiscovery.relabel
    \"kubernetes_pods\" {\n\t\ttargets = concat(discovery.kubernetes.pyroscope_kubernetes.targets)\n\n\t\trule
    {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_phase\"]\n\t\t\tregex
    \        = \"Pending|Succeeded|Failed|Completed\"\n\t\t}\n\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_memory_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_memory_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Memory\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_memory\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_memory_default_name.output, discovery.relabel.kubernetes_pods_memory_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_cpu_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_cpu_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape CPU\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_cpu\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_cpu_default_name.output, discovery.relabel.kubernetes_pods_cpu_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_goroutine_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_goroutine_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Goroutine\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_goroutine\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_goroutine_default_name.output,
    discovery.relabel.kubernetes_pods_goroutine_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled =
    false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_block_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_block_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Block\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_block\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_block_default_name.output, discovery.relabel.kubernetes_pods_block_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_mutex_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_mutex_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Mutex\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_mutex\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_mutex_default_name.output, discovery.relabel.kubernetes_pods_mutex_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_fgprof_default_name\"
    {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_fgprof_custom_name\" {\n\t\ttargets = concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Fgprof\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_fgprof\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = concat(discovery.relabel.kubernetes_pods_fgprof_default_name.output, discovery.relabel.kubernetes_pods_fgprof_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-profiles-66c27bc84g
  namespace: monitoring-system
---
apiVersion: v1
data:
  process-and-transform.alloy: "/*\nModule Components: process_and_transform\n\nDescription:
    Traces data collection processing and transformation\n*/\n\n// Processing And
    Transformation\ndeclare \"process_and_transform\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"traces_forward_to\" {\n\t\tcomment = \"Must be a list(TracesReceiver) where
    collected traces should be forwarded to\"\n\t}\n\n\targument \"logs_forward_to\"
    {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected logs should be
    forwarded to\"\n\t}\n\n\targument \"metrics_forward_to\" {\n\t\tcomment = \"Must
    be a list(MetricsReceiver) where collected metrics should be forwarded to\"\n\t}\n\n\targument
    \"cluster\" {\n\t\toptional = true\n\t\tdefault  = \"k3d-k3s-codelab\"\n\t}\n\n\targument
    \"otlp_http_endpoint\" {\n\t\toptional = true\n\t\tdefault  = \"0.0.0.0:4318\"\n\t}\n\n\targument
    \"otlp_grpc_endpoint\" {\n\t\toptional = true\n\t\tdefault  = \"0.0.0.0:4317\"\n\t}\n\n\t/*****************************************************************\n\t*
    Jaeger for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.jaeger
    \"default\" {\n\t\tprotocols {\n\t\t\tgrpc {\n\t\t\t\tendpoint = \"0.0.0.0:14250\"\n\t\t\t}\n\n\t\t\tthrift_http
    {\n\t\t\t\tendpoint = \"0.0.0.0:14268\"\n\t\t\t}\n\n\t\t\tthrift_binary {\n\t\t\t\tendpoint
    = \"0.0.0.0:6832\"\n\t\t\t}\n\n\t\t\tthrift_compact {\n\t\t\t\tendpoint = \"0.0.0.0:6831\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.batch.default.input]\n\t\t\tlogs    = [otelcol.processor.resourcedetection.default.input]\n\t\t\ttraces
    \ = [otelcol.processor.resourcedetection.default.input]\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Otelcol for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.otlp
    \"default\" {\n\t\tgrpc {\n\t\t\tendpoint = argument.otlp_grpc_endpoint.value\n\t\t}\n\n\t\thttp
    {\n\t\t\tendpoint = argument.otlp_http_endpoint.value\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics
    = [otelcol.processor.batch.default.input]\n\t\t\tlogs    = [otelcol.processor.resourcedetection.default.input]\n\t\t\ttraces
    \ = [\n\t\t\t\totelcol.processor.resourcedetection.default.input,\n\t\t\t\totelcol.connector.spanlogs.autologging.input,\n\t\t\t]\n\t\t}\n\t}\n\n\totelcol.processor.resourcedetection
    \"default\" {\n\t\tdetectors = [\"env\"]\n\n\t\toutput {\n\t\t\tlogs   = [otelcol.processor.k8sattributes.default.input]\n\t\t\ttraces
    = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.k8sattributes
    \"default\" {\n\t\textract {\n\t\t\tmetadata = [\n\t\t\t\t\"k8s.namespace.name\",\n\t\t\t\t\"k8s.pod.name\",\n\t\t\t\t\"k8s.deployment.name\",\n\t\t\t\t\"k8s.statefulset.name\",\n\t\t\t\t\"k8s.daemonset.name\",\n\t\t\t\t\"k8s.cronjob.name\",\n\t\t\t\t\"k8s.job.name\",\n\t\t\t\t\"k8s.node.name\",\n\t\t\t\t\"k8s.pod.uid\",\n\t\t\t\t\"k8s.pod.start_time\",\n\t\t\t]\n\t\t}\n\n\t\tpod_association
    {\n\t\t\tsource {\n\t\t\t\tfrom = \"connection\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\tlogs   = [otelcol.processor.transform.add_resource_attributes.input]\n\t\t\ttraces
    = [otelcol.processor.transform.add_resource_attributes.input]\n\t\t}\n\t}\n\n\totelcol.processor.transform
    \"add_resource_attributes\" {\n\t\terror_mode = \"ignore\"\n\n\t\tlog_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"pod\"],
    attributes[\"k8s.pod.name\"])`,\n\t\t\t\t`set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])`,\n\t\t\t\t`set(attributes[\"loki.resource.labels\"],
    \"pod, namespace, cluster, job\")`,\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\ttrace_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\toutput
    {\n\t\t\tlogs   = [otelcol.processor.filter.default.input]\n\t\t\ttraces = [otelcol.processor.filter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.filter
    \"default\" {\n\t\terror_mode = \"ignore\"\n\n\t\toutput {\n\t\t\tlogs   = [otelcol.processor.batch.default.input]\n\t\t\ttraces
    = [otelcol.processor.batch.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.batch
    \"default\" {\n\t\tsend_batch_size     = 16384\n\t\tsend_batch_max_size = 0\n\t\ttimeout
    \            = \"5s\"\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.memory_limiter.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.memory_limiter.default.input]\n\t\t\ttraces  = [otelcol.processor.memory_limiter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.memory_limiter
    \"default\" {\n\t\tcheck_interval         = \"1s\"\n\t\tlimit_percentage       =
    50\n\t\tspike_limit_percentage = 30\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.exporter.prometheus.tracesmetrics.input]\n\t\t\tlogs
    \   = [otelcol.exporter.loki.traceslogs.input]\n\t\t\ttraces  = argument.traces_forward_to.value\n\t\t}\n\t}\n\n\totelcol.exporter.prometheus
    \"tracesmetrics\" {\n\t\tforward_to = argument.metrics_forward_to.value\n\t}\n\n\totelcol.exporter.loki
    \"traceslogs\" {\n\t\tforward_to = [loki.process.traceslogs.receiver]\n\t}\n\n\t//
    The OpenTelemetry spanlog connector processes incoming trace spans and extracts
    data from them ready\n\t// for logging.\n\totelcol.connector.spanlogs \"autologging\"
    {\n\t\t// We only want to output a line for each root span (ie. every single trace),
    and not for every\n\t\t// process or span (outputting a line for every span would
    be extremely verbose).\n\t\tspans     = false\n\t\troots     = true\n\t\tprocesses
    = false\n\n\t\t// We want to ensure that the following three span attributes are
    included in the log line, if present.\n\t\tspan_attributes = [\n\t\t\t\"http.method\",\n\t\t\t\"http.target\",\n\t\t\t\"http.status_code\",\n\t\t]\n\n\t\t//
    Overrides the default key in the log line to be `traceId`, which is then used
    by Grafana to\n\t\t// identify the trace ID for correlation with the Tempo datasource.\n\t\toverrides
    {\n\t\t\ttrace_id_key = \"traceId\"\n\t\t}\n\n\t\t// Send to the OpenTelemetry
    Loki exporter.\n\t\toutput {\n\t\t\tlogs = [otelcol.exporter.loki.autologging.input]\n\t\t}\n\t}\n\n\t//
    Simply forwards the incoming OpenTelemetry log format out as a Loki log.\n\t//
    We need this stage to ensure we can then process the logline as a Loki object.\n\totelcol.exporter.loki
    \"autologging\" {\n\t\tforward_to = [loki.process.autologging.receiver]\n\t}\n\n\t//
    The Loki processor allows us to accept a correctly formatted Loki log and mutate
    it into\n\t// a set of fields for output.\n\tloki.process \"autologging\" {\n\t\t//
    The JSON stage simply extracts the `body` (the actual logline) from the Loki log,
    ignoring\n\t\t// all other fields.\n\t\tstage.json {\n\t\t\texpressions = {\"body\"
    = \"\"}\n\t\t}\n\t\t// The output stage takes the body (the main logline) and
    uses this as the source for the output\n\t\t// logline. In this case, it essentially
    turns it into logfmt.\n\t\tstage.output {\n\t\t\tsource = \"body\"\n\t\t}\n\n\t\tforward_to
    = [loki.process.traceslogs.receiver]\n\t}\n\n\tloki.process \"traceslogs\" {\n\t\tstage.tenant
    {\n\t\t\tvalue = \"anonymous\"\n\t\t}\n\n\t\tforward_to = argument.logs_forward_to.value\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"alloy_traces_input\" {\n\t\tvalue = otelcol.processor.batch.default.input\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-traces-8mgm8th9m5
  namespace: monitoring-system
---
apiVersion: v1
data:
  alertmanager_fallback_config.yaml: |
    route:
      group_wait: 0s
      receiver: empty-receiver

    receivers:
      # In this example we're not going to send any notification out of Alertmanager.
      - name: 'empty-receiver'
  mimir.yaml: |
    # Do not use this configuration in production.
    # It is for demonstration purposes only.
    multitenancy_enabled: false

    # -usage-stats.enabled=false
    usage_stats:
      enabled: false

    server:
      http_listen_port: 8080
      grpc_listen_port: 9095
      log_level: info

    # https://grafana.com/docs/mimir/latest/references/configuration-parameters/#use-environment-variables-in-the-configuration
    common:
      storage:
        backend: s3
        s3:
          endpoint:          ${MIMIR_S3_ENDPOINT:minio.minio-system.svc:443}
          access_key_id:     ${MIMIR_S3_ACCESS_KEY_ID:lgtmp}
          secret_access_key: ${MIMIR_S3_SECRET_ACCESS_KEY:supersecret}
          insecure:          ${MIMIR_S3_INSECURE:false}
          http:
            insecure_skip_verify: true

    alertmanager:
      data_dir: /data/alertmanager
      enable_api: true
      external_url: /alertmanager
      fallback_config_file: /etc/mimir/alertmanager_fallback_config.yaml
    alertmanager_storage:
      s3:
        bucket_name: mimir-alertmanager


    memberlist:
      join_members: [ mimir-memberlist:7946 ]

    ingester:
      ring:
        replication_factor: 1

    store_gateway:
      sharding_ring:
        replication_factor: 1


    blocks_storage:
      s3:
        bucket_name: mimir-blocks
      tsdb:
        dir: /data/ingester
        ship_interval: 1m
        block_ranges_period: [ 2h ]
        retention_period: 3h
      bucket_store:
        index_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        chunks_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        metadata_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

    ruler:
      rule_path: /data/rules
      enable_api: true
      alertmanager_url: http://localhost:8080/alertmanager
    ruler_storage:
      s3:
        bucket_name: mimir-ruler
      cache:
        backend: memcached
        memcached:
          addresses: dns+memcached.memcached-system.svc:11211

    compactor:
      compaction_interval: 30s
      data_dir: /data/mimir-compactor
      cleanup_interval:    1m
      tenant_cleanup_delay: 1m

    limits:
      native_histograms_ingestion_enabled: true

    overrides_exporter:
      ring:
        enabled: true
        wait_stability_min_duration: 30s

    runtime_config:
      file: /etc/mimir/runtime.yaml
  runtime.yaml: |-
    # This file can be used to set overrides or other runtime config.
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_fetched_series_per_query: 100000
        native_histograms_ingestion_enabled: true
        ruler_max_rules_per_rule_group: 50
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir-config-958c4gm5k9
  namespace: monitoring-system
---
apiVersion: v1
data:
  config.yaml: |
    analytics:
      reporting_enabled: false

    # https://grafana.com/docs/pyroscope/latest/configure-server/configure-disk-storage/#configure-pyroscope-disk-storage
    pyroscopedb:
      max_block_duration: 5m

    # https://grafana.com/docs/pyroscope/latest/configure-server/reference-configuration-parameters/#use-environment-variables-in-the-configuration
    storage:
      backend: s3
      s3:
        bucket_name: pyroscope-data
        endpoint: ${PYROSCOPE_STORAGE_S3_ENDPOINT:-minio.minio-system.svc:443}
        access_key_id: ${PYROSCOPE_STORAGE_S3_ACCESS_KEY_ID:-lgtmp}
        secret_access_key: ${PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY:-supersecret}
        insecure: ${PYROSCOPE_STORAGE_S3_INSECURE:-false}
        http:
          insecure_skip_verify: true
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-config
  namespace: profiles-system
---
apiVersion: v1
data:
  overrides.yaml: |
    overrides:
      {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-overrides-config
  namespace: profiles-system
---
apiVersion: v1
data:
  overrides.yaml: |
    overrides:
      "anonymous":
        ingestion:
          rate_strategy: local
          rate_limit_bytes: 15000000
          burst_size_bytes: 20000000
          max_traces_per_user: 10000
        read:
          max_bytes_per_tag_values_query: 5000000
        # global:
        #   max_bytes_per_trace: 1500000
        # metrics_generator:
        #   processors:
        #   - service-graphs
        #   - span-metrics
        #   remote_write_headers:
        #     X-Scope-OrgID: "anonymous"
  tempo.yaml: |
    # For more information on this configuration, see the complete reference guide at
    # https://grafana.com/docs/tempo/latest/configuration/

    stream_over_http_enabled: true

    multitenancy_enabled: false
    usage_report:
      reporting_enabled: false

    compactor:
      compaction:
        block_retention: 1h

    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

    ingester:
      trace_idle_period: 10s
      max_block_bytes: 1_000_000
      max_block_duration: 5m

    querier:
      frontend_worker:
        frontend_address: tempo:9095

    metrics_generator:
      processor:
        span_metrics:
          # Configure extra dimensions to add as metric labels.
          dimensions:
          - http.method
          - http.target
          - http.status_code
          - service.version
        # Service graph metrics create node and edge metrics for determinng service interactions.
        service_graphs:
          # Configure extra dimensions to add as metric labels.
          dimensions:
          - http.method
          - http.target
          - http.status_code
          - service.version
      storage:
        path: /tmp/tempo/generator/wal
        remote_write_add_org_id_header: true
        remote_write:
        - url: http://nginx.gateway.svc.cluster.local:8080/api/v1/push
          send_exemplars: true
          send_native_histograms: true
          headers:
            X-Scope-OrgID: "anonymous"

    server:
      http_listen_port: 3100
      grpc_listen_port: 9095

    storage:
      trace:
        backend: s3
        wal:
          path: /tmp/tempo/wal
        s3:
          bucket: tempo-data
          endpoint: ${TEMPO_S3_ENDPOINT:-minio.minio-system.svc:443}
          access_key: ${TEMPO_S3_ACCESS_KEY:-lgtmp}
          secret_key: ${TEMPO_S3_SECRET_KEY:-supersecret}
          insecure: ${TEMPO_S3_INSECURE:-false}
          tls_insecure_skip_verify: true

    # https://github.com/grafana/tempo/blob/main/docs/sources/tempo/configuration/_index.md#cache
    cache:
      background:
        writeback_goroutines: 5
      caches:
      - roles:
        - bloom
        - parquet-footer
        - parquet-page
        - frontend-search
        - parquet-column-idx
        - parquet-offset-idx
        memcached:
          addresses: "dns+memcached.memcached-system.svc:11211"

    overrides:
      per_tenant_override_config: /conf/overrides.yaml
      defaults:
        global:
          max_bytes_per_trace: 1500000
        metrics_generator:
          processors:
          - service-graphs
          - span-metrics
          remote_write_headers:
            X-Scope-OrgID: "anonymous"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.3.1
    helm.sh/chart: tempo-1.7.2
  name: tempo
  namespace: tracing-system
---
apiVersion: v1
data:
  LOKI_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki-env-58m52b99kc
  namespace: logging-system
type: Opaque
---
apiVersion: v1
data:
  MIMIR_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir-env-92ddctt858
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: pyroscope-env-h982fgc652
  namespace: profiles-system
type: Opaque
---
apiVersion: v1
data:
  JAEGER_AGENT_HOST: Z3JhZmFuYS1hZ2VudC5tb25pdG9yaW5nLXN5c3RlbS5zdmMuY2x1c3Rlci5sb2NhbA==
  JAEGER_AGENT_PORT: NjgzMQ==
  JAEGER_SAMPLER_PARAM: MQ==
  JAEGER_SAMPLER_TYPE: Y29uc3Q=
  JAEGER_TAGS: YXBwPXRlbXBv
  TEMPO_S3_SECRET_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: tempo-env-gk54k88t7g
  namespace: tracing-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  ports:
  - name: http-metrics
    port: 3100
  - appProtocol: grpc
    name: grpc
    port: 9095
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
    prometheus.io/service-monitor: "false"
  name: loki-headless
  namespace: logging-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 3100
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
    prometheus.io/service-monitor: "false"
  name: loki-memberlist
  namespace: logging-system
spec:
  clusterIP: None
  ports:
  - name: tcp-gossip-ring
    port: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
  namespace: monitoring-system
spec:
  internalTrafficPolicy: Cluster
  ports:
  - name: http-metrics
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
    prometheus.io/service-monitor: "false"
  name: mimir-memberlist
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
    prometheus.io/service-monitor: "false"
  name: pyroscope-headless
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope-memberlist
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: memberlist
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.3.1
    helm.sh/chart: tempo-1.7.2
  name: tempo
  namespace: tracing-system
spec:
  ports:
  - name: tempo-grpc
    port: 9095
    targetPort: 9095
  - name: tempo-prom-metrics
    port: 3100
    targetPort: 3100
  - name: tempo-jaeger-thrift-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: tempo-jaeger-thrift-binary
    port: 6832
    protocol: UDP
    targetPort: 6832
  - name: tempo-jaeger-thrift-http
    port: 14268
    protocol: TCP
    targetPort: 14268
  - name: grpc-tempo-jaeger
    port: 14250
    protocol: TCP
    targetPort: 14250
  - name: tempo-zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: tempo-otlp-legacy
    port: 55680
    protocol: TCP
    targetPort: 55680
  - name: tempo-otlp-http-legacy
    port: 55681
    protocol: TCP
    targetPort: 4318
  - name: grpc-tempo-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: tempo-otlp-http
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: tempo-opencensus
    port: 55678
    protocol: TCP
    targetPort: 55678
  selector:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/name: tempo
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
      app.kubernetes.io/part-of: memberlist
  template:
    metadata:
      annotations:
        logs.agent.grafana.com/scrape: "true"
        logs.agent.grafana.com/scrub-level: info
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: mimir
      labels:
        app.kubernetes.io/component: mimir
        app.kubernetes.io/instance: mimir-monolithic-mode
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
    spec:
      containers:
      - args:
        - -target=all
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - secretRef:
            name: mimir-env-92ddctt858
        image: docker.io/grafana/mimir:2.11.0
        imagePullPolicy: IfNotPresent
        name: mimir
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
        resources:
          limits:
            cpu: 999m
            memory: 1Gi
          requests:
            cpu: 10m
            memory: 55Mi
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /data
          name: storage
      terminationGracePeriodSeconds: 60
      volumes:
      - configMap:
          name: mimir-config-958c4gm5k9
        name: config
      - emptyDir: {}
        name: storage
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: loki
      app.kubernetes.io/instance: loki-monolithic-mode
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
  serviceName: loki-headless
  template:
    metadata:
      annotations:
        logs.agent.grafana.com/scrub-level: info
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: loki
      labels:
        app.kubernetes.io/component: loki
        app.kubernetes.io/instance: loki-monolithic-mode
        app.kubernetes.io/name: loki
        app.kubernetes.io/part-of: memberlist
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/etc/loki/config/config.yaml
        - -target=all
        - -config.expand-env=true
        envFrom:
        - secretRef:
            name: loki-env-58m52b99kc
        image: docker.io/grafana/loki:3.0.0
        imagePullPolicy: IfNotPresent
        name: loki
        ports:
        - containerPort: 3100
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /tmp
          name: tmp
        - mountPath: /etc/loki/config
          name: config
        - mountPath: /etc/loki/runtime-config
          name: runtime-config
        - mountPath: /var/loki
          name: storage
      enableServiceLinks: true
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: loki
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: tmp
      - configMap:
          items:
          - key: config.yaml
            path: config.yaml
          name: loki-config-5468cb29k9
        name: config
      - configMap:
          name: loki-runtime-9599m5k6h2
        name: runtime-config
  updateStrategy:
    rollingUpdate:
      partition: 0
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      labels:
        app.kubernetes.io/component: loki
        app.kubernetes.io/instance: loki-monolithic-mode
        app.kubernetes.io/name: loki
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
  serviceName: alloy
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        logs.agent.grafana.com/scrape: "true"
        logs.agent.grafana.com/scrub-level: debug
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: alloy
      labels:
        app.kubernetes.io/instance: alloy
        app.kubernetes.io/name: alloy
    spec:
      containers:
      - args:
        - run
        - /etc/alloy/config.alloy
        - --storage.path=/tmp/alloy
        - --server.http.listen-addr=0.0.0.0:12345
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=alloy-cluster
        - --stability.level=experimental
        env:
        - name: ALLOY_DEPLOY_MODE
          value: helm
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
        - secretRef:
            name: alloy-env
            optional: true
        image: docker.io/grafana/alloy:v1.0.0
        imagePullPolicy: IfNotPresent
        name: alloy
        ports:
        - containerPort: 12345
          name: http-metrics
        - containerPort: 4317
          name: grpc-otlp
          protocol: TCP
        - containerPort: 4318
          name: http-otlp
          protocol: TCP
        - containerPort: 9411
          name: zipkin
          protocol: TCP
        - containerPort: 6831
          name: jaeger-compact
          protocol: UDP
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 12345
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/alloy
          name: config
        - mountPath: /etc/alloy/modules/kubernetes/metrics
          name: modules-kubernetes-metrics
        - mountPath: /etc/alloy/modules/kubernetes/integrations
          name: modules-kubernetes-integrations
        - mountPath: /etc/alloy/modules/kubernetes/logs
          name: modules-kubernetes-logs
        - mountPath: /etc/alloy/modules/kubernetes/traces
          name: modules-kubernetes-traces
        - mountPath: /etc/alloy/modules/kubernetes/profiles
          name: modules-kubernetes-profiles
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: alloy
      volumes:
      - configMap:
          name: alloy-config-6b695gbdm6
        name: config
      - configMap:
          name: alloy-modules-kubernetes-metrics-tf9cd8b6bg
        name: modules-kubernetes-metrics
      - configMap:
          name: alloy-modules-kubernetes-integrations-9k86fb8hkg
        name: modules-kubernetes-integrations
      - configMap:
          name: alloy-modules-kubernetes-logs-d7c756mt2f
        name: modules-kubernetes-logs
      - configMap:
          name: alloy-modules-kubernetes-traces-8mgm8th9m5
        name: modules-kubernetes-traces
      - configMap:
          name: alloy-modules-kubernetes-profiles-66c27bc84g
        name: modules-kubernetes-profiles
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
  serviceName: pyroscope-headless
  template:
    metadata:
      annotations:
        checksum/config: b357ed79c949078f193a9e0254cfecb7a6747996038df7d269b85c32469c9077
        profiles.grafana.com/cpu.port_name: http2
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.port_name: http2
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/memory.port_name: http2
        profiles.grafana.com/memory.scrape: "true"
        pyroscope.io/service_name: pyroscope
      labels:
        app.kubernetes.io/component: all
        app.kubernetes.io/instance: pyroscope
        app.kubernetes.io/name: pyroscope
        name: pyroscope
    spec:
      containers:
      - args:
        - -target=all
        - -self-profiling.disable-push=true
        - -server.http-listen-port=4040
        - -memberlist.cluster-label=profiles-system-pyroscope
        - -memberlist.join=dns+pyroscope-memberlist.profiles-system.svc.cluster.local.:7946
        - -config.file=/etc/pyroscope/config.yaml
        - -runtime-config.file=/etc/pyroscope/overrides/overrides.yaml
        - -config.expand-env=true
        - -log.level=debug
        envFrom:
        - secretRef:
            name: pyroscope-env-h982fgc652
        image: grafana/pyroscope:1.5.0
        imagePullPolicy: IfNotPresent
        name: pyroscope
        ports:
        - containerPort: 4040
          name: http2
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http2
            scheme: HTTP
        resources: {}
        securityContext: {}
        volumeMounts:
        - mountPath: /etc/pyroscope/config.yaml
          name: config
          subPath: config.yaml
        - mountPath: /etc/pyroscope/overrides/
          name: overrides-config
        - mountPath: /data
          name: data
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: pyroscope
      volumes:
      - configMap:
          name: pyroscope-config
        name: config
      - configMap:
          name: pyroscope-overrides-config
        name: overrides-config
      - emptyDir: {}
        name: data
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: tempo
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tempo
    app.kubernetes.io/version: 2.3.1
    helm.sh/chart: tempo-1.7.2
  name: tempo
  namespace: tracing-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
  serviceName: tempo-headless
  template:
    metadata:
      annotations:
        checksum/config: f1d170a0130edd0da1b51eab83df1f9642ad10ca67130fd306f69cabe9540145
        logs.agent.grafana.com/scrape: "true"
        logs.agent.grafana.com/scrub-level: info
        profiles.grafana.com/cpu.port_name: prom-metrics
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.port_name: prom-metrics
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/memory.port_name: prom-metrics
        profiles.grafana.com/memory.scrape: "true"
        pyroscope.io/service_name: tempo
      labels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/conf/tempo.yaml
        - -mem-ballast-size-mbs=1024
        - -config.expand-env=true
        envFrom:
        - secretRef:
            name: tempo-env-gk54k88t7g
        image: grafana/tempo:2.4.1
        imagePullPolicy: IfNotPresent
        name: tempo
        ports:
        - containerPort: 9095
          name: tempo-grpc
        - containerPort: 3100
          name: prom-metrics
        - containerPort: 6831
          name: jaeger-thrift-c
          protocol: UDP
        - containerPort: 6832
          name: jaeger-thrift-b
          protocol: UDP
        - containerPort: 14268
          name: jaeger-thrift-h
        - containerPort: 14250
          name: jaeger-grpc
        - containerPort: 9411
          name: zipkin
        - containerPort: 55680
          name: otlp-legacy
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 55681
          name: otlp-httplegacy
        - containerPort: 4318
          name: otlp-http
        - containerPort: 55678
          name: opencensus
        resources: {}
        volumeMounts:
        - mountPath: /conf
          name: tempo-conf
        - mountPath: /tmp
          name: tmp
      serviceAccountName: tempo
      volumes:
      - configMap:
          name: tempo
        name: tempo-conf
      - emptyDir: {}
        name: tmp
  updateStrategy:
    type: RollingUpdate
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: loki
    app.kubernetes.io/instance: loki-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 3.0.0
  name: loki
  namespace: logging-system
spec:
  endpoints:
  - interval: 15s
    port: http-metrics
    relabelings:
    - action: replace
      replacement: logging-system/loki
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - logging-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
  namespace: monitoring-system
spec:
  endpoints:
  - honorLabels: true
    port: http-metrics
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.11.0
  name: mimir
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/mimir
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.5.0
    helm.sh/chart: pyroscope-1.5.1
  name: pyroscope
  namespace: profiles-system
spec:
  endpoints:
  - port: http2
    relabelings:
    - action: replace
      replacement: profiles-system/pyroscope
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - profiles-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tempo
  namespace: tracing-system
spec:
  endpoints:
  - interval: 15s
    port: tempo-prom-metrics
    relabelings:
    - action: replace
      replacement: tracing-system/tempo
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - tracing-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/version: v1.0.0
    helm.sh/chart: alloy-0.1.1
  name: alloy
  namespace: monitoring-system
spec:
  rules:
  - host: alloy.localhost
    http:
      paths:
      - backend:
          service:
            name: alloy
            port:
              number: 12345
        path: /
        pathType: Prefix
