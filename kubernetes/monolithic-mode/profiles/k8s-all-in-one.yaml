apiVersion: v1
kind: Namespace
metadata:
  name: profiles-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
  name: mimir
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope
  namespace: profiles-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: profiles-system-pyroscope
  namespace: profiles-system
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - replicasets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: profiles-system-pyroscope
  namespace: profiles-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: profiles-system-pyroscope
subjects:
- kind: ServiceAccount
  name: pyroscope
  namespace: profiles-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy
subjects:
- kind: ServiceAccount
  name: alloy
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-cluster-node.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Clustering documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/cli/run/#clustering"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "Node Info",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Information about a specific cluster node.\n\n* Lamport clock time: The observed Lamport time on the specific node's clock used to provide partial ordering around gossip messages. Nodes should ideally be observing roughly the same time, meaning they are up-to-date on the cluster state. If a node is falling behind, it means that it has not recently processed the same number of messages and may have an outdated view of its peers.\n\n* Internal cluster state observers: The number of Observer functions that are registered to run whenever the node detects a cluster change.\n\n* Gossip health score: A health score assigned to this node by the memberlist implementation. The lower, the better.\n\n* Gossip protocol version: The protocol version used by nodes to communicate with one another. It should match across all nodes.\n",
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_lamport_time{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}) \n",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Lamport clock time"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_update_observers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"})\n",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Internal cluster state observers"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_gossip_health_score{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"})\n",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Gossip health score"
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_gossip_proto_version{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"})\n",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false,
                      "refId": "Gossip protocol version"
                   }
                ],
                "title": "Node Info",
                "transformations": [
                   {
                      "id": "renameByRegex",
                      "options": {
                         "regex": "Value #(.*)",
                         "renamePattern": "$1"
                      }
                   },
                   {
                      "id": "reduce",
                      "options": { }
                   },
                   {
                      "id": "organize",
                      "options": {
                         "excludeByName": { },
                         "indexByName": { },
                         "renameByName": {
                            "Field": "Metric",
                            "Max": "Value"
                         }
                      }
                   }
                ],
                "type": "table"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_node_gossip_received_events_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{event}}",
                      "range": true
                   }
                ],
                "title": "Gossip ops/s",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Known peers to the node (including the local node).\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "suffix:peers"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 9
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"})\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Known peers",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Known peers to the node by state (including the local node).\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "suffix:nodes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 9
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{state}}",
                      "range": true
                   }
                ],
                "title": "Peers by state",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 17
                },
                "title": "Gossip Transport",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "axisCenteredZero": true
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_transport_rx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "rx",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "-1 * rate(cluster_transport_tx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "tx",
                      "range": true
                   }
                ],
                "title": "Transport bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_tx_packets_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval]) /\n  rate(cluster_transport_tx_packets_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "Tx success %",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_rx_packets_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval]) /\n  rate(cluster_transport_rx_packets_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "Rx success %",
                      "range": true
                   }
                ],
                "title": "Packet write success rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of packets enqueued currently to be decoded or encoded and sent during communication with other nodes.\n\nThe incoming and outgoing packet queue should be as empty as possible; a growing queue means that Alloy cannot keep up with the number of messages required to have all nodes informed of cluster changes, and the nodes may not converge in a timely fashion.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "pkts"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_tx_packet_queue_length{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "tx queue",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_rx_packet_queue_length{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "rx queue",
                      "range": true
                   }
                ],
                "title": "Pending packet queue",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "axisCenteredZero": true
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(cluster_transport_stream_rx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "rx",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "-1 * rate(cluster_transport_stream_tx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "tx",
                      "range": true
                   }
                ],
                "title": "Stream bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_stream_tx_packets_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval]) /\n  rate(cluster_transport_stream_tx_packets_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "Tx success %",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "1 - (\n  rate(cluster_transport_stream_rx_packets_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval]) /\n  rate(cluster_transport_stream_rx_packets_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "Rx success %",
                      "range": true
                   }
                ],
                "title": "Stream write success rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of open connections from this node to its peers.\n\nEach node picks up a subset of its peers to continuously gossip messages around cluster status using streaming HTTP/2 connections. This panel can be used to detect networking failures that result in cluster communication being disrupted and convergence taking longer than expected or outright failing.\n",
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 26
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_transport_streams{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "Open streams",
                      "range": true
                   }
                ],
                "title": "Open transport streams",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Cluster Node",
          "uid": "4047e755d822da63c8158cde32ae4dce"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-cluster-node.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-cluster-overview.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Clustering documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/cli/run/#clustering"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 9,
                   "w": 8,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "count(cluster_node_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})\n",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Nodes",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Nodes info.\n",
                "fieldConfig": {
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Dashboard"
                         },
                         "properties": [
                            {
                               "id": "mappings",
                               "value": [
                                  {
                                     "options": {
                                        "1": {
                                           "index": 0,
                                           "text": "Link"
                                        }
                                     },
                                     "type": "value"
                                  }
                               ]
                            },
                            {
                               "id": "links",
                               "value": [
                                  {
                                     "targetBlank": false,
                                     "title": "Detail dashboard for node",
                                     "url": "/d/4047e755d822da63c8158cde32ae4dce/alloy-cluster-node?var-instance=${__data.fields.instance}&var-datasource=${datasource}&var-loki_datasource=${loki_datasource}&var-job=${job}&var-cluster=${cluster}&var-namespace=${namespace}"
                                  }
                               ]
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 9,
                   "w": 16,
                   "x": 8,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "cluster_node_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}\n",
                      "format": "table",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Node table",
                "transformations": [
                   {
                      "id": "organize",
                      "options": {
                         "excludeByName": {
                            "Time": true,
                            "Value": false,
                            "__name__": true,
                            "cluster": true,
                            "namespace": true,
                            "state": false
                         },
                         "indexByName": { },
                         "renameByName": {
                            "Value": "Dashboard",
                            "instance": "",
                            "state": ""
                         }
                      }
                   }
                ],
                "type": "table"
             },
             {
                "datasource": "${datasource}",
                "description": "Whether the cluster state has converged.\n\nIt is normal for the cluster state to be diverged briefly as gossip events propagate. It is not normal for the cluster state to be diverged for a long period of time.\n\nThis will show one of the following:\n\n* Converged: Nodes are aware of all other nodes, with the correct states.\n* Not converged: A subset of nodes aren't aware of their peers, or don't have an updated view of peer states.\n",
                "fieldConfig": {
                   "defaults": {
                      "mappings": [
                         {
                            "options": {
                               "1": {
                                  "color": "red",
                                  "index": 1,
                                  "text": "Not converged"
                               }
                            },
                            "type": "value"
                         },
                         {
                            "options": {
                               "match": "null",
                               "result": {
                                  "color": "green",
                                  "index": 0,
                                  "text": "Converged"
                               }
                            },
                            "type": "special"
                         }
                      ],
                      "unit": "suffix:nodes"
                   }
                },
                "gridPos": {
                   "h": 9,
                   "w": 8,
                   "x": 0,
                   "y": 9
                },
                "options": {
                   "colorMode": "background",
                   "graphMode": "none",
                   "justifyMode": "auto",
                   "orientation": "auto",
                   "reduceOptions": {
                      "calcs": [
                         "lastNotNull"
                      ],
                      "fields": "",
                      "values": false
                   },
                   "textMode": "auto"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "clamp((\n  sum(stddev by (state) (cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}) != 0) or\n  (sum(abs(sum without (state) (cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})) - scalar(count(cluster_node_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})) != 0))\n  ),\n  1, 1\n)\n",
                      "format": "time_series",
                      "instant": true,
                      "legendFormat": "__auto",
                      "range": false
                   }
                ],
                "title": "Convergance state",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 80,
                         "spanNulls": true
                      },
                      "mappings": [
                         {
                            "options": {
                               "0": {
                                  "color": "green",
                                  "text": "Yes"
                               }
                            },
                            "type": "value"
                         },
                         {
                            "options": {
                               "1": {
                                  "color": "red",
                                  "text": "No"
                               }
                            },
                            "type": "value"
                         }
                      ],
                      "max": 1,
                      "noValue": 0
                   }
                },
                "gridPos": {
                   "h": 9,
                   "w": 16,
                   "x": 8,
                   "y": 9
                },
                "options": {
                   "mergeValues": true
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "ceil(clamp((\n  sum(stddev by (state) (cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})) or\n  (sum(abs(sum without (state) (cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})) - scalar(count(cluster_node_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}))))\n  ),\n  0, 1\n))\n",
                      "instant": false,
                      "legendFormat": "Converged",
                      "range": true
                   }
                ],
                "title": "Convergance state timeline",
                "type": "state-timeline"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of cluster peers seen by each instance.\n\nWhen cluster is converged, every peer should see all the other instances. When we have a split brain or one\npeer not joining the cluster, we will see two or more groups of instances that report different peer numbers\nfor an extended period of time and not converging.\n\nThis graph helps to identify which instances may be in a split brain state.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "peers"
                   }
                },
                "gridPos": {
                   "h": 12,
                   "w": 24,
                   "x": 0,
                   "y": 18
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by(instance) (cluster_node_peers{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Number of peers seen by each instance",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Cluster Overview",
          "uid": "3a6b7020692f53d8e53b49196f7637dd"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-cluster-overview.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  config.alloy: "logging {\n\tlevel  = coalesce(sys.env(\"ALLOY_LOG_LEVEL\"), \"warn\")\n\tformat
    = \"logfmt\"\n}\n\n/********************************************\n * Grafana LGTMP
    Stack Receiver Provider\n ********************************************/\nimport.git
    \"provider\" {\n\trepository     = \"https://github.com/qclaogui/codelab-monitoring.git\"\n\trevision
    \      = \"main\"\n\tpath           = \"alloy-modules/provider\"\n\tpull_frequency
    = \"24h\"\n}\n\nprovider.self_hosted_stack \"kubernetes\" {\n\tmetrics_endpoint_url
    \ = coalesce(sys.env(\"SELF_HOSTED_METRICS_ENDPOINT_URL\"), \"http://nginx.gateway.svc:8080/api/v1/push\")\n\tprofiles_endpoint_url
    = coalesce(sys.env(\"SELF_HOSTED_PROFILES_ENDPOINT_URL\"), \"http://nginx.gateway.svc:4040\")\n}\n\n/********************************************\n
    * Profiles\n ********************************************/\nimport.file \"profiles\"
    {\n\tfilename = coalesce(sys.env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/profiles\"\n}\n\nprofiles.annotations_scrape \"kubernetes\" {\n\tcluster
    = coalesce(sys.env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.profiles_receiver]\n}\n\n/********************************************\n
    * Metrics\n ********************************************/\nimport.file \"metrics\"
    {\n\tfilename = coalesce(sys.env(\"ALLOY_MODULES_FOLDER\"), \"/etc/alloy/modules\")
    + \"/kubernetes/metrics\"\n}\n\nmetrics.annotations_scrape \"kubernetes\" {\n\tcluster
    \        = coalesce(sys.env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"15s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\nmetrics.servicemonitors_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(sys.env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"15s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n//
    Jobs metrics\nimport.file \"jobs\" {\n\tfilename = coalesce(sys.env(\"ALLOY_MODULES_FOLDER\"),
    \"/etc/alloy/modules\") + \"/kubernetes/jobs\"\n}\n\njobs.kubelet_metrics_scrape
    \"kubernetes\" {\n\tcluster         = coalesce(sys.env(\"CLUSTER_NAME\"), \"k3d-k3s-codelab\")\n\tscrape_interval
    = \"30s\"\n\n\tforward_to = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n\n//
    Alloy integration metrics\nremote.kubernetes.configmap \"integrations\" {\n\tnamespace
    = \"monitoring-system\"\n\tname      = \"alloy-integrations\"\n}\n\n// Memcached
    Integrations\nimport.string \"memcached\" {\n\tcontent = remote.kubernetes.configmap.integrations.data[\"memcached.alloy\"]\n}\n\nmemcached.memcached_metrics_scrape
    \"instance\" {\n\tnamespace = \"monitoring-system\"\n\tname      = remote.kubernetes.configmap.integrations.data[\"MEMCACHED_K8S_SECRET_NAME\"]\n\n\tforward_to
    = [provider.self_hosted_stack.kubernetes.metrics_receiver]\n}\n"
kind: ConfigMap
metadata:
  name: alloy-config-g5hfmfd9ff
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-controller.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Component controller documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/concepts/component_controller/"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "description": "The number of Alloy instances whose metrics are being sent and reported.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "instances"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 0
                },
                "options": {
                   "colorMode": "none",
                   "graphMode": "none"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "count(group(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}) by (instance))\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Running instances",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "The number of running components across all running instances.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "components"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 4
                },
                "options": {
                   "colorMode": "none",
                   "graphMode": "none"
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Running components",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentage of components which are in a healthy state.\n",
                "fieldConfig": {
                   "defaults": {
                      "max": 1,
                      "min": 0,
                      "noValue": "No components",
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 4,
                   "w": 10,
                   "x": 0,
                   "y": 8
                },
                "options": {
                   "colorMode": "value",
                   "graphMode": "area",
                   "text": {
                      "valueSize": 80
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\",health_type=\"healthy\"}) /\nsum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"})\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Overall component health",
                "type": "stat"
             },
             {
                "datasource": "${datasource}",
                "description": "Breakdown of components by health across all running instances.\n\n* Healthy: components have been evaluated completely and are reporting themselves as healthy.\n* Unhealthy: Components either could not be evaluated or are reporting themselves as unhealthy.\n* Unknown: A component has been created but has not yet been started.\n* Exited: A component has exited. It will not return to the running state.\n\nMore information on a component's health state can be retrieved using\nthe Alloy UI.\n\nNote that components may be in a degraded state even if they report\nthemselves as healthy. Use component-specific dashboards and alerts\nto observe detailed information about the behavior of a component.\n",
                "fieldConfig": {
                   "defaults": {
                      "min": 0,
                      "thresholds": {
                         "mode": "absolute",
                         "steps": [
                            {
                               "color": "green",
                               "value": null
                            }
                         ]
                      }
                   },
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Unhealthy"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "red",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Unknown"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "blue",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Exited"
                         },
                         "properties": [
                            {
                               "id": "thresholds",
                               "value": {
                                  "mode": "absolute",
                                  "steps": [
                                     {
                                        "color": "green",
                                        "value": null
                                     },
                                     {
                                        "color": "orange",
                                        "value": 1
                                     }
                                  ]
                               }
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 12,
                   "w": 14,
                   "x": 10,
                   "y": 0
                },
                "options": {
                   "orientation": "vertical",
                   "showUnfilled": true
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", health_type=\"healthy\"}) or vector(0)\n",
                      "instant": true,
                      "legendFormat": "Healthy",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", health_type=\"unhealthy\"}) or vector(0)\n",
                      "instant": true,
                      "legendFormat": "Unhealthy",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", health_type=\"unknown\"}) or vector(0)\n",
                      "instant": true,
                      "legendFormat": "Unknown",
                      "range": false
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", health_type=\"exited\"}) or vector(0)\n",
                      "instant": true,
                      "legendFormat": "Exited",
                      "range": false
                   }
                ],
                "title": "Components by health",
                "type": "bargauge"
             },
             {
                "datasource": "${datasource}",
                "description": "The frequency at which components get updated.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "drawStyle": "points",
                         "pointSize": 3
                      },
                      "unit": "ops"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 12
                },
                "options": {
                   "tooltip": {
                      "mode": "multi"
                   }
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (instance) (rate(alloy_component_evaluation_seconds_count{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\n",
                      "instant": false,
                      "legendFormat": "__auto",
                      "range": true
                   }
                ],
                "title": "Component evaluation rate",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentiles for how long it takes to complete component evaluations.\n\nComponent evaluations must complete for components to have the latest\narguments. The longer the evaluations take, the slower it will be to\nreconcile the state of components.\n\nIf evaluation is taking too long, consider sharding your components to\ndeal with smaller amounts of data and reuse data as much as possible.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.99, sum(rate(alloy_component_evaluation_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\nor\nhistogram_quantile(0.99, sum by (le) (rate(alloy_component_evaluation_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "99th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.50, sum(rate(alloy_component_evaluation_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\nor\nhistogram_quantile(0.50, sum by (le) (rate(alloy_component_evaluation_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "50th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "(\n  histogram_sum(sum(rate(alloy_component_evaluation_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))) /\n  histogram_count(sum(rate(alloy_component_evaluation_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\n)\nor\n(\n  sum(rate(alloy_component_evaluation_seconds_sum{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])) /\n  sum(rate(alloy_component_evaluation_seconds_count{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\n)\n",
                      "instant": false,
                      "legendFormat": "Average",
                      "range": true
                   }
                ],
                "title": "Component evaluation time",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The percentage of time spent evaluating 'slow' components - components that took longer than 1 minute to evaluate.\n\nIdeally, no component should take more than 1 minute to evaluate. The components displayed in this chart\nmay be a sign of a problem with the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (component_path, component_id) (rate(alloy_component_evaluation_slow_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\n/ scalar(sum(rate(alloy_component_evaluation_seconds_sum{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval])))\n",
                      "instant": false,
                      "legendFormat": "{{component path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Slow components evaluation times",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Detailed histogram view of how long component evaluations take.\n\nThe goal is to design your config so that evaluations take as little\ntime as possible; under 100ms is a good goal.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 22
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "cellGap": 0,
                   "color": {
                      "scheme": "Spectral"
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 0.10000000000000001
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "s"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(increase(alloy_component_evaluation_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\nor ignoring (le)\nsum by (le) (increase(alloy_component_evaluation_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Component evaluation histogram",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "description": "Detailed histogram of how long components wait to be evaluated after their dependency is updated.\n\nThe goal is to design your config so that most of the time components do not\nqueue for long; under 10ms is a good goal.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 22
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "cellGap": 0,
                   "color": {
                      "scheme": "Spectral"
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 0.10000000000000001
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "s"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(increase(alloy_component_dependencies_wait_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\nor ignoring (le)\nsum by (le) (increase(alloy_component_dependencies_wait_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Component dependency wait histogram",
                "type": "heatmap"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Controller",
          "uid": "bf9f456aad7108b2c808dbd9973e386f"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-controller.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  MEMCACHED_K8S_SECRET_NAME: alloy-integrations-memcached
  MYSQL_K8S_SECRET_NAME: alloy-integrations-mysql
  REDIS_K8S_SECRET_NAME: alloy-integrations-redis
  memcached.alloy: "/*\nModule Components: component_memcached\n*/\n\ndeclare \"memcached_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/memcached)\"\n\t\toptional =
    true\n\t}\n\n\targument \"namespace\" {\n\t\tcomment  = \"kubernetes secret name
    (default: monitoring-system)\"\n\t\toptional = true\n\t}\n\n\targument \"name\"
    {\n\t\tcomment  = \"kubernetes secret name (default: alloy-integrations-redis)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\toptional = true\n\t\tdefault
    \ = \"(up|memcached_commands_total|memcached_connections_total|memcached_current_bytes|memcached_current_connections|memcached_current_items|memcached_items_evicted_total|memcached_items_total|memcached_max_connections|memcached_read_bytes_total|memcached_up|memcached_uptime_seconds|memcached_version|memcached_written_bytes_total)\"\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\tremote.kubernetes.secret
    \"memcached\" {\n\t\tnamespace = coalesce(argument.namespace.value, \"monitoring-system\")\n\t\tname
    \     = coalesce(argument.name.value, \"alloy-integrations-memcached\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Memcached\n\t****************************************************************/\n\t//
    https://grafana.com/docs/alloy/latest/reference/components/prometheus.exporter.memcached/\n\tprometheus.exporter.memcached
    \"integrations_memcached_exporter\" {\n\t\taddress = nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-address\"])\n\t\ttimeout
    = nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-timeout\"])\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_memcached_exporter\" {\n\t\ttargets = prometheus.exporter.memcached.integrations_memcached_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/memcached\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label
    = \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.memcached.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_memcached_exporter\" {\n\t\ttargets = array.concat(\n\t\t\tdiscovery.relabel.integrations_memcached_exporter.output,\n\t\t)\n\n\t\tscrape_classic_histograms
    = true\n\n\t\tscrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.integrations_memcached_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_memcached_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = argument.keep_metrics.value\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  mysql.alloy: "/*\nModule Components: component_mysql\n*/\n\ndeclare \"mysql_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/mysql)\"\n\t\toptional = true\n\t}\n\n\targument
    \"namespace\" {\n\t\tcomment  = \"kubernetes secret namespace (default: monitoring-system)\"\n\t\toptional
    = true\n\t}\n\n\targument \"name\" {\n\t\tcomment  = \"kubernetes secret name
    (default: alloy-integrations-mysql)\"\n\t\toptional = true\n\t}\n\n\targument
    \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep (default: see
    below)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment
    \ = \"How often to scrape metrics from the targets (default: 60s)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment  = \"How long before
    a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\tremote.kubernetes.secret
    \"mysql\" {\n\t\tname      = coalesce(argument.name.value, \"alloy-integrations-mysql\")\n\t\tnamespace
    = coalesce(argument.namespace.value, \"monitoring-system\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Mysql\n\t****************************************************************/\n\tprometheus.exporter.mysql
    \"integrations_mysqld_exporter\" {\n\t\tdata_source_name = nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-username\"])
    + \":\" + nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-password\"])
    + \"@(\" + nonsensitive(remote.kubernetes.secret.mysql.data[\"mysql-host\"]) +
    \")/\"\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_mysqld_exporter\" {\n\t\ttargets = prometheus.exporter.mysql.integrations_mysqld_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/mysql\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label =
    \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.mysql.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_mysqld_exporter\" {\n\t\ttargets = array.concat(\n\t\t\tdiscovery.relabel.integrations_mysqld_exporter.output,\n\t\t)\n\n\t\tscrape_classic_histograms
    = true\n\n\t\tscrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tclustering {\n\t\t\tenabled
    = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.integrations_mysqld_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_mysqld_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(up|instance:mysql_heartbeat_lag_seconds|instance:mysql_slave_lag_seconds|mysql_global_status_aborted_clients|mysql_global_status_aborted_connects|mysql_global_status_buffer_pool_pages|mysql_global_status_bytes_received|mysql_global_status_bytes_sent|mysql_global_status_commands_total|mysql_global_status_created_tmp_disk_tables|mysql_global_status_created_tmp_files|mysql_global_status_created_tmp_tables|mysql_global_status_handlers_total|mysql_global_status_innodb_log_waits|mysql_global_status_innodb_mem_adaptive_hash|mysql_global_status_innodb_mem_dictionary|mysql_global_status_innodb_num_open_files|mysql_global_status_innodb_page_size|mysql_global_status_max_used_connections|mysql_global_status_open_files|mysql_global_status_open_table_definitions|mysql_global_status_open_tables|mysql_global_status_opened_files|mysql_global_status_opened_table_definitions|mysql_global_status_opened_tables|mysql_global_status_qcache_free_memory|mysql_global_status_qcache_hits|mysql_global_status_qcache_inserts|mysql_global_status_qcache_lowmem_prunes|mysql_global_status_qcache_not_cached|mysql_global_status_qcache_queries_in_cache|mysql_global_status_queries|mysql_global_status_questions|mysql_global_status_select_full_join|mysql_global_status_select_full_range_join|mysql_global_status_select_range|mysql_global_status_select_range_check|mysql_global_status_select_scan|mysql_global_status_slow_queries|mysql_global_status_sort_merge_passes|mysql_global_status_sort_range|mysql_global_status_sort_rows|mysql_global_status_sort_scan|mysql_global_status_table_locks_immediate|mysql_global_status_table_locks_waited|mysql_global_status_table_open_cache_hits|mysql_global_status_table_open_cache_misses|mysql_global_status_table_open_cache_overflows|mysql_global_status_threads_cached|mysql_global_status_threads_connected|mysql_global_status_threads_created|mysql_global_status_threads_running|mysql_global_status_uptime|mysql_global_status_wsrep_local_recv_queue|mysql_global_status_wsrep_local_state|mysql_global_status_wsrep_ready|mysql_global_variables_innodb_additional_mem_pool_size|mysql_global_variables_innodb_buffer_pool_size|mysql_global_variables_innodb_log_buffer_size|mysql_global_variables_key_buffer_size|mysql_global_variables_max_connections|mysql_global_variables_open_files_limit|mysql_global_variables_query_cache_size|mysql_global_variables_table_definition_cache|mysql_global_variables_table_open_cache|mysql_global_variables_thread_cache_size|mysql_global_variables_tokudb_cache_size|mysql_global_variables_wsrep_desync|mysql_heartbeat_now_timestamp_seconds|mysql_heartbeat_stored_timestamp_seconds|mysql_info_schema_processlist_threads|mysql_slave_status_seconds_behind_master|mysql_slave_status_slave_io_running|mysql_slave_status_slave_sql_running|mysql_slave_status_sql_delay|mysql_up)\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  redis.alloy: "/*\nModule Components: component_redis_exporter\n*/\n\ndeclare \"redis_exporter_metrics_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\tcomment
    \ = \"job label (default: integrations/kubernetes/redis_exporter)\"\n\t\toptional
    = true\n\t}\n\n\targument \"namespace\" {\n\t\tcomment  = \"kubernetes secret
    name (default: monitoring-system)\"\n\t\toptional = true\n\t}\n\n\targument \"name\"
    {\n\t\tcomment  = \"kubernetes secret name (default: alloy-integrations-redis)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\toptional = true\n\t\tdefault
    \ = \"(up|redis_blocked_clients|redis_cluster_slots_fail|redis_cluster_slots_pfail|redis_cluster_state|redis_commands_duration_seconds_total|redis_commands_total|redis_connected_clients|redis_connected_slaves|redis_db_keys|redis_db_keys_expiring|redis_evicted_keys_total|redis_keyspace_hits_total|redis_keyspace_misses_total|redis_master_last_io_seconds_ago|redis_memory_fragmentation_ratio|redis_memory_max_bytes|redis_memory_used_bytes|redis_memory_used_rss_bytes|redis_total_system_memory_bytes|redis_up)\"\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t\tdefault  = \"60s\"\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t\tdefault  = \"10s\"\n\t}\n\n\tremote.kubernetes.secret
    \"redis\" {\n\t\tnamespace = coalesce(argument.namespace.value, \"monitoring-system\")\n\t\tname
    \     = coalesce(argument.name.value, \"alloy-integrations-redis\")\n\t}\n\n\t/***************************************************************\n\t*
    Integrations Redis\n\t****************************************************************/\n\tprometheus.exporter.redis
    \"integrations_redis_exporter\" {\n\t\tredis_addr     = nonsensitive(remote.kubernetes.secret.redis.data[\"instance-address\"])\n\t\tredis_password
    = nonsensitive(remote.kubernetes.secret.redis.data[\"instance-password\"])\n\t}\n\n\t/***************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t****************************************************************/\n\tdiscovery.relabel
    \"integrations_redis_exporter\" {\n\t\ttargets = prometheus.exporter.redis.integrations_redis_exporter.targets\n\n\t\trule
    {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement  = coalesce(argument.job_label.value,
    \"integrations/kubernetes/redis_exporter\")\n\t\t}\n\n\t\trule {\n\t\t\ttarget_label
    = \"instance\"\n\t\t\treplacement  = coalesce(nonsensitive(remote.kubernetes.secret.redis.data[\"instance-name\"]),
    constants.hostname)\n\t\t}\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Scrape Integrations Targets\n\t****************************************************************/\n\tprometheus.scrape
    \"integrations_redis_exporter\" {\n\t\ttargets = array.concat(\n\t\t\tdiscovery.relabel.integrations_redis_exporter.output,\n\t\t)\n\n\t\tscrape_classic_histograms
    = true\n\n\t\tscrape_interval = argument.scrape_interval.value\n\t\tscrape_timeout
    \ = argument.scrape_timeout.value\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to
    = [prometheus.relabel.integrations_redis_exporter.receiver]\n\t}\n\n\t/***************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t****************************************************************/\n\tprometheus.relabel
    \"integrations_redis_exporter\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = argument.keep_metrics.value\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-integrations
  namespace: monitoring-system
---
apiVersion: v1
data:
  apiserver.alloy: "/*\nModule Components: apiserver\nDescription: kubernetes Apiserver
    Metrics Scrape\n\n*/\n\ndeclare \"apiserver_metrics_scrape\" {\n\n\t/********************************************\n\t*
    ARGUMENTS\n\t********************************************/\n\targument \"forward_to\"
    {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected metrics should
    be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument \"namespaces\"
    {\n\t\tcomment  = \"The namespaces to look for targets in (default: default)\"\n\t\toptional
    = true\n\t}\n\n\targument \"field_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"metadata.name=kubernetes\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The value of the label
    for the selector (default: https)\"\n\t\toptional = true\n\t}\n\n\targument \"job_label\"
    {\n\t\tcomment  = \"The job label to add for all kube-apiserver metrics (default:
    integrations/kubernetes/apiserver)\"\n\t\toptional = true\n\t}\n\n\targument \"keep_metrics\"
    {\n\t\tcomment  = \"A regex of metrics to keep (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\t// drop metrics and les from kube-prometheus\n\t// https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml\n\targument
    \"drop_metrics\" {\n\t\tcomment  = \"A regular expression of metrics to drop (default:
    see below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_les\" {\n\t\tcomment
    \ = \"Regular expression of metric les label values to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"apiserver\" {\n\t\trole = \"service\"\n\n\t\tselectors {\n\t\t\trole  = \"service\"\n\t\t\tfield
    = join(coalesce(argument.field_selectors.value, [\"metadata.name=kubernetes\"]),
    \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value, []), \",\")\n\t\t}\n\n\t\tnamespaces
    {\n\t\t\tnames = coalesce(argument.namespaces.value, [\"default\"])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"apiserver\" {\n\t\ttargets = discovery.kubernetes.apiserver.targets\n\n\t\t//
    only keep targets with a matching port name\n\t\trule {\n\t\t\tsource_labels =
    [\"__meta_kubernetes_service_port_name\"]\n\t\t\tregex         = coalesce(argument.port_name.value,
    \"https\")\n\t\t\taction        = \"keep\"\n\t\t}\n\n\t\t// set the namespace\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the service_name\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_service_name\"]\n\t\t\ttarget_label
    \ = \"service\"\n\t\t}\n\n\t\t// set the app name if specified as metadata labels
    \"app:\" or \"app.kubernetes.io/name:\" or \"k8s-app:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_service_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_service_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_service_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"apiserver\" {\n\t\ttargets = discovery.relabel.apiserver.output\n\n\t\tjob_name
    \         = coalesce(argument.job_label.value, \"integrations/kubernetes/apiserver\")\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.apiserver.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"apiserver\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(((go|process)_.+)|kubelet_node_name|kubelet_(pod_(worker|start)_latency_microseconds|cgroup_manager_latency_microseconds|pleg_relist_(latency|interval)_microseconds|runtime_operations(_latency_microseconds|_errors)?|eviction_stats_age_microseconds|device_plugin_(registration_count|alloc_latency_microseconds)|network_plugin_operations_latency_microseconds)|scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_(predicate|priority|preemption)_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)|apiserver_(request_(count|latencies(_summary)?)|dropped_requests|storage_(data_key_generation|transformation_(failures_total|latencies_microseconds))|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)|kubelet_docker_(operations(_latency_microseconds|_errors|_timeout)?)|reflector_(items_per_(list|watch)|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)|etcd_(helper_(cache_(hit|miss)_count|cache_entry_count|object_counts)|request_(cache_(get|add)_latencies_summary|latencies_summary)|debugging.*|disk.*|server.*)|transformation_(latencies_microseconds|failures_total)|(admission_quota_controller|APIServiceOpenAPIAggregationControllerQueue1|APIServiceRegistrationController|autoregister|AvailableConditionController|crd_(autoregistration_controller|Establishing|finalizer|naming_condition_controller|openapi_controller)|DiscoveryController|non_structural_schema_condition_controller|kubeproxy_sync_proxy_rules|rest_client_request_latency|storage_operation_(errors_total|status_count))(_.*)|apiserver_admission_(controller_admission|step_admission)_latencies_seconds_.*)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// drop metrics whose name and le label match
    the drop_les regex\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__name__\",\n\t\t\t\t\"le\",\n\t\t\t]\n\t\t\tregex
    \ = coalesce(argument.drop_les.value, \"apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)\")\n\t\t\taction
    = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics regex\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(.+)\")\n\t\t\taction        = \"keep\"\n\t\t}\n\t}\n}\n"
  kube-state-metrics.alloy: "/*\nModule Components: kube_state_metrics\nDescription:
    kubernetes kube_state_metrics Metrics Scrape\n\n*/\n\ndeclare \"kube_state_metrics_scrape\"
    {\n\n\t/********************************************\n\t* ARGUMENTS\n\t********************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"namespaces\" {\n\t\tcomment  = \"The namespaces to look for targets in (default:
    [] is all namespaces)\"\n\t\toptional = true\n\t}\n\n\targument \"field_selectors\"
    {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"app.kubernetes.io/name=kube-state-metrics\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The of the port to
    scrape metrics from (default: http)\"\n\t\toptional = true\n\t}\n\n\targument
    \"job_label\" {\n\t\tcomment  = \"The job label to add for all kube_state_metrics
    metrics (default: integrations/kubernetes/kube-state-metrics)\"\n\t\toptional
    = true\n\t}\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics
    to keep (default: see below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\"
    {\n\t\tcomment  = \"A regular expression of metrics to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Service Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"kube_state_metrics\" {\n\t\trole = \"service\"\n\n\t\tselectors {\n\t\t\trole
    \ = \"service\"\n\t\t\tfield = join(coalesce(argument.field_selectors.value, []),
    \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value, [\"app.kubernetes.io/name=kube-state-metrics\"]),
    \",\")\n\t\t}\n\n\t\tnamespaces {\n\t\t\tnames = coalesce(argument.namespaces.value,
    [])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"kube_state_metrics\" {\n\t\ttargets = discovery.kubernetes.kube_state_metrics.targets\n\n\t\t//
    only keep targets with a matching port name\n\t\trule {\n\t\t\tsource_labels =
    [\"__meta_kubernetes_service_port_name\"]\n\t\t\tregex         = coalesce(argument.port_name.value,
    \"http\")\n\t\t\taction        = \"keep\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"kube_state_metrics\" {\n\t\ttargets = discovery.relabel.kube_state_metrics.output\n\n\t\tjob_name
    \       = coalesce(argument.job_label.value, \"integrations/kubernetes/kube-state-metrics\")\n\t\tscrape_interval
    = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value,
    \"10s\")\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to =
    [prometheus.relabel.kube_state_metrics.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"kube_state_metrics\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go|process)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(up|kube_(daemonset.*|deployment_(metadata_generation|spec_replicas|status_(observed_generation|replicas_(available|updated)))|horizontalpodautoscaler_(spec_(max|min)_replicas|status_(current|desired)_replicas)|job.*|namespace_status_phase|node.*|persistentvolumeclaim_resource_requests_storage_bytes|pod_(container_(info|resource_(limits|requests)|status_(last_terminated_reason|restarts_total|waiting_reason))|info|owner|start_time|status_(phase|reason))|replicaset.*|resourcequota|statefulset.*))\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\t}\n}\n"
  kubelet.alloy: "/*\nModule Components: kubelet\nDescription: kubernetes kubelet
    Metrics Scrape\n\n*/\n\ndeclare \"kubelet_metrics_scrape\" {\n\n\t/********************************************\n\t*
    ARGUMENTS\n\t********************************************/\n\targument \"forward_to\"
    {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected metrics should
    be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument \"keep_metrics\"
    {\n\t\tcomment  = \"A regex of metrics to keep (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment  = \"A regular expression
    of metrics to drop (default: see below)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"max_cache_size\" {\n\t\tcomment  = \"The maximum number of elements to hold
    in the relabeling cache (default: 100000).  This should be at least 2x-5x your
    largest scrape target or samples appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"node\" {\n\t\trole = \"node\"\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"node\" {\n\t\ttargets = discovery.kubernetes.node.targets\n\n\t\t// set the
    address to use the kubernetes service dns name\n\t\trule {\n\t\t\ttarget_label
    = \"__address__\"\n\t\t\treplacement  = \"kubernetes.default.svc.cluster.local:443\"\n\t\t}\n\n\t\t//
    set the node label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\ttarget_label
    \ = \"node\"\n\t\t}\n\n\t\t// set the app name if specified as metadata labels
    \"app:\" or \"app.kubernetes.io/name:\" or \"k8s-app:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_service_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_service_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_service_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubelet\" {\n\t\ttargets = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label
    = \"job\"\n\t\t\treplacement  = \"integrations/kubernetes/kubelet\"\n\t\t}\n\n\t\t//
    set the metrics path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         =
    \"(.+)\"\n\t\t\treplacement   = \"/api/v1/nodes/${1}/proxy/metrics\"\n\t\t\ttarget_label
    \ = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"resources\" {\n\t\ttargets
    = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement
    \ = \"integrations/kubernetes/kube-resources\"\n\t\t}\n\n\t\t// set the metrics
    path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         = \"(.+)\"\n\t\t\treplacement
    \  = \"/api/v1/nodes/${1}/proxy/metrics/resource\"\n\t\t\ttarget_label  = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"probes\" {\n\t\ttargets = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label
    = \"job\"\n\t\t\treplacement  = \"integrations/kubernetes/kube-probes\"\n\t\t}\n\n\t\t//
    set the metrics path to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         =
    \"(.+)\"\n\t\t\treplacement   = \"/api/v1/nodes/${1}/proxy/metrics/probes\"\n\t\t\ttarget_label
    \ = \"__metrics_path__\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"cadvisor\" {\n\t\ttargets
    = discovery.relabel.node.output\n\n\t\trule {\n\t\t\ttarget_label = \"job\"\n\t\t\treplacement
    \ = \"integrations/kubernetes/cadvisor\"\n\t\t}\n\n\t\t// set the metrics path
    to use the proxy path to the nodes kubelet metrics endpoint\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_node_name\"]\n\t\t\tregex         = \"(.+)\"\n\t\t\treplacement
    \  = \"/api/v1/nodes/${1}/proxy/metrics/cadvisor\"\n\t\t\ttarget_label  = \"__metrics_path__\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"kubelet\" {\n\t\ttargets = array.concat(\n\t\t\tdiscovery.relabel.kubelet.output,\n\t\t\tdiscovery.relabel.resources.output,\n\t\t\tdiscovery.relabel.probes.output,\n\t\t\tdiscovery.relabel.cadvisor.output,\n\t\t)\n\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.kubelet.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"kubelet\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go|process)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t\taction        = \"keep\"\n\t\t}\n\t\t//
    Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"container\"]\n\t\t\tseparator     =
    \"@\"\n\t\t\tregex         = \"(container_cpu_.*|container_fs_.*|container_memory_.*)@\"\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"image\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex
    \        = \"(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@\"\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// Normalizing unimportant labels (not deleting
    to continue satisfying <label>!=\"\" checks)\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"boot_id\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"machine_memory_bytes@.*\"\n\t\t\ttarget_label  = \"boot_id\"\n\t\t\treplacement
    \  = \"NA\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__name__\", \"system_uuid\"]\n\t\t\tseparator
    \    = \"@\"\n\t\t\tregex         = \"machine_memory_bytes@.*\"\n\t\t\ttarget_label
    \ = \"system_uuid\"\n\t\t\treplacement   = \"NA\"\n\t\t}\n\n\t\t// Filter out
    non-physical devices/interfaces\n\t\trule {\n\t\t\tsource_labels = [\"__name__\",
    \"device\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         = \"container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"__keepme\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_fs_.*@\"\n\t\t\taction        = \"drop\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = \"container_fs_.*\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"interface\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\", \"__keepme\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex         =
    \"container_network_.*@\"\n\t\t\taction        = \"drop\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = \"container_network_.*\"\n\t\t\ttarget_label
    \ = \"__keepme\"\n\t\t\treplacement   = \"\"\n\t\t}\n\t}\n}\n"
  node-exporter.alloy: "/*\nModule Components: node_exporter\nDescription: kubernetes
    node_exporter Metrics Scrape\n\n*/\n\ndeclare \"node_exporter_metrics_scrape\"
    {\n\n\t/********************************************\n\t* ARGUMENTS\n\t********************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricsReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"namespaces\" {\n\t\tcomment  = \"The namespaces to look for targets in (default:
    [] is all namespaces)\"\n\t\toptional = true\n\t}\n\n\targument \"field_selectors\"
    {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [])\"\n\t\toptional
    = true\n\t}\n\n\targument \"label_selectors\" {\n\t\t// Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\t\tcomment
    \ = \"The label selectors to use to find matching targets (default: [\\\"app.kubernetes.io/name=prometheus-node-exporter\\\"])\"\n\t\toptional
    = true\n\t}\n\n\targument \"port_name\" {\n\t\tcomment  = \"The of the port to
    scrape metrics from (default: metrics)\"\n\t\toptional = true\n\t}\n\n\targument
    \"job_label\" {\n\t\tcomment  = \"The job label to add for all node-exporter metrics
    (default: integrations/kubernetes/node-exporter)\"\n\t\toptional = true\n\t}\n\n\targument
    \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep (default: see
    below)\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regular expression of metrics to drop (default: see below)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_interval\" {\n\t\tcomment  = \"How often to
    scrape metrics from the targets (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_timeout\" {\n\t\tcomment  = \"How long before a scrape times out (default:
    10s)\"\n\t\toptional = true\n\t}\n\n\targument \"max_cache_size\" {\n\t\tcomment
    \ = \"The maximum number of elements to hold in the relabeling cache (default:
    100000).  This should be at least 2x-5x your largest scrape target or samples
    appended rate.\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Service Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"node_exporter\" {\n\t\trole = \"pod\"\n\n\t\tselectors {\n\t\t\trole  = \"pod\"\n\t\t\tfield
    = join(coalesce(argument.field_selectors.value, []), \",\")\n\t\t\tlabel = join(coalesce(argument.label_selectors.value,
    [\"app.kubernetes.io/name=prometheus-node-exporter\"]), \",\")\n\t\t}\n\n\t\tnamespaces
    {\n\t\t\tnames = coalesce(argument.namespaces.value, [])\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\tdiscovery.relabel
    \"node_exporter\" {\n\t\ttargets = discovery.kubernetes.node_exporter.targets\n\n\t\t//
    keep only the specified metrics port name, and pods that are Running and ready\n\t\trule
    {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_container_port_name\",\n\t\t\t\t\"__meta_kubernetes_pod_phase\",\n\t\t\t\t\"__meta_kubernetes_pod_ready\",\n\t\t\t\t\"__meta_kubernetes_pod_container_init\",\n\t\t\t]\n\t\t\tseparator
    = \"@\"\n\t\t\tregex     = coalesce(argument.port_name.value, \"metrics\") + \"@Running@true@false\"\n\t\t\taction
    \   = \"keep\"\n\t\t}\n\n\t\t// set the namespace label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label  = \"namespace\"\n\t\t}\n\n\t\t//
    set the pod label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label
    \ = \"pod\"\n\t\t}\n\n\t\t// set the container label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label  = \"container\"\n\t\t}\n\n\t\t//
    set a workload label\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \"/\"\n\t\t\ttarget_label = \"workload\"\n\t\t}\n\t\t// remove the hash
    from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels = [\"workload\"]\n\t\t\tregex
    \        = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label  = \"workload\"\n\t\t}\n\n\t\t//
    set the app name if specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"
    or \"k8s-app:\"\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the component if specified
    as metadata labels \"component:\" or \"app.kubernetes.io/component:\" or \"k8s-component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the cluster label\n\t\trule {\n\t\t\taction
    \      = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\tprometheus.scrape
    \"node_exporter\" {\n\t\ttargets = discovery.relabel.node_exporter.output\n\n\t\tjob_name
    \       = coalesce(argument.job_label.value, \"integrations/kubernetes/node-exporter\")\n\t\tscrape_interval
    = coalesce(argument.scrape_interval.value, \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value,
    \"10s\")\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to =
    [prometheus.relabel.node_exporter.receiver]\n\t}\n\n\t/********************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t********************************************/\n\tprometheus.relabel
    \"node_exporter\" {\n\t\tforward_to     = argument.forward_to.value\n\t\tmax_cache_size
    = coalesce(argument.max_cache_size.value, 100000)\n\n\t\t// drop metrics that
    match the drop_metrics regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"(^(go)_.+$)\")\n\t\t\taction
    \       = \"drop\"\n\t\t}\n\n\t\t// keep only metrics that match the keep_metrics
    regex\n\t\trule {\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(up|node_exporter_build_info|scrape_(duration_seconds|series_added|samples_(post_metric_relabeling|scraped))|node_(arp_entries|boot_time_seconds|context_switches_total|cpu_seconds_total|disk_(io_time_seconds_total|io_time_weighted_seconds_total|read_(bytes_total|time_seconds_total)|reads_completed_total|write_time_seconds_total|writes_completed_total|written_bytes_total)|file(fd_(allocated|maximum)|system_(avail_bytes|device_error|files(_free)?|readonly|size_bytes))|intr_total|load(1|15|5)|md_disks(_required)?|memory_(Active_(anon_bytes|bytes|file_bytes)|Anon(HugePages_bytes|Pages_bytes)|Bounce_bytes|Buffers_bytes|Cached_bytes|CommitLimit_bytes|Committed_AS_bytes|DirectMap(1G|2M|4k)_bytes|Dirty_bytes|HugePages_(Free|Rsvd|Surp|Total)|Hugepagesize_bytes|Inactive_(anon_bytes|bytes|file_bytes)|Mapped_bytes|Mem(Available|Free|Total)_bytes|S(Reclaimable|Unreclaim)_bytes|Shmem(HugePages_bytes|PmdMapped_bytes|_bytes)|Slab_bytes|SwapTotal_bytes|Vmalloc(Chunk|Total|Used)_bytes|Writeback(Tmp|)_bytes)|netstat_(Icmp6_(InErrors|InMsgs|OutMsgs)|Icmp_(InErrors|InMsgs|OutMsgs)|IpExt_(InOctets|OutOctets)|TcpExt_(Listen(Drops|Overflows)|TCPSynRetrans)|Tcp_(InErrs|InSegs|OutRsts|OutSegs|RetransSegs)|Udp6_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors)|Udp(Lite|)_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors))|network_(carrier|info|mtu_bytes|receive_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total)|speed_bytes|transmit_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total|queue_length)|up)|nf_conntrack_(entries(_limit)?|limit)|os_info|sockstat_(FRAG6|FRAG|RAW6|RAW|TCP6|TCP_(alloc|inuse|mem(_bytes)?|orphan|tw)|UDP6|UDPLITE6|UDPLITE|UDP_(inuse|mem(_bytes)?)|sockets_used)|softnet_(dropped_total|processed_total|times_squeezed_total)|systemd_unit_state|textfile_scrape_error|time_zone_offset_seconds|timex_(estimated_error_seconds|maxerror_seconds|offset_seconds|sync_status)|uname_info|vmstat_(oom_kill|pgfault|pgmajfault|pgpgin|pgpgout|pswpin|pswpout)|process_(max_fds|open_fds)))\")\n\t\t\taction
    \       = \"keep\"\n\t\t}\n\n\t\t// Drop metrics for certain file systems\n\t\trule
    {\n\t\t\tsource_labels = [\"__name__\", \"fstype\"]\n\t\t\tseparator     = \"@\"\n\t\t\tregex
    \        = \"node_filesystem.*@(tempfs)\"\n\t\t\taction        = \"drop\"\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-jobs-9d9gc54kgg
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for logs based on kubernetes Pod annotations\n\n  Annotations:\n
    \   logs.grafana.com/scrape: true\n    logs.grafana.com/tenant: \"primary\"\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  =
    \"The tenant to filter logs to.  This does not have to be the tenantId, this is
    the value to look for in the logs.grafana.com/tenant annotation, and this can
    be a regex.\"\n\t\toptional = true\n\t\tdefault  = \".*\"\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"annotation_prefix\" {\n\t\tcomment  = \"The annotation_prefix
    to use (default: logs.grafana.com)\"\n\t\tdefault  = \"logs.grafana.com\"\n\t\toptional
    = true\n\t}\n\n\targument \"__sd_annotation\" {\n\t\toptional = true\n\t\tcomment
    \ = \"The logic is used to transform the annotation argument into a valid label
    name by removing unsupported characters.\"\n\t\tdefault  = string.replace(string.replace(string.replace(coalesce(argument.annotation_prefix.value,
    \"logs.grafana.com\"), \".\", \"_\"), \"/\", \"_\"), \"-\", \"_\")\n\t}\n\n\t//
    find all pods\n\tdiscovery.kubernetes \"annotation_logs\" {\n\t\trole = \"pod\"\n\t}\n\n\t//
    filter logs by kubernetes annotations\n\tdiscovery.relabel \"annotation_logs_filter\"
    {\n\t\ttargets = discovery.kubernetes.annotation_logs.targets\n\n\t\t// allow
    pods to declare their logs to be ingested or not, the default is true\n\t\t//
    \  i.e. logs.grafana.com/scrape: false\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\" + argument.__sd_annotation.value
    + \"_scrape\",\n\t\t\t]\n\t\t\tregex = \"^(true|)$\"\n\t\t}\n\n\t\t// allow pods
    to declare what tenant their logs should be written to, the following annotation
    is supported:\n\t\t//   logs.grafana.com/tenant: \"primary\"\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_annotation_\"
    + argument.__sd_annotation.value + \"_tenant\",\n\t\t\t]\n\t\t\tregex = \"^(\"
    + argument.tenant.value + \")$\"\n\t\t}\n\n\t\t// set the instance label as the
    name of the worker node the pod is on\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_node_name\"]\n\t\t\ttarget_label  = \"instance\"\n\t\t}\n\n\t\t//
    set the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    set the namespace label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the pod label\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\t//
    set the container label\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\n\t\t// set a workload label\n\t\trule {\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \"/\"\n\t\t\ttarget_label = \"workload\"\n\t\t}\n\t\t// remove the hash
    from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels = [\"workload\"]\n\t\t\tregex
    \        = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label  = \"workload\"\n\t\t}\n\n\t\t//
    set the app name if specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"
    or \"k8s-app:\"\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t// set the component if specified
    as metadata labels \"component:\" or \"app.kubernetes.io/component:\" or \"k8s-component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_k8s_component\",\n\t\t\t\t\"__meta_kubernetes_pod_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the version if specified as metadata labels
    \"version:\" or \"app.kubernetes.io/version:\" or \"app_version:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_version\",\n\t\t\t\t\"__meta_kubernetes_pod_label_app_version\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"version\"\n\t\t}\n\n\t\t// set a source label\n\t\trule {\n\t\t\taction       =
    \"replace\"\n\t\t\treplacement  = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\n\t\t//
    set the job label to be namespace / friendly pod name\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"workload\",\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t]\n\t\t\tregex
    \       = \".+\\\\/(.+);(.+)\"\n\t\t\treplacement  = \"$2/$1\"\n\t\t\ttarget_label
    = \"job\"\n\t\t}\n\n\t\t// make all labels on the pod available to the pipeline
    as labels,\n\t\t// they are omitted before write via labelallow unless explicitly
    set\n\t\trule {\n\t\t\taction = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\t//
    make all annotations on the pod available to the pipeline as labels,\n\t\t// they
    are omitted before write via labelallow unless explicitly set\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_annotation_(.+)\"\n\t\t}\n\n\t\t//
    as a result of kubernetes service discovery for pods, all of the meta data information
    is exposed in labels\n\t\t// __meta_kubernetes_pod_*, including __meta_kubernetes_pod_container_id
    which can be used to determine what\n\t\t// the pods container runtime is, docker
    (docker://...) or containerd (containerd://...) this will inform us\n\t\t// which
    parsing stage to use.  However, any labels that begin with __* are not passed
    to loki.process\n\t\t// (pipeline) stages. Use a relabeling stage to set a label
    that can be used a LogQL selector in the stage\n\t\t// below so parsing can be
    automatically determined, then drop the label from the loki.process stage.\n\t\t//
    set the container runtime as a label\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_id\"]\n\t\t\tregex         = \"^(\\\\w+):\\\\/\\\\/.+$\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t\ttarget_label  = \"tmp_container_runtime\"\n\t\t}\n\t}\n\n\tloki.source.kubernetes
    \"lsd_kubernetes_logs\" {\n\t\ttargets    = discovery.relabel.annotation_logs_filter.output\n\t\tforward_to
    = [loki.process.parse.receiver]\n\t}\n\n\t// parse the log based on the container
    runtime\n\tloki.process \"parse\" {\n\t\tforward_to = argument.forward_to.value\n\t\t/*******************************************************************************\n\t\t*
    \                        Container Runtime Parsing\n\t\t********************************************************************************/\n\t\t//
    if the label tmp_container_runtime from above is containerd parse using cri\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"containerd\\\"}\"\n\t\t\t// the
    cri processing stage extracts the following k/v pairs: log, stream, time, flags\n\t\t\tstage.cri
    { }\n\n\t\t\t// Set the extract flags and stream values as labels\n\t\t\tstage.labels
    {\n\t\t\t\tvalues = {\n\t\t\t\t\tflags  = \"\",\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t//
    if the label tmp_container_runtime from above is docker parse using docker\n\t\tstage.match
    {\n\t\t\tselector = \"{tmp_container_runtime=\\\"docker\\\"}\"\n\t\t\t// the docker
    processing stage extracts the following k/v pairs: log, stream, time\n\t\t\tstage.docker
    { }\n\n\t\t\t// Set the extract stream value as a label\n\t\t\tstage.labels {\n\t\t\t\tvalues
    = {\n\t\t\t\t\tstream = \"\",\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// drop the temporary
    container runtime label as it is no longer needed\n\t\tstage.label_drop {\n\t\t\tvalues
    = [\"tmp_container_runtime\"]\n\t\t}\n\t}\n}\n"
  k8s-events.alloy: "/*\nModule Components: component_cluster_events\n*/\n\ndeclare
    \"kubernetes_cluster_events\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"job_label\" {\n\t\toptional
    = true\n\t}\n\n\targument \"cluster\" { }\n\n\tloki.source.kubernetes_events \"cluster_events\"
    {\n\t\tjob_name   = coalesce(argument.job_label.value, \"integrations/kubernetes/eventhandler\")\n\t\tlog_format
    = \"logfmt\"\n\t\tforward_to = [loki.process.logs_service.receiver]\n\t}\n\n\tloki.process
    \"logs_service\" {\n\t\tstage.static_labels {\n\t\t\tvalues = {\n\t\t\t\tcluster
    = argument.cluster.value,\n\t\t\t}\n\t\t}\n\t\tforward_to = argument.forward_to.value\n\t}\n}\n"
  keep-labels.alloy: "/*\nModule Components: keep_labels\nDescription: Pre-defined
    set of labels to keep, this stage should always be in-place as the previous relabeing\n
    \            stages make every pod label and annotation a label in the pipeline,
    which we do not want created\n             in Loki as that would have extremely
    high-cardinality.\n*/\n\ndeclare \"keep_labels\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"keep_labels\" {\n\t\toptional
    = true\n\t\tcomment  = \"List of labels to keep before the log message is written
    to Loki\"\n\t\tdefault  = [\n\t\t\t\"app\",\n\t\t\t\"job\",\n\t\t\t\"region\",\n\t\t\t\"cluster\",\n\t\t\t\"namespace\",\n\t\t\t\"pod\",\n\t\t\t\"container\",\n\t\t\t\"component\",\n\t\t\t\"env\",\n\t\t\t\"level\",\n\t\t\t\"service\",\n\t\t\t\"squad\",\n\t\t\t\"team\",\n\t\t\t\"workload\",\n\t\t]\n\t}\n\n\t/*****************************************************************\n\t*
    LOKI PROCESS\n\t*****************************************************************/\n\tloki.process
    \"keep_labels\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t/*\n\t\tAs
    all of the pod labels and annotations we transformed into labels in the previous
    relabelings to make\n\t\tthem available to the pipeline processing we need to
    ensure they are not automatically created in Loki.\n\t\tThis would result in an
    extremely high number of labels and values severely impacting query performance.\n\t\tNot
    every log has to contain these labels, but this list should reflect the set of
    labels that you want\n\t\tto explicitly allow.\n\t\t*/\n\t\tstage.label_keep {\n\t\t\tvalues
    = argument.keep_labels.value\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"receiver\" {\n\t\tvalue = loki.process.keep_labels.receiver\n\t}\n}\n"
  rules-to-loki.alloy: "/*\nModule Components: rules_to_loki\nDescription: Auto discovers
    PrometheusRule Kubernetes resources and loads them into a Loki instance.\n*/\n\ndeclare
    \"rules_to_loki\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"address\" {\n\t\tcomment  = \"URL of the Loki ruler. (default: http://nginx.gateway.svc:3100)\"\n\t\toptional
    = true\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  = \"Mimir tenant ID. (default:
    fake)\"\n\t\toptional = true\n\t}\n\n\t/********************************************\n\t*
    Kubernetes Prometheus Rules To Loki\n\t********************************************/\n\tloki.rules.kubernetes
    \"rules_to_loki\" {\n\t\taddress   = coalesce(argument.address.value, \"http://nginx.gateway.svc:3100\")\n\t\ttenant_id
    = coalesce(argument.tenant.value, \"anonymous\")\n\n\t\t// rule_namespace_selector
    {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_loki= \"true\",\n\t\t//
    \t}\n\t\t// }\n\n\t\trule_selector {\n\t\t\tmatch_labels = {\n\t\t\t\tauto_rules_to_loki
    = \"true\",\n\t\t\t}\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-logs-b2t74ghdgm
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for metrics based on kubernetes annotations\n\nNote: Every argument
    except for \"forward_to\" is optional, and does have a defined default value.
    \ However, the values for these\n      arguments are not defined using the default
    = \" ... \" argument syntax, but rather using the coalesce(argument.value, \"
    ... \").\n      This is because if the argument passed in from another consuming
    module is set to null, the default = \" ... \" syntax will\n      does not override
    the value passed in, where coalesce() will return the first non-null value.\n\nKubernetes
    Annotation Auto-Scraping\n------------------------------------------------------------------------------------------------------------------------------------\nThis
    module is meant to be used to automatically scrape targets based on a certain
    role and set of annotations.  This module can be consumed\nmultiple times with
    different roles.  The supported roles are:\n\n  - pod\n  - service\n  - endpoints\n\nTypically,
    if mimicking the behavior of the prometheus-operator, and ServiceMonitor functionality
    you would use role=\"endpoints\", if\nmimicking the behavior of the PodMonitor
    functionality you would use role=\"pod\".  It is important to understand that
    with endpoints,\nthe target is typically going to be a pod, and whatever annotations
    that are set on the service will automatically be propagated to the\nendpoints.
    \ This is why the role \"endpoints\" is used, because it will scrape the pod,
    but also consider the service annotations.  Using\nrole=\"endpoints\", which scrape
    each endpoint associated to the service.  If role=\"service\" is used, it will
    only scrape the service, only\nhitting one of the endpoints associated to the
    service.\n\nThis is where you must consider your scraping strategy, for example
    if you scrape a service like \"kube-state-metrics\" using\nrole=\"endpoints\"
    you should only have a single replica of the kube-state-metrics pod, if you have
    multiple replicas, you should use\nrole=\"service\" or a separate non-annotation
    job completely.  Scraping a service instead of endpoints, is typically a rare
    use case, but\nit is supported.\n\nThere are other considerations for using annotation
    based scraping as well, which is metric relabeling rules that happen post scrape.
    \ If\nyou have a target that you want to apply a bunch of relabelings to or a
    very large metrics response payload, performance wise it will be\nbetter to have
    a separate job for that target, rather than using use annotations.  As every targert
    will go through the ssame relabeling.\nTypical deployment strategies/options would
    be:\n\nOption #1 (recommended):\n  - Annotation Scraping for role=\"endpoints\"\n
    \ - Separate Jobs for specific service scrapes (i.e. kube-state-metrics, node-exporter,
    etc.) or large metric payloads\n  - Separate Jobs for K8s API scraping (i.e. cadvisor,
    kube-apiserver, kube-scheduler, etc.)\n\nOption #2:\n  - Annotation Scraping for
    role=\"pod\"\n  - Annotation Scraping for role=\"service\"  (i.e. kube-state-metrics,
    node-exporter, etc.)\n  - Separate Jobs for specific use cases or large metric
    payloads\n  - Separate Jobs for K8s API scraping (i.e. cadvisor, kube-apiserver,
    kube-scheduler, etc.)\n\nAt no point should you use role=\"endpoints\" and role=\"pod\"
    together, as this will result in duplicate targets being scraped, thus\ngenerating
    duplicate metrics.  If you want to scrape both the pod and the service, use Option
    #2.\n\nEach port attached to an service/pod/endpoint is an eligible target, oftentimes
    it will have multiple ports.\nThere may be instances when you want to scrape all
    ports or some ports and not others. To support this\nthe following annotations
    are available:\n\n  metrics.grafana.com/scrape: true\n\nthe default scraping scheme
    is http, this can be specified as a single value which would override, the schema
    being used for all\nports attached to the target:\n\n  metrics.grafana.com/scheme:
    https\n\nthe default path to scrape is /metrics, this can be specified as a single
    value which would override, the scrape path being used\nfor all ports attached
    to the target:\n\n  metrics.grafana.com/path: /metrics/some_path\n\nthe default
    port to scrape is the target port, this can be specified as a single value which
    would override the scrape port being\nused for all ports attached to the target,
    note that even if aan target had multiple targets, the relabel_config targets
    are\ndeduped before scraping:\n\n  metrics.grafana.com/port: 8080\n\nthe default
    interval to scrape is 1m, this can be specified as a single value which would
    override, the scrape interval being used\nfor all ports attached to the target:\n\n
    \ metrics.grafana.com/interval: 5m\n\nthe default timeout for scraping is 10s,
    this can be specified as a single value which would override, the scrape interval
    being\nused for all ports attached to the target:\n\n  metrics.grafana.com/timeout:
    30s\n\nthe default job is namespace/{{ service name }} or namespace/{{ controller_name
    }} depending on the role, there may be instances\nin which a different job name
    is required because of a set of dashboards, rules, etc. to support this there
    is a job annotation\nwhich will override the default value:\n\n  metrics.grafana.com/job:
    integrations/kubernetes/kube-state-metrics\n*/\n\ndeclare \"annotations_scrape\"
    {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t}\n\n\targument \"tenant\" {\n\t\tcomment
    \ = \"The tenant to filter metrics to.  This does not have to be the tenantId,
    this is the value to look for in the metrics.agent.grafana.com/tenant annotation,
    and this can be a regex.\"\n\t\toptional = true\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"annotation_prefix\" {\n\t\tcomment  = \"The annotation_prefix
    to use (default: metrics.grafana.com)\"\n\t\tdefault  = \"metrics.grafana.com\"\n\t\toptional
    = true\n\t}\n\n\targument \"role\" {\n\t\tcomment  = \"The role to use when looking
    for targets to scrape via annotations, can be: endpoints, service, pod (default:
    endpoints)\"\n\t\toptional = true\n\t}\n\n\targument \"__sd_annotation\" {\n\t\toptional
    = true\n\t\tcomment  = \"The logic is used to transform the annotation argument
    into a valid label name by removing unsupported characters.\"\n\t\tdefault  =
    string.replace(string.replace(string.replace(coalesce(argument.annotation_prefix.value,
    \"metrics.grafana.com\"), \".\", \"_\"), \"/\", \"_\"), \"-\", \"_\")\n\t}\n\n\targument
    \"__pod_role\" {\n\t\tcomment  = \"Most annotation targets service or pod that
    is all you want, however if the role is endpoints you want the pod\"\n\t\toptional
    = true\n\t\tdefault  = string.replace(coalesce(argument.role.value, \"endpoints\"),
    \"endpoints\", \"pod\")\n\t}\n\n\targument \"__service_role\" {\n\t\tcomment  =
    \"Most annotation targets service or pod that is all you want, however if the
    role is endpoints you we also want to consider service annotations\"\n\t\toptional
    = true\n\t\tdefault  = string.replace(coalesce(argument.role.value, \"endpoints\"),
    \"endpoints\", \"service\")\n\t}\n\n\targument \"scrape_port_named_metrics\" {\n\t\tcomment
    \ = \"Whether or not to automatically scrape endpoints that have a port with 'metrics'
    in the name\"\n\t\toptional = true\n\t\tdefault  = false\n\t}\n\n\targument \"keep_metrics\"
    {\n\t\tcomment  = \"A regex of metrics to keep (default: (.+))\"\n\t\toptional
    = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment  = \"A regex of metrics
    to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_interval\"
    {\n\t\tcomment  = \"How often to scrape metrics from the targets (default: 60s)\"\n\t\toptional
    = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment  = \"How long before
    a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Targets From Docker Discovery\n\t*****************************************************************/\n\tdiscovery.kubernetes
    \"annotation_metrics\" {\n\t\trole = coalesce(argument.role.value, \"endpoints\")\n\n\t\tselectors
    {\n\t\t\trole = coalesce(argument.role.value, \"endpoints\")\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Discovery Relabelings (pre-scrape)\n\t*****************************************************************/\n\t//
    filter metrics by kubernetes annotations\n\tdiscovery.relabel \"annotation_metrics_filter\"
    {\n\t\ttargets = discovery.kubernetes.annotation_metrics.targets\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Targets to Keep or Drop\n\t\t****************************************************************************************************************/\n\t\t//
    allow resources to declare their metrics scraped or not\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/scrape: false\n\t\t//\n\t\t// the label prometheus.io/service-monitor:
    \"false\" is a common label for headless services, when performing endpoint\n\t\t//
    service discovery, if there is both a load-balanced service and headless service,
    this can result in duplicate\n\t\t// scrapes if the name of the service is attached
    as a label.  any targets with this label or annotation set should be dropped\n\t\trule
    {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = \"false\"\n\t\t\ttarget_label
    = \"__tmp_scrape\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_scrape\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_scrape\",\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value,
    \"endpoints\") + \"_label_prometheus_io_service_monitor\",\n\t\t\t]\n\t\t\tseparator
    = \";\"\n\t\t\t// only allow empty or true, otherwise defaults to false\n\t\t\tregex
    \       = \"^(?:;*)?(true)(;|true)*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"__tmp_scrape\"\n\t\t}\n\n\t\t// add a __tmp_scrape_port_named_metrics from
    the argument.scrape_port_named_metrics\n\t\trule {\n\t\t\treplacement  = string.format(\"%t\",
    argument.scrape_port_named_metrics.value)\n\t\t\ttarget_label = \"__tmp_scrape_port_named_metrics\"\n\t\t}\n\n\t\t//
    only keep targets that have scrape: true or \"metrics\" in the port name if the
    argument scrape_port_named_metrics\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__tmp_scrape\",\n\t\t\t\t\"__tmp_scrape_port_named_metrics\",\n\t\t\t\t//
    endpoints is the role and most meta labels started with \"endpoints\", however
    the port name is an exception and starts with \"endpoint\"\n\t\t\t\t\"__meta_kubernetes_\"
    + string.replace(coalesce(argument.role.value, \"endpoints\"), \"endpoints\",
    \"endpoint\") + \"_port_name\",\n\t\t\t]\n\t\t\tseparator = \";\"\n\t\t\tregex
    \    = \"^(true;.*|(|true);true;(.*metrics.*))$\"\n\t\t}\n\n\t\t// only keep targets
    where the pod is running or the pod_phase is empty and is not an init container.
    \ This will only exist for role=\"pod\" or\n\t\t// potentially role=\"endpoints\",
    if it is a service the value is empty and thus allowed to pass, if it is an endpoint
    but not associated to a\n\t\t// pod but rather a static IP or hostname, that could
    be outside of kubernetes allow endpoints to declare what tenant their metrics
    should be\n\t\t// written to\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_phase\"]\n\t\t\tregex         = \"^(?i)(Running|)$\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_ready\"]\n\t\t\tregex
    \        = \"^(true|)$\"\n\t\t}\n\t\t// if the container is an init container,
    drop it\n\t\trule {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_init\"]\n\t\t\tregex
    \        = \"^(true)$\"\n\t\t}\n\n\t\t// allow resources to declare their metrics
    the tenant their metrics should be sent to,\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/tenant: primary\n\t\t//\n\t\t// Note: This does not necessarily
    have to be the actual tenantId, it can be a friendly name as well that is simply
    used\n\t\t//       to determine if the metrics should be gathered for the current
    tenant\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_tenant\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_tenant\",\n\t\t\t]\n\t\t\tregex
    = \"^(\" + coalesce(argument.tenant.value, \".*\") + \")$\"\n\t\t}\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Setting Scrape Metadata i.e. path, port, interval etc.\n\t\t****************************************************************************************************************/\n\t\t//
    allow resources to declare the protocol to use when collecting metrics, the default
    value is \"http\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/scheme:
    http\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = \"http\"\n\t\t\ttarget_label
    = \"__scheme__\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_scheme\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_scheme\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?(https?).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scheme__\"\n\t\t}\n\n\t\t// allow resources
    to declare the port to use when collecting metrics, the default value is the discovered
    port from\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/port: 9090\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__address__\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_port\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value +
    \"_annotation_\" + argument.__sd_annotation.value + \"_port\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^([^:]+)(?::\\\\d+)?;(\\\\d+)$\"\n\t\t\treplacement
    \ = \"$1:$2\"\n\t\t\ttarget_label = \"__address__\"\n\t\t}\n\n\t\t// allow resources
    to declare their the path to use when collecting their metrics, the default value
    is \"/metrics\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/path:
    /metrics/foo\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_path\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_path\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__metrics_path__\"\n\t\t}\n\n\t\t// allow resources
    to declare how often their metrics should be collected, the default value is 1m,\n\t\t//
    the following duration formats are supported (s|m|ms|h|d):\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/interval: 5m\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\ttarget_label = \"__scrape_interval__\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_interval\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_interval\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?(\\\\d+(s|m|ms|h|d)).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scrape_interval__\"\n\t\t}\n\n\t\t// allow
    resources to declare the timeout of the scrape request, the default value is 10s,\n\t\t//
    the following duration formats are supported (s|m|ms|h|d):\n\t\t// Example Annotation:\n\t\t//
    \  metrics.grafana.com/timeout: 30s\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t\ttarget_label = \"__scrape_timeout__\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_annotation_\" + argument.__sd_annotation.value
    + \"_timeout\",\n\t\t\t\t\"__meta_kubernetes_\" + argument.__service_role.value
    + \"_annotation_\" + argument.__sd_annotation.value + \"_timeout\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?(\\\\d+(s|m|ms|h|d)).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"__scrape_timeout__\"\n\t\t}\n\n\t\t/****************************************************************************************************************\n\t\t*
    Handle Setting Common Labels\n\t\t****************************************************************************************************************/\n\t\t//
    set a source label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = \"kubernetes\"\n\t\t\ttarget_label = \"source\"\n\t\t}\n\n\t\t// set the cluster
    label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement  = argument.cluster.value\n\t\t\ttarget_label
    = \"cluster\"\n\t\t}\n\n\t\t// set the namespace label\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\t// set the target name label i.e. service name,
    pod name, etc.\n\t\t// if the role is endpoints, the first valued field is used
    which would be __meta_kubernetes_pod_name, if the pod name is empty\n\t\t// then
    the endpoint name would be used\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + argument.__pod_role.value + \"_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  =
    \"$1\"\n\t\t\ttarget_label = argument.__pod_role.value\n\t\t}\n\n\t\t// set a
    default job label to be the namespace/pod_controller_name or namespace/service_name\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t\targument.__pod_role.value,\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^([^;]+)(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1/$2\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// if the controller
    is a ReplicaSet, drop the hash from the end of the ReplicaSet\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_type\",\n\t\t\t\t\"__meta_kubernetes_namespace\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"^(?:ReplicaSet);([^;]+);([^;]+)-.+$\"\n\t\t\treplacement
    \ = \"$1/$2\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// allow resources to
    declare their the job label value to use when collecting their metrics, the default
    value is \"\",\n\t\t// Example Annotation:\n\t\t//   metrics.grafana.com/job:
    integrations/kubernetes/cadvisor\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\n\t\t\t\t\"__meta_kubernetes_\" + coalesce(argument.role.value, \"endpoints\")
    + \"_annotation_\" + argument.__sd_annotation.value + \"_job\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_annotation_\" + argument.__sd_annotation.value
    + \"_job\",\n\t\t\t]\n\t\t\tseparator    = \";\"\n\t\t\tregex        = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement
    \ = \"$1\"\n\t\t\ttarget_label = \"job\"\n\t\t}\n\n\t\t// set the app name if
    specified as metadata labels \"app:\" or \"app.kubernetes.io/name:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_name\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_k8s_app\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app\",\n\t\t\t]\n\t\t\tregex        =
    \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label = \"app\"\n\t\t}\n\n\t\t//
    set the app component if specified as metadata labels \"component:\" or \"app.kubernetes.io/component:\"\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_component\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_component\",\n\t\t\t]\n\t\t\tregex
    \       = \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label
    = \"component\"\n\t\t}\n\n\t\t// set the version if specified as metadata labels
    \"version:\" or \"app.kubernetes.io/version:\" or \"app_version:\"\n\t\trule {\n\t\t\taction
    \       = \"replace\"\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + coalesce(argument.role.value, \"endpoints\") + \"_label_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__pod_role.value + \"_label_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_app_kubernetes_io_version\",\n\t\t\t\t\"__meta_kubernetes_\"
    + argument.__service_role.value + \"_label_version\",\n\t\t\t]\n\t\t\tregex        =
    \"^(?:;*)?([^;]+).*$\"\n\t\t\treplacement  = \"$1\"\n\t\t\ttarget_label = \"version\"\n\t\t}\n\n\t\t//
    set a workload label if the resource is a pod\n\t\t// example: grafana-agent-68nv9
    becomes DaemonSet/grafana-agent\n\t\trule {\n\t\t\tsource_labels = [\n\t\t\t\t\"__meta_kubernetes_pod_controller_kind\",\n\t\t\t\t\"__meta_kubernetes_pod_controller_name\",\n\t\t\t]\n\t\t\tseparator
    \   = \";\"\n\t\t\tregex        = \"(.+);(.+)\"\n\t\t\treplacement  = \"$1/$2\"\n\t\t\ttarget_label
    = \"workload\"\n\t\t}\n\t\t// remove the hash from the ReplicaSet\n\t\trule {\n\t\t\tsource_labels
    = [\"workload\"]\n\t\t\tregex         = \"(ReplicaSet/.+)-.+\"\n\t\t\ttarget_label
    \ = \"workload\"\n\t\t}\n\t}\n\n\t// only keep http targets\n\tdiscovery.relabel
    \"http_annotations\" {\n\t\ttargets = discovery.relabel.annotation_metrics_filter.output\n\n\t\trule
    {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\"__scheme__\"]\n\t\t\tregex
    \        = \"http\"\n\t\t}\n\t}\n\n\t// only keep https targets\n\tdiscovery.relabel
    \"https_annotations\" {\n\t\ttargets = discovery.relabel.annotation_metrics_filter.output\n\n\t\trule
    {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels = [\"__scheme__\"]\n\t\t\tregex
    \        = \"https\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Scrape Labels Targets\n\t*****************************************************************/\n\t//
    scrape http only targtetsa\n\tprometheus.scrape \"http_annotations\" {\n\t\ttargets
    = discovery.relabel.http_annotations.output\n\n\t\tjob_name        = \"annotation-metrics-http\"\n\t\tscheme
    \         = \"http\"\n\t\tscrape_interval = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout  = coalesce(argument.scrape_timeout.value, \"10s\")\n\n\t\tscrape_classic_histograms
    = true\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.annotations.receiver]\n\t}\n\n\t//
    scrape https only targtets\n\tprometheus.scrape \"https_annotations\" {\n\t\ttargets
    = discovery.relabel.https_annotations.output\n\n\t\tjob_name          = \"annotation-metrics-https\"\n\t\tscheme
    \           = \"https\"\n\t\tscrape_interval   = coalesce(argument.scrape_interval.value,
    \"60s\")\n\t\tscrape_timeout    = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\tbearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n\t\ttls_config {\n\t\t\tca_file
    \             = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n\t\t\tinsecure_skip_verify
    = false\n\t\t\tserver_name          = \"kubernetes\"\n\t\t}\n\n\t\tscrape_classic_histograms
    = true\n\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\tforward_to = [prometheus.relabel.annotations.receiver]\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"annotations\" {\n\t\tforward_to = argument.forward_to.value\n\t\t// keep only
    metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction        = \"keep\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.keep_metrics.value,
    \"(.+)\")\n\t\t}\n\n\t\t// drop metrics that match the drop_metrics regex\n\t\trule
    {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex
    \        = coalesce(argument.drop_metrics.value, \"\")\n\t\t}\n\t}\n}\n"
  podmonitors-scrape.alloy: "/*\nModule Components: podmonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.podmonitors\n*/\n\ndeclare
    \"podmonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep
    (default: (.+))\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regex of metrics to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape PodMonitors\n\t*****************************************************************/\n\tprometheus.operator.podmonitors
    \"scrape\" {\n\t\tforward_to = [prometheus.relabel.podmonitors.receiver]\n\n\t\tscrape
    {\n\t\t\tdefault_scrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\tdefault_scrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\n\t\t// selector {\n\t\t// \tmatch_expression
    {\n\t\t// \t\tkey      = \"team\"\n\t\t// \t\toperator = \"In\"\n\t\t// \t\tvalues
    \  = [\"team-infra\"]\n\t\t// \t}\n\t\t// }\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"podmonitors\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t// set the
    cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t}\n\n\t\t// drop metrics
    that match the drop_metrics regex\n\t\trule {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.drop_metrics.value,
    \"\")\n\t\t}\n\t}\n}\n"
  rules-to-mimir.alloy: "/*\nModule Components: rules_to_mimir\nDescription: Auto
    discovers PrometheusRule Kubernetes resources and loads them into a Mimir instance.\n*/\n\ndeclare
    \"rules_to_mimir\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"address\" {\n\t\tcomment  = \"URL of the Mimir ruler. (default: http://nginx.gateway.svc:8080)\"\n\t\toptional
    = true\n\t}\n\n\targument \"tenant\" {\n\t\tcomment  = \"Mimir tenant ID. (default:
    anonymous)\"\n\t\toptional = true\n\t}\n\n\t/********************************************\n\t*
    Kubernetes Prometheus Rules To Mimir\n\t********************************************/\n\tmimir.rules.kubernetes
    \"rules_to_mimir\" {\n\t\taddress   = coalesce(argument.address.value, \"http://nginx.gateway.svc:8080\")\n\t\ttenant_id
    = coalesce(argument.tenant.value, \"anonymous\")\n\n\t\t// rule_namespace_selector
    {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_mimir= \"true\",\n\t\t//
    \t}\n\t\t// }\n\n\t\t// rule_selector {\n\t\t// \tmatch_labels = {\n\t\t// \t\tauto_rules_to_mimir=
    \"true\",\n\t\t// \t}\n\t\t// }\n\t}\n}\n"
  servicemonitors-scrape.alloy: "/*\nModule Components: servicemonitors_scrape\nDescription:
    Scrapes targets for metrics based on prometheus.operator.servicemonitors\n*/\n\ndeclare
    \"servicemonitors_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment  = \"Must be a list(MetricssReceiver) where collected
    metrics should be forwarded to\"\n\t\toptional = false\n\t}\n\n\targument \"cluster\"
    { }\n\n\targument \"keep_metrics\" {\n\t\tcomment  = \"A regex of metrics to keep
    (default: (.+))\"\n\t\toptional = true\n\t}\n\n\targument \"drop_metrics\" {\n\t\tcomment
    \ = \"A regex of metrics to drop (default: \\\"\\\")\"\n\t\toptional = true\n\t}\n\n\targument
    \"scrape_interval\" {\n\t\tcomment  = \"How often to scrape metrics from the targets
    (default: 60s)\"\n\t\toptional = true\n\t}\n\n\targument \"scrape_timeout\" {\n\t\tcomment
    \ = \"How long before a scrape times out (default: 10s)\"\n\t\toptional = true\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Auto Scrape ServiceMonitors\n\t*****************************************************************/\n\tprometheus.operator.servicemonitors
    \"scrape\" {\n\t\tforward_to = [prometheus.relabel.servicemonitors.receiver]\n\n\t\tscrape
    {\n\t\t\tdefault_scrape_interval = coalesce(argument.scrape_interval.value, \"60s\")\n\t\t\tdefault_scrape_timeout
    \ = coalesce(argument.scrape_timeout.value, \"10s\")\n\t\t}\n\n\t\tclustering
    {\n\t\t\tenabled = true\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Prometheus Metric Relabelings (post-scrape)\n\t*****************************************************************/\n\t//
    perform generic relabeling using keep_metrics and drop_metrics\n\tprometheus.relabel
    \"servicemonitors\" {\n\t\tforward_to = argument.forward_to.value\n\n\t\t// set
    the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\t//
    keep only metrics that match the keep_metrics regex\n\t\trule {\n\t\t\taction
    \       = \"keep\"\n\t\t\tsource_labels = [\"__name__\"]\n\t\t\tregex         =
    coalesce(argument.keep_metrics.value, \"(.+)\")\n\t\t}\n\n\t\t// drop metrics
    that match the drop_metrics regex\n\t\trule {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels
    = [\"__name__\"]\n\t\t\tregex         = coalesce(argument.drop_metrics.value,
    \"\")\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-metrics-8287kbcb62
  namespace: monitoring-system
---
apiVersion: v1
data:
  annotations-scrape.alloy: "/*\nModule Components: annotations_scrape\nDescription:
    Scrapes targets for metrics based on kubernetes Pod annotations\n\n*/\n\ndeclare
    \"annotations_scrape\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"forward_to\" {\n\t\tcomment = \"Must be a list(ProfilessReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\tdiscovery.kubernetes
    \"pyroscope_kubernetes\" {\n\t\trole = \"pod\"\n\t}\n\n\t// The default scrape
    config allows to define annotations based scraping.\n\t//\n\t// For example the
    following annotations:\n\t//\n\t// ```\n\t// profiles.grafana.com/memory.scrape:
    \"true\"\n\t// profiles.grafana.com/memory.port: \"8080\"\n\t// profiles.grafana.com/cpu.scrape:
    \"true\"\n\t// profiles.grafana.com/cpu.port: \"8080\"\n\t// profiles.grafana.com/goroutine.scrape:
    \"true\"\n\t// profiles.grafana.com/goroutine.port: \"8080\"\n\t// ```\n\t//\n\t//
    will scrape the `memory`, `cpu` and `goroutine` profiles from the `8080` port
    of the pod.\n\t//\n\t// For more information see https://grafana.com/docs/phlare/latest/operators-guide/deploy-kubernetes/#optional-scrape-your-own-workloads-profiles\n\tdiscovery.relabel
    \"kubernetes_pods\" {\n\t\ttargets = array.concat(discovery.kubernetes.pyroscope_kubernetes.targets)\n\n\t\trule
    {\n\t\t\taction        = \"drop\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_phase\"]\n\t\t\tregex
    \        = \"Pending|Succeeded|Failed|Completed\"\n\t\t}\n\n\t\trule {\n\t\t\taction
    = \"labelmap\"\n\t\t\tregex  = \"__meta_kubernetes_pod_label_(.+)\"\n\t\t}\n\n\t\t//
    set the cluster label\n\t\trule {\n\t\t\taction       = \"replace\"\n\t\t\treplacement
    \ = argument.cluster.value\n\t\t\ttarget_label = \"cluster\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_namespace\"]\n\t\t\ttarget_label
    \ = \"namespace\"\n\t\t}\n\n\t\trule {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_name\"]\n\t\t\ttarget_label  = \"pod\"\n\t\t}\n\n\t\trule
    {\n\t\t\taction        = \"replace\"\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_container_name\"]\n\t\t\ttarget_label
    \ = \"container\"\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_memory_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_memory_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Memory\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_memory\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_memory_default_name.output,
    discovery.relabel.kubernetes_pods_memory_custom_name.output)\n\t\tforward_to =
    argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_cpu_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_cpu_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape CPU\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_cpu\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_cpu_default_name.output,
    discovery.relabel.kubernetes_pods_cpu_custom_name.output)\n\t\tforward_to = argument.forward_to.value\n\n\t\tprofiling_config
    {\n\t\t\tprofile.memory {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.goroutine {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.mutex
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_goroutine_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_goroutine_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Goroutine\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_goroutine\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_goroutine_default_name.output,
    discovery.relabel.kubernetes_pods_goroutine_custom_name.output)\n\t\tforward_to
    = argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled =
    false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_block_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_block_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Block\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_block\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_block_default_name.output,
    discovery.relabel.kubernetes_pods_block_custom_name.output)\n\t\tforward_to =
    argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = true\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_mutex_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_mutex_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Mutex\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_mutex\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_mutex_default_name.output,
    discovery.relabel.kubernetes_pods_mutex_custom_name.output)\n\t\tforward_to =
    argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = true\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\t\t}\n\t}\n\n\tdiscovery.relabel \"kubernetes_pods_fgprof_default_name\"
    {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\tdiscovery.relabel
    \"kubernetes_pods_fgprof_custom_name\" {\n\t\ttargets = array.concat(discovery.relabel.kubernetes_pods.output)\n\n\t\trule
    {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape\"]\n\t\t\taction
    \       = \"keep\"\n\t\t\tregex         = \"true\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"]\n\t\t\taction
    \       = \"drop\"\n\t\t\tregex         = \"\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_container_port_name\"]\n\t\t\ttarget_label  = \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name\"\n\t\t\taction
    \       = \"keepequal\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(https?)\"\n\t\t\ttarget_label
    \ = \"__scheme__\"\n\t\t\treplacement   = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels
    = [\"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+)\"\n\t\t\ttarget_label  = \"__profile_path__\"\n\t\t\treplacement
    \  = \"$1\"\n\t\t}\n\n\t\trule {\n\t\t\tsource_labels = [\"__address__\", \"__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port\"]\n\t\t\taction
    \       = \"replace\"\n\t\t\tregex         = \"(.+?)(?::\\\\d+)?;(\\\\d+)\"\n\t\t\ttarget_label
    \ = \"__address__\"\n\t\t\treplacement   = \"$1:$2\"\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Kubernetes Pyroscope Scrape Fgprof\n\t*****************************************************************/\n\tpyroscope.scrape
    \"pyroscope_scrape_fgprof\" {\n\t\tclustering {\n\t\t\tenabled = true\n\t\t}\n\n\t\ttargets
    \   = array.concat(discovery.relabel.kubernetes_pods_fgprof_default_name.output,
    discovery.relabel.kubernetes_pods_fgprof_custom_name.output)\n\t\tforward_to =
    argument.forward_to.value\n\n\t\tprofiling_config {\n\t\t\tprofile.memory {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.process_cpu {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.goroutine
    {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.block {\n\t\t\t\tenabled
    = false\n\t\t\t}\n\n\t\t\tprofile.mutex {\n\t\t\t\tenabled = false\n\t\t\t}\n\n\t\t\tprofile.fgprof
    {\n\t\t\t\tenabled = true\n\t\t\t}\n\t\t}\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-profiles-6479gm5dc2
  namespace: monitoring-system
---
apiVersion: v1
data:
  process-and-transform.alloy: "/*\nModule Components: process_and_transform\n\nDescription:
    Traces data collection processing and transformation\n*/\n\n// Processing And
    Transformation\ndeclare \"process_and_transform\" {\n\n\t/*****************************************************************\n\t*
    ARGUMENTS\n\t*****************************************************************/\n\targument
    \"traces_forward_to\" {\n\t\tcomment = \"Must be a list(TracesReceiver) where
    collected traces should be forwarded to\"\n\t}\n\n\targument \"cluster\" { }\n\n\targument
    \"logs_forward_to\" {\n\t\tcomment = \"Must be a list(LogsReceiver) where collected
    logs should be forwarded to\"\n\t}\n\n\targument \"metrics_forward_to\" {\n\t\tcomment
    = \"Must be a list(MetricsReceiver) where collected metrics should be forwarded
    to\"\n\t}\n\n\targument \"otlp_http_endpoint\" {\n\t\toptional = true\n\t\tdefault
    \ = \"0.0.0.0:4318\"\n\t}\n\n\targument \"otlp_grpc_endpoint\" {\n\t\toptional
    = true\n\t\tdefault  = \"0.0.0.0:4317\"\n\t}\n\n\t/*****************************************************************\n\t*
    Jaeger for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.jaeger
    \"default\" {\n\t\tprotocols {\n\t\t\tgrpc {\n\t\t\t\tendpoint = \"0.0.0.0:14250\"\n\t\t\t}\n\n\t\t\tthrift_http
    {\n\t\t\t\tendpoint = \"0.0.0.0:14268\"\n\t\t\t}\n\n\t\t\tthrift_binary {\n\t\t\t\tendpoint
    = \"0.0.0.0:6832\"\n\t\t\t}\n\n\t\t\tthrift_compact {\n\t\t\t\tendpoint = \"0.0.0.0:6831\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\ttraces = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\t/*****************************************************************\n\t*
    Otelcol for Metrics Logs Traces\n\t*****************************************************************/\n\totelcol.receiver.otlp
    \"default\" {\n\t\tdebug_metrics {\n\t\t\tdisable_high_cardinality_metrics = true\n\t\t}\n\n\t\tgrpc
    {\n\t\t\tendpoint = argument.otlp_grpc_endpoint.value\n\t\t}\n\n\t\thttp {\n\t\t\tendpoint
    = argument.otlp_http_endpoint.value\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.resourcedetection.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.resourcedetection.default.input]\n\t\t\ttraces  = [\n\t\t\t\totelcol.processor.resourcedetection.default.input,\n\t\t\t\totelcol.connector.spanlogs.autologging.input,\n\t\t\t]\n\t\t}\n\t}\n\n\totelcol.processor.resourcedetection
    \"default\" {\n\t\tdetectors = [\"env\", \"system\"]\n\n\t\tsystem {\n\t\t\thostname_sources
    = [\"os\"]\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]\n\n\t\t\tlogs
    \  = [otelcol.processor.k8sattributes.default.input]\n\t\t\ttraces = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.transform
    \"add_metric_datapoint_attributes\" {\n\t\terror_mode = \"ignore\"\n\n\t\tmetric_statements
    {\n\t\t\tcontext    = \"datapoint\"\n\t\t\tstatements = [\n\t\t\t\t\"set(attributes[\\\"deployment.environment\\\"],
    resource.attributes[\\\"deployment.environment\\\"])\",\n\t\t\t\t\"set(attributes[\\\"service.version\\\"],
    resource.attributes[\\\"service.version\\\"])\",\n\t\t\t]\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.k8sattributes.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.k8sattributes
    \"default\" {\n\t\textract {\n\t\t\tmetadata = [\n\t\t\t\t\"k8s.namespace.name\",\n\t\t\t\t\"k8s.pod.name\",\n\t\t\t\t\"k8s.deployment.name\",\n\t\t\t\t\"k8s.statefulset.name\",\n\t\t\t\t\"k8s.daemonset.name\",\n\t\t\t\t\"k8s.cronjob.name\",\n\t\t\t\t\"k8s.job.name\",\n\t\t\t\t\"k8s.node.name\",\n\t\t\t\t\"k8s.pod.uid\",\n\t\t\t\t\"k8s.pod.start_time\",\n\t\t\t]\n\t\t}\n\n\t\tpod_association
    {\n\t\t\tsource {\n\t\t\t\tfrom = \"connection\"\n\t\t\t}\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.transform.default.input]\n\t\t\tlogs    =
    [otelcol.processor.transform.default.input]\n\t\t\ttraces  = [\n\t\t\t\totelcol.processor.transform.default.input,\n\t\t\t\totelcol.connector.host_info.default.input,\n\t\t\t]\n\t\t}\n\t}\n\n\totelcol.connector.host_info
    \"default\" {\n\t\thost_identifiers = [\"k8s.node.name\"]\n\n\t\toutput {\n\t\t\tmetrics
    = [otelcol.processor.batch.host_info_batch.input]\n\t\t}\n\t}\n\n\totelcol.processor.batch
    \"host_info_batch\" {\n\t\toutput {\n\t\t\tmetrics = [otelcol.exporter.prometheus.host_info_metrics.input]\n\t\t}\n\t}\n\n\totelcol.exporter.prometheus
    \"host_info_metrics\" {\n\t\tadd_metric_suffixes = false\n\t\tforward_to          =
    argument.metrics_forward_to.value\n\t}\n\n\totelcol.processor.transform \"default\"
    {\n\t\terror_mode = \"ignore\"\n\n\t\tmetric_statements {\n\t\t\tcontext    =
    \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\tlog_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`set(attributes[\"pod\"],
    attributes[\"k8s.pod.name\"])`,\n\t\t\t\t`set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])`,\n\t\t\t\t`set(attributes[\"loki.resource.labels\"],
    \"pod, namespace, cluster, job\")`,\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\ttrace_statements
    {\n\t\t\tcontext    = \"resource\"\n\t\t\tstatements = [\n\t\t\t\t`limit(attributes,
    100, [])`,\n\t\t\t\t`truncate_all(attributes, 4096)`,\n\t\t\t\t`set(attributes[\"k8s.cluster.name\"],
    \"k3d-k3s-codelab\") where attributes[\"k8s.cluster.name\"] == nil`,\n\t\t\t]\n\t\t}\n\n\t\ttrace_statements
    {\n\t\t\tcontext    = \"span\"\n\t\t\tstatements = [\n\t\t\t\t`limit(attributes,
    100, [])`,\n\t\t\t\t`truncate_all(attributes, 4096)`,\n\t\t\t]\n\t\t}\n\n\t\toutput
    {\n\t\t\tmetrics = [otelcol.processor.filter.default.input]\n\t\t\tlogs    = [otelcol.processor.filter.default.input]\n\t\t\ttraces
    \ = [otelcol.processor.filter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.filter
    \"default\" {\n\t\terror_mode = \"ignore\"\n\n\t\ttraces {\n\t\t\tspan = [\n\t\t\t\t\"attributes[\\\"http.route\\\"]
    == \\\"/live\\\"\",\n\t\t\t\t\"attributes[\\\"http.route\\\"] == \\\"/healthy\\\"\",\n\t\t\t\t\"attributes[\\\"http.route\\\"]
    == \\\"/ready\\\"\",\n\t\t\t]\n\t\t}\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.batch.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.batch.default.input]\n\t\t\ttraces  = [otelcol.processor.batch.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.batch
    \"default\" {\n\t\tsend_batch_size     = 16384\n\t\tsend_batch_max_size = 0\n\t\ttimeout
    \            = \"5s\"\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.processor.memory_limiter.default.input]\n\t\t\tlogs
    \   = [otelcol.processor.memory_limiter.default.input]\n\t\t\ttraces  = [otelcol.processor.memory_limiter.default.input]\n\t\t}\n\t}\n\n\totelcol.processor.memory_limiter
    \"default\" {\n\t\tcheck_interval         = \"1s\"\n\t\tlimit_percentage       =
    50\n\t\tspike_limit_percentage = 30\n\n\t\toutput {\n\t\t\tmetrics = [otelcol.exporter.prometheus.tracesmetrics.input]\n\t\t\tlogs
    \   = [otelcol.exporter.loki.traceslogs.input]\n\t\t\ttraces  = argument.traces_forward_to.value\n\t\t}\n\t}\n\n\totelcol.exporter.prometheus
    \"tracesmetrics\" {\n\t\tforward_to = argument.metrics_forward_to.value\n\t}\n\n\totelcol.exporter.loki
    \"traceslogs\" {\n\t\tforward_to = [loki.process.traceslogs.receiver]\n\t}\n\n\t//
    The OpenTelemetry spanlog connector processes incoming trace spans and extracts
    data from them ready\n\t// for logging.\n\totelcol.connector.spanlogs \"autologging\"
    {\n\t\t// We only want to output a line for each root span (ie. every single trace),
    and not for every\n\t\t// process or span (outputting a line for every span would
    be extremely verbose).\n\t\tspans     = false\n\t\troots     = true\n\t\tprocesses
    = false\n\n\t\t// We want to ensure that the following three span attributes are
    included in the log line, if present.\n\t\tspan_attributes = [\n\t\t\t\"http.method\",\n\t\t\t\"http.target\",\n\t\t\t\"http.status_code\",\n\t\t]\n\n\t\t//
    Overrides the default key in the log line to be `traceId`, which is then used
    by Grafana to\n\t\t// identify the trace ID for correlation with the Tempo datasource.\n\t\toverrides
    {\n\t\t\ttrace_id_key = \"traceId\"\n\t\t}\n\n\t\t// Send to the OpenTelemetry
    Loki exporter.\n\t\toutput {\n\t\t\tlogs = [otelcol.exporter.loki.autologging.input]\n\t\t}\n\t}\n\n\t//
    Simply forwards the incoming OpenTelemetry log format out as a Loki log.\n\t//
    We need this stage to ensure we can then process the logline as a Loki object.\n\totelcol.exporter.loki
    \"autologging\" {\n\t\tforward_to = [loki.process.autologging.receiver]\n\t}\n\n\t//
    The Loki processor allows us to accept a correctly formatted Loki log and mutate
    it into\n\t// a set of fields for output.\n\tloki.process \"autologging\" {\n\t\t//
    The JSON stage simply extracts the `body` (the actual logline) from the Loki log,
    ignoring\n\t\t// all other fields.\n\t\tstage.json {\n\t\t\texpressions = {\"body\"
    = \"\"}\n\t\t}\n\t\t// The output stage takes the body (the main logline) and
    uses this as the source for the output\n\t\t// logline. In this case, it essentially
    turns it into logfmt.\n\t\tstage.output {\n\t\t\tsource = \"body\"\n\t\t}\n\n\t\tforward_to
    = [loki.process.traceslogs.receiver]\n\t}\n\n\tloki.process \"traceslogs\" {\n\t\tstage.tenant
    {\n\t\t\tvalue = \"anonymous\"\n\t\t}\n\n\t\tforward_to = argument.logs_forward_to.value\n\t}\n\n\t/*****************************************************************\n\t*
    EXPORTS\n\t*****************************************************************/\n\texport
    \"alloy_traces_input\" {\n\t\tvalue = otelcol.processor.batch.default.input\n\t}\n}\n"
kind: ConfigMap
metadata:
  name: alloy-modules-kubernetes-traces-4h9946thb4
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-opentelemetry.json: |-
    {
          "graphTooltip": 1,
          "links": [
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "Receivers for traces [otelcol.receiver]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans successfully pushed into the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(otelcol_receiver_accepted_spans_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }} / {{ transport }}",
                      "range": true
                   }
                ],
                "title": "Accepted spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans that could not be pushed into the pipeline.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(otelcol_receiver_refused_spans_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }} / {{ transport }}",
                      "range": true
                   }
                ],
                "title": "Refused spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "The duration of inbound RPCs.\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 0
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "color": {
                      "exponent": 0.5,
                      "fill": "dark-orange",
                      "mode": "scheme",
                      "scale": "exponential",
                      "scheme": "Oranges",
                      "steps": 65
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 1.0000000000000001e-09
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "ms"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (le) (increase(rpc_server_duration_milliseconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", rpc_service=\"opentelemetry.proto.collector.trace.v1.TraceService\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "RPC server duration",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 10
                },
                "title": "Batching of logs, metrics, and traces [otelcol.processor.batch]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans, metric datapoints, or log lines in a batch\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 10
                },
                "maxDataPoints": 30,
                "options": {
                   "calculate": false,
                   "color": {
                      "exponent": 0.5,
                      "fill": "dark-orange",
                      "mode": "scheme",
                      "scale": "exponential",
                      "scheme": "Oranges",
                      "steps": 65
                   },
                   "exemplars": {
                      "color": "rgba(255,0,255,0.7)"
                   },
                   "filterValues": {
                      "le": 1.0000000000000001e-09
                   },
                   "tooltip": {
                      "show": true,
                      "yHistogram": true
                   },
                   "yAxis": {
                      "unit": "short"
                   }
                },
                "pluginVersion": "9.0.6",
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (le) (increase(otelcol_processor_batch_batch_send_size_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval]))\n",
                      "format": "heatmap",
                      "instant": false,
                      "legendFormat": "{{le}}",
                      "range": true
                   }
                ],
                "title": "Number of units in the batch",
                "type": "heatmap"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of distinct metadata value combinations being processed\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 10
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "otelcol_processor_batch_metadata_cardinality{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Distinct metadata values",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of times the batch was sent due to a timeout trigger\n",
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 10
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(otelcol_processor_batch_timeout_trigger_send_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Timeout trigger",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 20
                },
                "title": "Exporters for traces [otelcol.exporter]",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans successfully sent to destination.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 20
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(otelcol_exporter_sent_spans_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Exported sent spans",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of spans in failed attempts to send to destination.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      }
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 20
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(otelcol_exporter_send_failed_spans_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{ pod }}",
                      "range": true
                   }
                ],
                "title": "Exported failed spans",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / OpenTelemetry",
          "uid": "9b6d37c8603e19e8922133984faad93d"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-opentelemetry.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-prometheus-remote-write.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=~\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "icon": "doc",
                "targetBlank": true,
                "title": "Documentation",
                "tooltip": "Component documentation",
                "type": "link",
                "url": "https://grafana.com/docs/alloy/latest/reference/components/prometheus.remote_write/"
             },
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "collapsed": false,
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 0
                },
                "title": "prometheus.scrape",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Percentage of targets successfully scraped by prometheus.scrape\ncomponents.\n\nThis metric is calculated by dividing the number of targets\nsuccessfully scraped by the total number of targets scraped,\nacross all the namespaces in the selected cluster.\n\nLow success rates can indicate a problem with scrape targets,\nstale service discovery, or Alloy misconfiguration.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 0,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(up{job=~\"$job\", cluster=~\"$cluster\"})\n/\ncount (up{job=~\"$job\", cluster=~\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "% of targets successfully scraped",
                      "range": true
                   }
                ],
                "title": "Scrape success rate in $cluster",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Duration of successful scrapes by prometheus.scrape components,\nacross all the namespaces in the selected cluster.\n\nThis metric should be below your configured scrape interval.\nHigh durations can indicate a problem with a scrape target or\na performance issue with Alloy.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 12,
                   "y": 1
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.99, scrape_duration_seconds{job=~\"$job\", cluster=~\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p99",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.95, scrape_duration_seconds{job=~\"$job\", cluster=~\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p95",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "quantile(0.50, scrape_duration_seconds{job=~\"$job\", cluster=~\"$cluster\"})\n",
                      "instant": false,
                      "legendFormat": "p50",
                      "range": true
                   }
                ],
                "title": "Scrape duration in $cluster",
                "type": "timeseries"
             },
             {
                "collapsed": false,
                "datasource": "${datasource}",
                "gridPos": {
                   "h": 1,
                   "w": 24,
                   "x": 0,
                   "y": 11
                },
                "title": "prometheus.remote_write",
                "type": "row"
             },
             {
                "datasource": "${datasource}",
                "description": "Percentage of samples sent by prometheus.remote_write that succeeded.\n\nLow success rates can indicate a problem with Alloy or the remote storage.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 0,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "(\n    1 - \n    (\n        sum(rate(prometheus_remote_storage_samples_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval]))\n    )\n    /\n    (\n        sum(rate(prometheus_remote_storage_samples_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval]))\n    )\n)\n",
                      "instant": false,
                      "legendFormat": "% of samples successfully sent",
                      "range": true
                   }
                ],
                "title": "Remote write success rate in $cluster",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Latency of writes to the remote system made by\nprometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 12,
                   "x": 12,
                   "y": 12
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.99, sum by (le) (\n  rate(prometheus_remote_storage_sent_batch_duration_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n))\n",
                      "instant": false,
                      "legendFormat": "99th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "histogram_quantile(0.50, sum by (le) (\n  rate(prometheus_remote_storage_sent_batch_duration_seconds_bucket{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n))\n",
                      "instant": false,
                      "legendFormat": "50th percentile",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(rate(prometheus_remote_storage_sent_batch_duration_seconds_sum{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}[$__rate_interval])) /\nsum(rate(prometheus_remote_storage_sent_batch_duration_seconds_count{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}[$__rate_interval]))\n",
                      "instant": false,
                      "legendFormat": "Average",
                      "range": true
                   }
                ],
                "title": "Write latency in $cluster",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "How far behind prometheus.remote_write from samples recently written\nto the WAL.\n\nEach endpoint prometheus.remote_write is configured to send metrics\nhas its own delay. The time shown here is the sum across all\nendpoints for the given component.\n\nIt is normal for the WAL delay to be within 1-3 scrape intervals. If\nthe WAL delay continues to increase beyond that amount, try\nincreasing the number of maximum shards.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "s"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (instance, component_path, component_id) (\n  prometheus_remote_storage_highest_timestamp_in_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\"}\n  - ignoring(url, remote_name) group_right(instance)\n  prometheus_remote_storage_queue_highest_sent_timestamp_seconds{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "WAL delay",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data containing samples and metadata sent by\nprometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (remote_name, url) (\n    rate(prometheus_remote_storage_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval]) +\n    rate(prometheus_remote_storage_metadata_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Data write throughput",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of shards which are concurrently sending samples read\nfrom the Write-Ahead Log.\n\nShards are bound to a minimum and maximum, displayed on the graph.\nThe lowest minimum and the highest maximum across all clients is\nshown.\n\nEach client has its own set of shards, minimum shards, and maximum\nshards; filter to a specific URL to display more granular\ninformation.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "none"
                   },
                   "overrides": [
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Minimum"
                         },
                         "properties": [
                            {
                               "id": "custom.lineStyle",
                               "value": {
                                  "dash": [
                                     10,
                                     15
                                  ],
                                  "fill": "dash"
                               }
                            },
                            {
                               "id": "custom.showPoints",
                               "value": "never"
                            },
                            {
                               "id": "custom.hideFrom",
                               "value": {
                                  "legend": true,
                                  "tooltip": false,
                                  "viz": false
                               }
                            }
                         ]
                      },
                      {
                         "matcher": {
                            "id": "byName",
                            "options": "Maximum"
                         },
                         "properties": [
                            {
                               "id": "custom.lineStyle",
                               "value": {
                                  "dash": [
                                     10,
                                     15
                                  ],
                                  "fill": "dash"
                               }
                            },
                            {
                               "id": "custom.showPoints",
                               "value": "never"
                            },
                            {
                               "id": "custom.hideFrom",
                               "value": {
                                  "legend": true,
                                  "tooltip": false,
                                  "viz": false
                               }
                            }
                         ]
                      }
                   ]
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 22
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (remote_name, url) (\n    prometheus_remote_storage_shards{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "min (\n    prometheus_remote_storage_shards_min{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "Minimum",
                      "range": true
                   },
                   {
                      "datasource": "${datasource}",
                      "expr": "max (\n    prometheus_remote_storage_shards_max{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n)\n",
                      "instant": false,
                      "legendFormat": "Maximum",
                      "range": true
                   }
                ],
                "title": "Shards",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total outgoing samples sent by prometheus.remote_write.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 32
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url, remote_name) (\n  rate(prometheus_remote_storage_samples_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Sent samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of samples which prometheus.remote_write could not send due to\nnon-recoverable errors.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 32
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url,remote_name) (\n  rate(prometheus_remote_storage_samples_failed_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Failed samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of samples which prometheus.remote_write attempted to resend\nafter receiving a recoverable error.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 20,
                         "gradientMode": "hue",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "cps"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 32
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum without (url,remote_name) (\n  rate(prometheus_remote_storage_samples_retried_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}[$__rate_interval])\n)\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Retried samples / second",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series across all components.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 0,
                   "y": 42
                },
                "options": {
                   "legend": {
                      "showLegend": false
                   }
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum(prometheus_remote_write_wal_storage_active_series{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"})\n",
                      "instant": false,
                      "legendFormat": "Series",
                      "range": true
                   }
                ],
                "title": "Active series (total)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series which are currently being tracked by\nprometheus.remote_write components, with separate lines for each Alloy instance.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 8,
                   "y": 42
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "prometheus_remote_write_wal_storage_active_series{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_id!=\"\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}} / {{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Active series (by instance/component)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Total number of active series which are currently being tracked by\nprometheus.remote_write components, aggregated across all instances.\n\nAn \"active series\" is a series that prometheus.remote_write recently\nreceived a sample for. Active series are garbage collected whenever a\ntruncation of the WAL occurs.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "short"
                   }
                },
                "gridPos": {
                   "h": 10,
                   "w": 8,
                   "x": 16,
                   "y": 42
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "sum by (component_path, component_id) (prometheus_remote_write_wal_storage_active_series{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_id!=\"\", component_path=~\"$component_path\", component_id=~\"$component\", url=~\"$url\"})\n",
                      "instant": false,
                      "legendFormat": "{{component_path}} {{component_id}}",
                      "range": true
                   }
                ],
                "title": "Active series (by component)",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "component_path",
                   "multi": true,
                   "name": "component_path",
                   "query": {
                      "query": "label_values(prometheus_remote_write_wal_samples_appended_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_id=~\"prometheus.remote_write.*\", component_path=~\".*\"}, component_path)\n",
                      "refId": "component_path"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "component",
                   "multi": true,
                   "name": "component",
                   "query": {
                      "query": "label_values(prometheus_remote_write_wal_samples_appended_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\", component_id=~\"prometheus.remote_write.*\"}, component_id)\n",
                      "refId": "component"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "url",
                   "multi": true,
                   "name": "url",
                   "query": {
                      "query": "label_values(prometheus_remote_storage_sent_batch_duration_seconds_sum{cluster=~\"$cluster\", namespace=~\"$namespace\", job=\"$job\", instance=~\"$instance\", component_id=~\"$component\"}, url)\n",
                      "refId": "url"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Prometheus Components",
          "uid": "e324cc55567d7f3a8e32860ff8e6d0d9"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-prometheus-remote-write.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alloy-resources.json: |-
    {
          "annotations": {
             "list": [
                {
                   "datasource": "$loki_datasource",
                   "enable": true,
                   "expr": "{cluster=\"$cluster\", container=\"kube-diff-logger\"} | json | namespace_extracted=\"alloy\" | name_extracted=~\"alloy.*\"",
                   "iconColor": "rgba(0, 211, 255, 1)",
                   "instant": false,
                   "name": "Deployments",
                   "titleFormat": "{{cluster}}/{{namespace}}"
                }
             ]
          },
          "graphTooltip": 1,
          "links": [
             {
                "asDropdown": true,
                "icon": "external link",
                "includeVars": true,
                "keepTime": true,
                "tags": [
                   "alloy-mixin"
                ],
                "targetBlank": false,
                "title": "Dashboards",
                "type": "dashboards"
             }
          ],
          "panels": [
             {
                "datasource": "${datasource}",
                "description": "CPU usage of the Alloy process relative to 1 CPU core.\n\nFor example, 100% means using one entire CPU core.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "percentunit"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_process_cpu_seconds_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "CPU usage",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Resident memory size of the Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "decbytes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 0
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "alloy_resources_process_resident_memory_bytes{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Memory (RSS)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate at which the Alloy process performs garbage collections.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "drawStyle": "points",
                         "pointSize": 3
                      },
                      "unit": "ops"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 0,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(go_gc_duration_seconds_count{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[5m])\nand on(instance)\nalloy_build_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Garbage collections",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Number of goroutines which are running in parallel. An infinitely\ngrowing number of these indicates a goroutine leak.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "none"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 8,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "go_goroutines{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\nand on(instance)\nalloy_build_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Goroutines",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Heap memory currently in use by the Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "unit": "decbytes"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 8,
                   "x": 16,
                   "y": 8
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "go_memstats_heap_inuse_bytes{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\nand on(instance)\nalloy_build_info{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Memory (heap inuse)",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data received across all network interfaces for the machine\nAlloy is running on.\n\nData shown here is across all running processes and not exclusive to\nthe running Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 30,
                         "gradientMode": "none",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 0,
                   "y": 16
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_machine_rx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Network receive bandwidth",
                "type": "timeseries"
             },
             {
                "datasource": "${datasource}",
                "description": "Rate of data sent across all network interfaces for the machine\nAlloy is running on.\n\nData shown here is across all running processes and not exclusive to\nthe running Alloy process.\n",
                "fieldConfig": {
                   "defaults": {
                      "custom": {
                         "fillOpacity": 30,
                         "gradientMode": "none",
                         "stacking": {
                            "mode": "normal"
                         }
                      },
                      "unit": "Bps"
                   }
                },
                "gridPos": {
                   "h": 8,
                   "w": 12,
                   "x": 12,
                   "y": 16
                },
                "targets": [
                   {
                      "datasource": "${datasource}",
                      "expr": "rate(alloy_resources_machine_tx_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\", instance=~\"$instance\"}[$__rate_interval])\n",
                      "instant": false,
                      "legendFormat": "{{instance}}",
                      "range": true
                   }
                ],
                "title": "Network send bandwidth",
                "type": "timeseries"
             }
          ],
          "refresh": "10s",
          "schemaVersion": 36,
          "tags": [
             "alloy-mixin"
          ],
          "templating": {
             "list": [
                {
                   "label": "Data Source",
                   "name": "datasource",
                   "query": "prometheus",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "label": "Loki Data Source",
                   "name": "loki_datasource",
                   "query": "loki",
                   "refresh": 1,
                   "sort": 2,
                   "type": "datasource"
                },
                {
                   "datasource": "${datasource}",
                   "label": "cluster",
                   "name": "cluster",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components, cluster)\n",
                      "refId": "cluster"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "namespace",
                   "name": "namespace",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\"}, namespace)\n",
                      "refId": "namespace"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "datasource": "${datasource}",
                   "label": "job",
                   "name": "job",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\"}, job)\n",
                      "refId": "job"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                },
                {
                   "allValue": ".*",
                   "datasource": "${datasource}",
                   "includeAll": true,
                   "label": "instance",
                   "multi": true,
                   "name": "instance",
                   "query": {
                      "query": "label_values(alloy_component_controller_running_components{cluster=~\"$cluster\", namespace=~\"$namespace\", job=~\"$job\"}, instance)\n",
                      "refId": "instance"
                   },
                   "refresh": 2,
                   "sort": 2,
                   "type": "query"
                }
             ]
          },
          "time": {
             "from": "now-1h",
             "to": "now"
          },
          "timepicker": {
             "refresh_intervals": [
                "5s",
                "10s",
                "30s",
                "1m",
                "5m",
                "15m",
                "30m",
                "1h",
                "2h",
                "1d"
             ],
             "time_options": [
                "5m",
                "15m",
                "1h",
                "6h",
                "12h",
                "24h",
                "2d",
                "7d",
                "30d",
                "90d"
             ]
          },
          "timezone": "utc",
          "title": "Alloy / Resources",
          "uid": "d6a8574c31f3d7cb8f1345ec84d15a67"
       }
kind: ConfigMap
metadata:
  annotations:
    grafana_dashboard_folder: /dashboards/alloy-mixin
  labels:
    grafana_dashboard: "1"
  name: alloy-resources.json
  namespace: monitoring-system
---
apiVersion: v1
data:
  alertmanager_fallback_config.yaml: |
    route:
      group_wait: 0s
      receiver: empty-receiver

    receivers:
      # In this example we're not going to send any notification out of Alertmanager.
      - name: 'empty-receiver'
  mimir.yaml: |
    # Do not use this configuration in production.
    # It is for demonstration purposes only.
    multitenancy_enabled: true

    # -usage-stats.enabled=false
    usage_stats:
      enabled: false

    server:
      http_listen_port: 8080
      grpc_listen_port: 9095
      log_level: info

    # https://grafana.com/docs/mimir/latest/references/configuration-parameters/#use-environment-variables-in-the-configuration
    common:
      storage:
        backend: s3
        s3:
          endpoint:          ${MIMIR_S3_ENDPOINT:minio.minio-system.svc:443}
          access_key_id:     ${MIMIR_S3_ACCESS_KEY_ID:lgtmp}
          secret_access_key: ${MIMIR_S3_SECRET_ACCESS_KEY:supersecret}
          insecure:          ${MIMIR_S3_INSECURE:false}
          http:
            insecure_skip_verify: true

    alertmanager:
      data_dir: /data/alertmanager
      enable_api: true
      external_url: /alertmanager
      fallback_config_file: /etc/mimir/alertmanager_fallback_config.yaml
    alertmanager_storage:
      s3:
        bucket_name: mimir-alertmanager


    memberlist:
      join_members: [ mimir-memberlist:7946 ]

    ingester:
      ring:
        replication_factor: 1

    store_gateway:
      sharding_ring:
        replication_factor: 1


    blocks_storage:
      s3:
        bucket_name: mimir-blocks
      tsdb:
        dir: /data/ingester
        ship_interval: 1m
        block_ranges_period: [ 2h ]
        retention_period: 3h
      bucket_store:
        index_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        chunks_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

        metadata_cache:
          backend: memcached
          memcached:
            addresses: dns+memcached.memcached-system.svc:11211

    ruler:
      rule_path: /data/rules
      enable_api: true
      alertmanager_url: http://localhost:8080/alertmanager
    ruler_storage:
      s3:
        bucket_name: mimir-ruler
      cache:
        backend: memcached
        memcached:
          addresses: dns+memcached.memcached-system.svc:11211

    compactor:
      compaction_interval: 30s
      data_dir: /data/mimir-compactor
      cleanup_interval:    1m
      tenant_cleanup_delay: 1m

    limits:
      native_histograms_ingestion_enabled: true

    overrides_exporter:
      ring:
        enabled: true
        wait_stability_min_duration: 30s

    runtime_config:
      file: /etc/mimir/runtime.yaml
  runtime.yaml: |-
    # This file can be used to set overrides or other runtime config.
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_fetched_series_per_query: 100000
        native_histograms_ingestion_enabled: true
        ruler_max_rules_per_rule_group: 50
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
  name: mimir-config-mt42964996
  namespace: monitoring-system
---
apiVersion: v1
data:
  config.yaml: |
    multitenancy_enabled: true
    analytics:
      reporting_enabled: false

    show_banner: false

    # https://grafana.com/docs/pyroscope/latest/configure-server/configure-disk-storage/#configure-pyroscope-disk-storage
    pyroscopedb:
      max_block_duration: 5m

    # https://grafana.com/docs/pyroscope/latest/configure-server/reference-configuration-parameters/#use-environment-variables-in-the-configuration
    storage:
      backend: s3
      s3:
        bucket_name: pyroscope-data
        endpoint: ${PYROSCOPE_STORAGE_S3_ENDPOINT:-minio.minio-system.svc:443}
        access_key_id: ${PYROSCOPE_STORAGE_S3_ACCESS_KEY_ID:-lgtmp}
        secret_access_key: ${PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY:-supersecret}
        insecure: ${PYROSCOPE_STORAGE_S3_INSECURE:-false}
        http:
          insecure_skip_verify: true
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope-config
  namespace: profiles-system
---
apiVersion: v1
data:
  overrides.yaml: |
    overrides:
      {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope-overrides-config
  namespace: profiles-system
---
apiVersion: v1
data:
  instance-address: bWVtY2FjaGVkLm1lbWNhY2hlZC1zeXN0ZW0uc3ZjLmNsdXN0ZXIubG9jYWw6MTEyMTE=
  instance-name: cHJpbWFyeQ==
  instance-timeout: NXM=
kind: Secret
metadata:
  name: alloy-integrations-memcached
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  instance-name: cHJpbWFyeQ==
  mysql-host: bXlzcWwubXlzcWwtc3lzdGVtLnN2Yy5jbHVzdGVyLmxvY2Fs
  mysql-password: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
  mysql-username: bGd0bXA=
kind: Secret
metadata:
  name: alloy-integrations-mysql
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  instance-address: cmVkaXMtbWFzdGVyLnJlZGlzLXN5c3RlbS5zdmMuY2x1c3Rlci5sb2NhbDo2Mzc5
  instance-name: cHJpbWFyeQ==
  instance-password: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: alloy-integrations-redis
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  MIMIR_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
  name: mimir-env-92ddctt858
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  PYROSCOPE_STORAGE_S3_SECRET_ACCESS_KEY: VkQ1MzhPWXhTRWlHRDRJOW1tRmZxRk1DR3ExdklpR20=
kind: Secret
metadata:
  name: pyroscope-env-h982fgc652
  namespace: profiles-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
  namespace: monitoring-system
spec:
  internalTrafficPolicy: Cluster
  ports:
  - name: http-metrics
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 12345
    protocol: TCP
    targetPort: 12345
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/name: alloy
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
  name: mimir
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
    prometheus.io/service-monitor: "false"
  name: mimir-memberlist
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope
  namespace: profiles-system
spec:
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
    prometheus.io/service-monitor: "false"
  name: pyroscope-headless
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: http2
    port: 4040
    protocol: TCP
    targetPort: http2
  selector:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope-memberlist
  namespace: profiles-system
spec:
  clusterIP: None
  ports:
  - name: memberlist
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/name: pyroscope
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.13.0
  name: mimir
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
      app.kubernetes.io/part-of: memberlist
  template:
    metadata:
      annotations:
        logs.grafana.com/scrape: "true"
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: mimir
      labels:
        app.kubernetes.io/component: mimir
        app.kubernetes.io/instance: mimir-monolithic-mode
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
    spec:
      containers:
      - args:
        - -target=all
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - secretRef:
            name: mimir-env-92ddctt858
        image: docker.io/grafana/mimir-alpine:2.13.0
        imagePullPolicy: IfNotPresent
        name: mimir
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
        resources:
          limits:
            cpu: 999m
            memory: 1Gi
          requests:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /data
          name: storage
      terminationGracePeriodSeconds: 60
      volumes:
      - configMap:
          name: mimir-config-mt42964996
        name: config
      - emptyDir: {}
        name: storage
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
  serviceName: alloy
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        logs.grafana.com/scrape: "true"
        profiles.grafana.com/cpu.port_name: http-metrics
        profiles.grafana.com/cpu.scrape: "false"
        profiles.grafana.com/goroutine.port_name: http-metrics
        profiles.grafana.com/goroutine.scrape: "false"
        profiles.grafana.com/memory.port_name: http-metrics
        profiles.grafana.com/memory.scrape: "false"
        pyroscope.io/service_name: alloy
      labels:
        app.kubernetes.io/instance: alloy
        app.kubernetes.io/name: alloy
    spec:
      containers:
      - args:
        - run
        - /etc/alloy/config.alloy
        - --storage.path=/tmp/alloy
        - --server.http.listen-addr=0.0.0.0:12345
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=alloy-cluster
        - --stability.level=experimental
        env:
        - name: ALLOY_DEPLOY_MODE
          value: helm
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
        - secretRef:
            name: alloy-env
            optional: true
        image: docker.io/grafana/alloy:v1.4.1
        imagePullPolicy: IfNotPresent
        name: alloy
        ports:
        - containerPort: 12345
          name: http-metrics
        - containerPort: 4317
          name: grpc-otlp
          protocol: TCP
        - containerPort: 4318
          name: http-otlp
          protocol: TCP
        - containerPort: 9411
          name: zipkin
          protocol: TCP
        - containerPort: 6831
          name: jaeger-compact
          protocol: UDP
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 12345
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 250m
            memory: 1024Mi
          requests:
            cpu: 100m
            memory: 256Mi
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /etc/alloy
          name: config
        - mountPath: /etc/alloy/modules/kubernetes/metrics
          name: modules-kubernetes-metrics
        - mountPath: /etc/alloy/modules/kubernetes/logs
          name: modules-kubernetes-logs
        - mountPath: /etc/alloy/modules/kubernetes/traces
          name: modules-kubernetes-traces
        - mountPath: /etc/alloy/modules/kubernetes/profiles
          name: modules-kubernetes-profiles
        - mountPath: /etc/alloy/modules/kubernetes/jobs
          name: modules-kubernetes-jobs
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: alloy
      volumes:
      - configMap:
          name: alloy-config-g5hfmfd9ff
        name: config
      - configMap:
          name: alloy-modules-kubernetes-metrics-8287kbcb62
        name: modules-kubernetes-metrics
      - configMap:
          name: alloy-modules-kubernetes-logs-b2t74ghdgm
        name: modules-kubernetes-logs
      - configMap:
          name: alloy-modules-kubernetes-traces-4h9946thb4
        name: modules-kubernetes-traces
      - configMap:
          name: alloy-modules-kubernetes-profiles-6479gm5dc2
        name: modules-kubernetes-profiles
      - configMap:
          name: alloy-modules-kubernetes-jobs-9d9gc54kgg
        name: modules-kubernetes-jobs
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope
  namespace: profiles-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
  serviceName: pyroscope-headless
  template:
    metadata:
      annotations:
        checksum/config: 8b7c24d1e7ecfe5c334a39bcbefeb58391619db7f6fecebc48e60677fdb415e8
        profiles.grafana.com/cpu.port_name: http2
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.port_name: http2
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/memory.port_name: http2
        profiles.grafana.com/memory.scrape: "true"
        pyroscope.io/service_name: pyroscope
      labels:
        app.kubernetes.io/component: all
        app.kubernetes.io/instance: pyroscope
        app.kubernetes.io/name: pyroscope
        name: pyroscope
    spec:
      containers:
      - args:
        - -target=all
        - -self-profiling.disable-push=true
        - -server.http-listen-port=4040
        - -memberlist.cluster-label=profiles-system-pyroscope
        - -memberlist.join=dns+pyroscope-memberlist.profiles-system.svc.cluster.local.:7946
        - -config.file=/etc/pyroscope/config.yaml
        - -runtime-config.file=/etc/pyroscope/overrides/overrides.yaml
        - -config.expand-env=true
        - -log.level=warn
        envFrom:
        - secretRef:
            name: pyroscope-env-h982fgc652
        image: grafana/pyroscope:1.7.1
        imagePullPolicy: IfNotPresent
        name: pyroscope
        ports:
        - containerPort: 4040
          name: http2
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http2
            scheme: HTTP
        resources:
          limits:
            cpu: 999m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 256Mi
        securityContext: {}
        volumeMounts:
        - mountPath: /etc/pyroscope/config.yaml
          name: config
          subPath: config.yaml
        - mountPath: /etc/pyroscope/overrides/
          name: overrides-config
        - mountPath: /data
          name: data
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: pyroscope
      volumes:
      - configMap:
          name: pyroscope-config
        name: config
      - configMap:
          name: pyroscope-overrides-config
        name: overrides-config
      - emptyDir: {}
        name: data
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope
  namespace: profiles-system
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alloy-mixin-alerts
  namespace: monitoring-system
spec:
  groups:
  - name: alloy_clustering
    rules:
    - alert: ClusterNotConverging
      annotations:
        description: 'Cluster is not converging: nodes report different number of
          peers in the cluster. Job is {{ $labels.job }}'
        summary: Cluster is not converging.
      expr: stddev by (cluster, namespace, job, cluster_name) (sum without (state)
        (cluster_node_peers)) != 0
      for: 10m
      labels:
        severity: warning
    - alert: ClusterNodeCountMismatch
      annotations:
        description: Nodes report different number of peers vs. the count of observed
          Alloy metrics. Some Alloy metrics may be missing or the cluster is in a
          split brain state. Job is {{ $labels.job }}
        summary: Nodes report different number of peers vs. the count of observed
          Alloy metrics.
      expr: |
        sum without (state) (cluster_node_peers) !=
        on (cluster, namespace, job, cluster_name) group_left
        count by (cluster, namespace, job, cluster_name) (cluster_node_info)
      for: 15m
      labels:
        severity: warning
    - alert: ClusterNodeUnhealthy
      annotations:
        description: Cluster node is reporting a gossip protocol health score > 0.
          Job is {{ $labels.job }}
        summary: Cluster unhealthy.
      expr: |
        cluster_node_gossip_health_score > 0
      for: 10m
      labels:
        severity: warning
    - alert: ClusterNodeNameConflict
      annotations:
        description: A node tried to join the cluster with a name conflicting with
          an existing peer. Job is {{ $labels.job }}
        summary: Cluster Node Name Conflict.
      expr: sum by (cluster, namespace, job, cluster_name) (rate(cluster_node_gossip_received_events_total{event="node_conflict"}[2m]))
        > 0
      for: 10m
      labels:
        severity: warning
    - alert: ClusterNodeStuckTerminating
      annotations:
        description: There is a node within the cluster that is stuck in Terminating
          state. Job is {{ $labels.job }}
        summary: Cluster node stuck in Terminating state.
      expr: sum by (cluster, namespace, job, instance, cluster_name) (cluster_node_peers{state="terminating"})
        > 0
      for: 10m
      labels:
        severity: warning
    - alert: ClusterConfigurationDrift
      annotations:
        description: Cluster nodes are not using the same configuration file. Job
          is {{ $labels.job }}
        summary: Cluster configuration drifting.
      expr: |
        count without (sha256) (
            max by (cluster, namespace, sha256, job, cluster_name) (alloy_config_hash and on(cluster, namespace, job, cluster_name) cluster_node_info)
        ) > 1
      for: 5m
      labels:
        severity: warning
  - name: alloy_controller
    rules:
    - alert: SlowComponentEvaluations
      annotations:
        description: Component evaluations are taking too long under job {{ $labels.job
          }}, component_path {{ $labels.component_path }}, component_id {{ $labels.component_id
          }}.
        summary: Component evaluations are taking too long.
      expr: sum by (cluster, namespace, job, component_path, component_id) (rate(alloy_component_evaluation_slow_seconds[10m]))
        > 0
      for: 15m
      labels:
        severity: warning
    - alert: UnhealthyComponents
      annotations:
        description: Unhealthy components detected under job {{ $labels.job }}
        summary: Unhealthy components detected.
      expr: sum by (cluster, namespace, job) (alloy_component_controller_running_components{health_type!="healthy"})
        > 0
      for: 15m
      labels:
        severity: warning
  - name: alloy_otelcol
    rules:
    - alert: OtelcolReceiverRefusedSpans
      annotations:
        description: The receiver could not push some spans to the pipeline under
          job {{ $labels.job }}. This could be due to reaching a limit such as the
          ones imposed by otelcol.processor.memory_limiter.
        summary: The receiver could not push some spans to the pipeline.
      expr: sum by (cluster, namespace, job) (rate(otelcol_receiver_refused_spans_total{}[1m]))
        > 0
      for: 5m
      labels:
        severity: warning
    - alert: OtelcolExporterFailedSpans
      annotations:
        description: The exporter failed to send spans to their destination under
          job {{ $labels.job }}. There could be an issue with the payload or with
          the destination endpoint.
        summary: The exporter failed to send spans to their destination.
      expr: sum by (cluster, namespace, job) (rate(otelcol_exporter_send_failed_spans_total{}[1m]))
        > 0
      for: 5m
      labels:
        severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
  namespace: monitoring-system
spec:
  endpoints:
  - honorLabels: true
    port: http-metrics
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/name: alloy
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: mimir
    app.kubernetes.io/instance: mimir-monolithic-mode
    app.kubernetes.io/managed-by: Kustomize
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.13.0
  name: mimir
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/mimir
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: mimir
      app.kubernetes.io/instance: mimir-monolithic-mode
      app.kubernetes.io/name: mimir
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/component: all
    app.kubernetes.io/instance: pyroscope
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: pyroscope
    app.kubernetes.io/version: 1.7.1
    helm.sh/chart: pyroscope-1.7.1
  name: pyroscope
  namespace: profiles-system
spec:
  endpoints:
  - port: http2
    relabelings:
    - action: replace
      replacement: profiles-system/pyroscope
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - profiles-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app.kubernetes.io/component: all
      app.kubernetes.io/instance: pyroscope
      app.kubernetes.io/name: pyroscope
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: networking
    app.kubernetes.io/instance: alloy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: alloy
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/version: v1.4.1
    helm.sh/chart: alloy-0.8.1
  name: alloy
  namespace: monitoring-system
spec:
  rules:
  - host: alloy.localhost
    http:
      paths:
      - backend:
          service:
            name: alloy
            port:
              number: 12345
        path: /
        pathType: Prefix
